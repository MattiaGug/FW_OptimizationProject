{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7a36719-313b-45ec-8378-63ab7c085d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import svd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import functions as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62cc971-758a-48bc-9809-dc839de0c7ba",
   "metadata": {},
   "source": [
    "# Amazon Books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d1886-ac6c-41ea-a4c0-8ba3f337ab0a",
   "metadata": {},
   "source": [
    "This dataset is publically available on Kaggle: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4946549-ea8f-4cb1-971f-e200ff520b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! unzip \"/Users/fabiopimentel/clases/Seguendo año cuarto semestre/Optimization/proyecto/codigo/fabio/data/amazon_books.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "701ff0b3-2d0b-47da-95a1-a6fae25890a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books_reviews = pd.read_csv(r\"C:\\Users\\utente13\\Desktop\\data\\Books_rating.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92291a30-c4b4-4d90-9509-1d934249e1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>User_id</th>\n",
       "      <th>profileName</th>\n",
       "      <th>review/helpfulness</th>\n",
       "      <th>review/score</th>\n",
       "      <th>review/time</th>\n",
       "      <th>review/summary</th>\n",
       "      <th>review/text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1882931173</td>\n",
       "      <td>Its Only Art If Its Well Hung!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AVCGYZL8FQQTD</td>\n",
       "      <td>Jim of Oz \"jim-of-oz\"</td>\n",
       "      <td>7/7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>940636800</td>\n",
       "      <td>Nice collection of Julie Strain images</td>\n",
       "      <td>This is only for Julie Strain fans. It's a col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A30TK6U7DNS82R</td>\n",
       "      <td>Kevin Killian</td>\n",
       "      <td>10/10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1095724800</td>\n",
       "      <td>Really Enjoyed It</td>\n",
       "      <td>I don't care much for Dr. Seuss but after read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A3UH4UZ4RSVO82</td>\n",
       "      <td>John Granger</td>\n",
       "      <td>10/11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1078790400</td>\n",
       "      <td>Essential for every personal and Public Library</td>\n",
       "      <td>If people become the books they read and if \"t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A2MVUWT453QH61</td>\n",
       "      <td>Roy E. Perry \"amateur philosopher\"</td>\n",
       "      <td>7/7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1090713600</td>\n",
       "      <td>Phlip Nel gives silly Seuss a serious treatment</td>\n",
       "      <td>Theodore Seuss Geisel (1904-1991), aka &amp;quot;D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A22X4XUPKF66MR</td>\n",
       "      <td>D. H. Richards \"ninthwavestore\"</td>\n",
       "      <td>3/3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1107993600</td>\n",
       "      <td>Good academic overview</td>\n",
       "      <td>Philip Nel - Dr. Seuss: American IconThis is b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id                           Title  Price         User_id  \\\n",
       "0  1882931173  Its Only Art If Its Well Hung!    NaN   AVCGYZL8FQQTD   \n",
       "1  0826414346        Dr. Seuss: American Icon    NaN  A30TK6U7DNS82R   \n",
       "2  0826414346        Dr. Seuss: American Icon    NaN  A3UH4UZ4RSVO82   \n",
       "3  0826414346        Dr. Seuss: American Icon    NaN  A2MVUWT453QH61   \n",
       "4  0826414346        Dr. Seuss: American Icon    NaN  A22X4XUPKF66MR   \n",
       "\n",
       "                          profileName review/helpfulness  review/score  \\\n",
       "0               Jim of Oz \"jim-of-oz\"                7/7           4.0   \n",
       "1                       Kevin Killian              10/10           5.0   \n",
       "2                        John Granger              10/11           5.0   \n",
       "3  Roy E. Perry \"amateur philosopher\"                7/7           4.0   \n",
       "4     D. H. Richards \"ninthwavestore\"                3/3           4.0   \n",
       "\n",
       "   review/time                                   review/summary  \\\n",
       "0    940636800           Nice collection of Julie Strain images   \n",
       "1   1095724800                                Really Enjoyed It   \n",
       "2   1078790400  Essential for every personal and Public Library   \n",
       "3   1090713600  Phlip Nel gives silly Seuss a serious treatment   \n",
       "4   1107993600                           Good academic overview   \n",
       "\n",
       "                                         review/text  \n",
       "0  This is only for Julie Strain fans. It's a col...  \n",
       "1  I don't care much for Dr. Seuss but after read...  \n",
       "2  If people become the books they read and if \"t...  \n",
       "3  Theodore Seuss Geisel (1904-1991), aka &quot;D...  \n",
       "4  Philip Nel - Dr. Seuss: American IconThis is b...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_books_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f877610-dbd8-49a9-8528-1452e01542d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews_groups = df_books_reviews.groupby(\"review/score\")[\"review/score\"].count()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(reviews_groups.index, reviews_groups.values, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Ratings Distribution\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(reviews_groups.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056012a9-5116-44cf-92c4-ebb7e75e4faf",
   "metadata": {},
   "source": [
    "# Inspecting the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dde1931-4c0a-4990-b451-4eb08a47e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users: 1008972 items: 221998 observations: 3000000 density: 1.3393469053626162e-05\n",
      "user counts:\n",
      " count    1.008972e+06\n",
      "mean     2.416532e+00\n",
      "std      1.213263e+01\n",
      "min      1.000000e+00\n",
      "1%       1.000000e+00\n",
      "5%       1.000000e+00\n",
      "25%      1.000000e+00\n",
      "50%      1.000000e+00\n",
      "75%      2.000000e+00\n",
      "95%      7.000000e+00\n",
      "99%      1.900000e+01\n",
      "max      5.795000e+03\n",
      "dtype: float64\n",
      "item counts:\n",
      " count    221998.000000\n",
      "mean         13.513635\n",
      "std          76.174268\n",
      "min           1.000000\n",
      "1%            1.000000\n",
      "5%            1.000000\n",
      "25%           1.000000\n",
      "50%           3.000000\n",
      "75%           8.000000\n",
      "95%          40.000000\n",
      "99%         179.000000\n",
      "max        6796.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_users = df_books_reviews['User_id'].nunique() # Count how many unique users exist in the dataset\n",
    "n_items = df_books_reviews['Id'].nunique() # Count how many unique items (products) exist in the dataset\n",
    "\n",
    "n_obs = len(df_books_reviews) # Count how many (user, item, rating) rows exist\n",
    "density = n_obs / (n_users * n_items) #   density = (# observed ratings) / (# possible user–item pairs)\n",
    "\n",
    "print(\"users:\", n_users, \"items:\", n_items, \"observations:\", n_obs, \"density:\", density)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# USER ACTIVITY DISTRIBUTION\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# Compute how many ratings each user has:\n",
    "#   df.groupby('user').size() returns a Series mapping user → count of ratings.\n",
    "# Then we call `.describe(...)` to compute summary statistics such as:\n",
    "#   min, max, mean, std, and selected percentiles.\n",
    "user_counts = df_books_reviews.groupby('User_id').size().describe(percentiles=[.01,.05,.25,.5,.75,.95,.99])\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# ITEM POPULARITY DISTRIBUTION\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# Same logic as above, but grouping by items:\n",
    "#   df.groupby('item').size() gives item → number of ratings\n",
    "# Then `.describe(...)` summarizes it with percentiles.\n",
    "item_counts = df_books_reviews.groupby('Id').size().describe(percentiles=[.01,.05,.25,.5,.75,.95,.99])\n",
    "\n",
    "\n",
    "print(\"user counts:\\n\", user_counts)\n",
    "print(\"item counts:\\n\", item_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac467b-d1c8-4cf8-8a69-9b3023586237",
   "metadata": {},
   "source": [
    "This matrix cannot be used for matrix completion in its current form.\n",
    "\n",
    "That means:\n",
    "- 99.9987% of the matrix entries are missing\n",
    "- Average 2.4 ratings per user\n",
    "- Median = 1 rating per user\n",
    "- Average 13.5 ratings per item\n",
    "- Median = 3 ratings per item\n",
    "This is far below the threshold where matrix completion is possible.\n",
    "\n",
    "Why is this a problem?\n",
    "Matrix completion requires:\n",
    "- Enough information per user to estimate their latent preferences\n",
    "- Enough information per item to estimate its latent features\n",
    "- A connected interaction graph (users ↔ items)\n",
    "With 1 rating per user, the matrix is almost entirely disconnected.\n",
    "Each user gives you almost no signal about their row of the matrix.\n",
    "\n",
    "This is not a sparse “recommender system dataset”; it's essentially a very large set of isolated interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59867fcd-e581-4737-a2cf-a1fb0b0c9190",
   "metadata": {},
   "source": [
    "# Preprocessing code before using the filtering pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b1061-41cb-4ead-bddc-464b3bf9109a",
   "metadata": {},
   "source": [
    "Why this step?\n",
    "\n",
    "- The filtering pipeline assumes the dataframe has exactly:\n",
    "    - user (user IDs)\n",
    "    - item (item/product IDs)\n",
    "    - rating (numeric rating signal)\n",
    "- Extra columns will confuse the filtering steps, especially the bipartite graph construction (which expects only user/item identifiers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cdcf8ea-9a9a-4147-a99a-15cb60ba873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce original dataframe\n",
    "df_small = df_books_reviews.rename(columns={\n",
    "    'User_id': 'user',\n",
    "    'Id': 'item',\n",
    "    'review/score': 'rating'\n",
    "})[['user', 'item', 'rating']]\n",
    "\n",
    "# Ensure types are consistent\n",
    "df_small['user'] = df_small['user'].astype(str)\n",
    "df_small['item'] = df_small['item'].astype(str)\n",
    "df_small['rating'] = df_small['rating'].astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c8cdd-5593-4566-9e4b-a0d5dec060fc",
   "metadata": {},
   "source": [
    "# Filtering Pipeline Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e640b5e8-7e6a-4e75-bfb3-c1f201c98498",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 1. Load data\n",
    "########################################\n",
    "# df must contain: 'user', 'item', 'rating'\n",
    "# Example:\n",
    "# df = pd.read_csv(\"amazon_books.csv\")\n",
    "\n",
    "########################################\n",
    "# 2. Minimum-interactions filtering\n",
    "########################################\n",
    "\n",
    "def filter_min_interactions(df, min_user_ratings=5, min_item_ratings=5):\n",
    "    \"\"\"\n",
    "    Iteratively filter users and items until all remaining users and items\n",
    "    satisfy the minimum interaction thresholds.\n",
    "    \"\"\"\n",
    "    prev_shape = (-1, -1)\n",
    "    \n",
    "    while True:\n",
    "        # Filter users\n",
    "        user_counts = df['user'].value_counts()\n",
    "        good_users = user_counts[user_counts >= min_user_ratings].index\n",
    "        df = df[df['user'].isin(good_users)]\n",
    "        \n",
    "        # Filter items\n",
    "        item_counts = df['item'].value_counts()\n",
    "        good_items = item_counts[item_counts >= min_item_ratings].index\n",
    "        df = df[df['item'].isin(good_items)]\n",
    "\n",
    "        # Stop if converged (no further data removed)\n",
    "        if df.shape == prev_shape:\n",
    "            break\n",
    "        \n",
    "        prev_shape = df.shape\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "########################################\n",
    "# 3. Compute density of matrix\n",
    "########################################\n",
    "\n",
    "def compute_density(df):\n",
    "    n_users = df['user'].nunique()\n",
    "    n_items = df['item'].nunique()\n",
    "    n_obs = len(df)\n",
    "    density = n_obs / (n_users * n_items)\n",
    "    return n_users, n_items, n_obs, density\n",
    "\n",
    "\n",
    "########################################\n",
    "# 4. Extract the largest connected component\n",
    "########################################\n",
    "\n",
    "def extract_largest_component(df):\n",
    "    \"\"\"\n",
    "    Build bipartite graph (users ↔ items) and keep only entries inside \n",
    "    the largest connected component. This helps remove isolated pockets.\n",
    "    \"\"\"\n",
    "    # Relabel with prefixes to avoid overlap\n",
    "    df['user_node'] = df['user'].astype(str).apply(lambda x: f\"u_{x}\")\n",
    "    df['item_node'] = df['item'].astype(str).apply(lambda x: f\"i_{x}\")\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(df['user_node'], bipartite=0)\n",
    "    G.add_nodes_from(df['item_node'], bipartite=1)\n",
    "    G.add_edges_from(zip(df['user_node'], df['item_node']))\n",
    "\n",
    "    # Find largest connected component\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "\n",
    "    # Filter df to only nodes in the largest CC\n",
    "    df = df[df['user_node'].isin(largest_cc) & df['item_node'].isin(largest_cc)]\n",
    "    \n",
    "    # Drop helper columns\n",
    "    return df.drop(columns=['user_node', 'item_node'])\n",
    "\n",
    "\n",
    "########################################\n",
    "# 5. Full pipeline\n",
    "########################################\n",
    "\n",
    "def build_filtered_matrix(df, min_user_ratings=10, min_item_ratings=10):\n",
    "    print(\"Original shape:\", df.shape)\n",
    "\n",
    "    # Step A: minimum-interaction filtering\n",
    "    df_filt = filter_min_interactions(\n",
    "        df, \n",
    "        min_user_ratings=min_user_ratings, \n",
    "        min_item_ratings=min_item_ratings\n",
    "    )\n",
    "    print(\"After min-count filtering:\", df_filt.shape)\n",
    "    \n",
    "    # Step B: largest connected component\n",
    "    df_cc = extract_largest_component(df_filt)\n",
    "    print(\"After extracting largest connected component:\", df_cc.shape)\n",
    "\n",
    "    # Step C: recompute density\n",
    "    n_users, n_items, n_obs, density = compute_density(df_cc)\n",
    "    print(\"\\n--- Final dataset stats ---\")\n",
    "    print(\"Users:\", n_users)\n",
    "    print(\"Items:\", n_items)\n",
    "    print(\"Observations:\", n_obs)\n",
    "    print(\"Density:\", density)\n",
    "\n",
    "    return df_cc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41f8e7e4-c0ac-42d6-bf49-38e8a48098bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (3000000, 3)\n",
      "After min-count filtering: (651032, 3)\n",
      "After extracting largest connected component: (650912, 3)\n",
      "\n",
      "--- Final dataset stats ---\n",
      "Users: 5892\n",
      "Items: 7437\n",
      "Observations: 650912\n",
      "Density: 0.014854627250894387\n"
     ]
    }
   ],
   "source": [
    "# Now run the filtering pipeline\n",
    "df_filtered = build_filtered_matrix(df_small,\n",
    "                                    min_user_ratings=20,\n",
    "                                    min_item_ratings=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d30ffe-5d8c-46e7-9568-18a9e4739186",
   "metadata": {},
   "source": [
    "The filtered dataset is now perfectly suitable for matrix completion and for running Frank–Wolfe or matrix completation experiments.\n",
    "\n",
    "This means:\n",
    "\n",
    "- The matrix contains enough signal\n",
    "- Rows/columns are sufficiently connected\n",
    "- Low-rank structure can be learned\n",
    "- Frank–Wolfe is feasible\n",
    "- Evaluation will be stable\n",
    "\n",
    "The final size (5.9k × 7.4k) is big enough to be meaningful and small enough to run experiments fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2964fd1f-1ef0-4ca8-b135-bb0e9624bbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>nan</td>\n",
       "      <td>0671551345</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>nan</td>\n",
       "      <td>0671551345</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>nan</td>\n",
       "      <td>0671551345</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>nan</td>\n",
       "      <td>0671551345</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>nan</td>\n",
       "      <td>0671551345</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999689</th>\n",
       "      <td>nan</td>\n",
       "      <td>B000P91JYW</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999691</th>\n",
       "      <td>nan</td>\n",
       "      <td>B000P91JYW</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999692</th>\n",
       "      <td>nan</td>\n",
       "      <td>B000P91JYW</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999693</th>\n",
       "      <td>nan</td>\n",
       "      <td>B000P91JYW</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999694</th>\n",
       "      <td>nan</td>\n",
       "      <td>B000P91JYW</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>650912 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user        item  rating\n",
       "392      nan  0671551345     5.0\n",
       "393      nan  0671551345     3.0\n",
       "395      nan  0671551345     5.0\n",
       "396      nan  0671551345     5.0\n",
       "398      nan  0671551345     5.0\n",
       "...      ...         ...     ...\n",
       "2999689  nan  B000P91JYW     1.0\n",
       "2999691  nan  B000P91JYW     1.0\n",
       "2999692  nan  B000P91JYW     1.0\n",
       "2999693  nan  B000P91JYW     1.0\n",
       "2999694  nan  B000P91JYW     1.0\n",
       "\n",
       "[650912 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1845374c-dc6d-410a-b8e8-0df6cdaf26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the filtered data and transforming it into a user-rating matrix\n",
    "user_rating_matrix = pd.pivot_table(df_filtered, values = \"rating\", index=\"user\", columns=\"item\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0afab637-4583-456a-a4dd-3225c8849d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       nan,        nan,        nan, ...,        nan,        nan,\n",
       "               nan],\n",
       "       [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "               nan],\n",
       "       [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "               nan],\n",
       "       ...,\n",
       "       [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "               nan],\n",
       "       [       nan,        nan,        nan, ...,        nan,        nan,\n",
       "               nan],\n",
       "       [3.9       , 4.61904762, 5.        , ..., 4.64705882, 3.07692308,\n",
       "        5.        ]], shape=(5892, 7437))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e515e36e-bf60-4f1e-ade7-b9491b82f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with zero\n",
    "user_rating_matrix[np.isnan(user_rating_matrix)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4ac0476-871a-4cf0-879e-a346dacb7c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [3.9       , 4.61904762, 5.        , ..., 4.64705882, 3.07692308,\n",
       "        5.        ]], shape=(5892, 7437))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1970cdb-a096-4a56-870c-6f29d9234778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Users count: 5892\n",
      "Top books count: 7437 \n",
      "Sparsity: 99.37 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Users count: {}\\nTop books count: {} \".format(user_rating_matrix.shape[0],user_rating_matrix.shape[1]))\n",
    "\n",
    "sparsity = (user_rating_matrix == 0).sum() / (user_rating_matrix.shape[0] * user_rating_matrix.shape[1])\n",
    "print(\"Sparsity: {} %\".format(round(sparsity * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44370d5-f679-4f84-b755-56aeb1687929",
   "metadata": {},
   "source": [
    "99.37% sparsity is absolutely normal and acceptable for matrix completion.\n",
    "\n",
    "Why sparsity ≠ bad for matrix completion\n",
    "\n",
    "Frank-Wolfe–based matrix completion assumes:\n",
    "- The matrix is low-rank or approximately low-rank\n",
    "- You observe random entries (approximately)\n",
    "\n",
    "Even with 99%+ sparsity, if:\n",
    "- Each user has ≥20 ratings\n",
    "- Each item has ≥20 ratings (your filtering ensures this)\n",
    "- The graph is a single giant connected component (you ensured this too)\n",
    "\n",
    "… then matrix completion is feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5df47a-d09d-4c80-a145-9d10c3b1ec54",
   "metadata": {},
   "source": [
    "# Dense Core Extraction for Amazon Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5bc53a16-5f58-4dc8-b81c-c91ea68c16da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def extract_dense_core(R, movie_quantile=0.50, user_quantile=0.935):\n",
    "    \"\"\"\n",
    "    Extract a dense core of the user-item matrix R using quantile filtering.\n",
    "    \n",
    "    R: numpy array (users x items), 0 = missing value.\n",
    "    movie_quantile: quantile for item popularity\n",
    "    user_quantile: quantile for user activity\n",
    "    \n",
    "    Returns:\n",
    "        R_core: filtered dense-core matrix\n",
    "        user_mask: boolean mask for selected users\n",
    "        item_mask: boolean mask for selected items\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1. Count #ratings per user\n",
    "    # ----------------------------\n",
    "    user_counts = np.count_nonzero(R, axis=1)  # shape: (n_users,)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2. Count #ratings per item\n",
    "    # ----------------------------\n",
    "    item_counts = np.count_nonzero(R, axis=0)  # shape: (n_items,)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3. Compute quantile thresholds\n",
    "    # ----------------------------\n",
    "    user_threshold = np.quantile(user_counts, user_quantile)\n",
    "    item_threshold = np.quantile(item_counts, movie_quantile)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4. Create masks\n",
    "    # ----------------------------\n",
    "    user_mask = user_counts >= user_threshold\n",
    "    item_mask = item_counts >= item_threshold\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5. Extract dense core\n",
    "    # ----------------------------\n",
    "    R_core = R[user_mask][:, item_mask]\n",
    "\n",
    "    return R_core, user_mask, item_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61311ca8-a628-4743-ace5-e0edcff84b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (5892, 7437)\n",
      "Dense core shape: (389, 3866)\n",
      "Sparsity: 95.53%\n"
     ]
    }
   ],
   "source": [
    "R_core, user_mask, item_mask = extract_dense_core(\n",
    "    R=user_rating_matrix, \n",
    "    movie_quantile=0.50,\n",
    "    user_quantile=0.935\n",
    ")\n",
    "\n",
    "print(\"Original shape:\", user_rating_matrix.shape)\n",
    "print(\"Dense core shape:\", R_core.shape)\n",
    "\n",
    "sparsity = (R_core == 0).sum() / (R_core.size)\n",
    "print(\"Sparsity: {:.2f}%\".format(sparsity * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2f609f-1a21-48c1-80ca-fae18ad43454",
   "metadata": {},
   "source": [
    "Why is the dense core still very sparse (95.53%)?\n",
    "\n",
    "Because Amazon is extremely sparse by nature.\n",
    "\n",
    "Even after quantile filtering:\n",
    "- You kept only top 6.6% most active users\n",
    "- And top 50% most popular items\n",
    "- But unlike MovieLens, in Amazon:\n",
    "- Power users don’t rate thousands of items\n",
    "- Popular items aren't rated by tens of thousands of users\n",
    "- Even the “most active” users only rate ~20–50 items\n",
    "- Even the “most popular” books may only have ~40–200 reviews\n",
    "\n",
    "Result:\n",
    "\n",
    "👉 The dense core is still sparse.\n",
    "\n",
    "This is not a problem — this is typical for Amazon books.\n",
    "\n",
    "The dense cores of Amazon data rarely go below 90% sparsity.\n",
    "\n",
    "MovieLens dense cores are 25–35% sparsity because:\n",
    "- users watch many movies\n",
    "- movies get many ratings\n",
    "- their distributions are extremely power-law heavy\n",
    "\n",
    "Amazon Books does NOT behave that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36f2e004-a2c9-4a6b-88b4-fba159cb4e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the directory containing functions.py to the path if necessary\n",
    "# (Assuming functions.py is in the same folder as the notebook)\n",
    "sys.path.append('.') \n",
    "\n",
    "import functions as fn\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dedaf3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train stats: 53721 ratings\n",
      "Test stats: 13430 ratings\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(R, test_ratio=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Masks a percentage of the non-zero entries in R to create a test set.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Get coordinates of observed ratings\n",
    "    rows, cols = R.nonzero()\n",
    "    n_observed = len(rows)\n",
    "    \n",
    "    # Select indices for testing\n",
    "    test_size = int(n_observed * test_ratio)\n",
    "    perm = np.random.permutation(n_observed)\n",
    "    test_indices = perm[:test_size]\n",
    "    train_indices = perm[test_size:]\n",
    "    \n",
    "    # Create Train Matrix (R with test entries removed/set to 0)\n",
    "    R_train = R.copy()\n",
    "    R_train[rows[test_indices], cols[test_indices]] = 0\n",
    "    \n",
    "    # Create Test Matrix (Only the entries we masked)\n",
    "    R_test = np.zeros_like(R)\n",
    "    R_test[rows[test_indices], cols[test_indices]] = R[rows[test_indices], cols[test_indices]]\n",
    "    \n",
    "    return R_train, R_test\n",
    "\n",
    "# Execute the split on your dense core\n",
    "R_train, R_test = train_test_split(R_core, test_ratio=0.2)\n",
    "print(f\"Train stats: {np.count_nonzero(R_train)} ratings\")\n",
    "print(f\"Test stats: {np.count_nonzero(R_test)} ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87d9adc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TEST FW STANDARD (Delta=5000) ---\n",
      "Inizio Frank-Wolfe (Delta=5000, LineSearch=armijo)...\n",
      "Iter 1/5000 | Loss: 14.66878 | Gap: 62.57300 | Gamma: 0.2500\n",
      "Iter 2/5000 | Loss: 11.36616 | Gap: 95.55159 | Gamma: 0.1250\n",
      "Iter 3/5000 | Loss: 11.08132 | Gap: 42.75703 | Gamma: 0.1250\n",
      "Iter 4/5000 | Loss: 9.26173 | Gap: 59.60729 | Gamma: 0.0625\n",
      "Iter 5/5000 | Loss: 4.75602 | Gap: 20.98315 | Gamma: 0.5000\n",
      "Iter 6/5000 | Loss: 4.53128 | Gap: 33.24703 | Gamma: 0.0625\n",
      "Iter 7/5000 | Loss: 4.52425 | Gap: 26.27605 | Gamma: 0.0625\n",
      "Iter 8/5000 | Loss: 4.11998 | Gap: 27.18569 | Gamma: 0.0312\n",
      "Iter 9/5000 | Loss: 3.78490 | Gap: 12.07361 | Gamma: 0.0625\n",
      "Iter 10/5000 | Loss: 3.53601 | Gap: 7.74235 | Gamma: 0.1250\n",
      "Iter 11/5000 | Loss: 3.28802 | Gap: 19.73520 | Gamma: 0.0312\n",
      "Iter 12/5000 | Loss: 3.07853 | Gap: 7.31426 | Gamma: 0.0625\n",
      "Iter 13/5000 | Loss: 3.01451 | Gap: 9.37334 | Gamma: 0.0312\n",
      "Iter 14/5000 | Loss: 2.94647 | Gap: 8.49279 | Gamma: 0.0312\n",
      "Iter 15/5000 | Loss: 2.87826 | Gap: 7.91884 | Gamma: 0.0312\n",
      "Iter 16/5000 | Loss: 2.82039 | Gap: 7.40409 | Gamma: 0.0312\n",
      "Iter 17/5000 | Loss: 2.76264 | Gap: 7.48179 | Gamma: 0.0312\n",
      "Iter 18/5000 | Loss: 2.71538 | Gap: 7.25868 | Gamma: 0.0312\n",
      "Iter 19/5000 | Loss: 2.66849 | Gap: 7.40009 | Gamma: 0.0312\n",
      "Iter 20/5000 | Loss: 2.63234 | Gap: 7.24436 | Gamma: 0.0312\n",
      "Iter 21/5000 | Loss: 2.59552 | Gap: 7.44291 | Gamma: 0.0312\n",
      "Iter 22/5000 | Loss: 2.56888 | Gap: 7.29585 | Gamma: 0.0312\n",
      "Iter 23/5000 | Loss: 2.53952 | Gap: 7.51351 | Gamma: 0.0312\n",
      "Iter 24/5000 | Loss: 2.51897 | Gap: 7.33150 | Gamma: 0.0312\n",
      "Iter 25/5000 | Loss: 2.49437 | Gap: 7.52950 | Gamma: 0.0312\n",
      "Iter 26/5000 | Loss: 2.47736 | Gap: 7.32546 | Gamma: 0.0312\n",
      "Iter 27/5000 | Loss: 2.45607 | Gap: 7.50013 | Gamma: 0.0312\n",
      "Iter 28/5000 | Loss: 2.44156 | Gap: 7.30224 | Gamma: 0.0312\n",
      "Iter 29/5000 | Loss: 2.42300 | Gap: 7.46291 | Gamma: 0.0312\n",
      "Iter 30/5000 | Loss: 2.41067 | Gap: 7.28207 | Gamma: 0.0312\n",
      "Iter 31/5000 | Loss: 2.39451 | Gap: 7.43441 | Gamma: 0.0312\n",
      "Iter 32/5000 | Loss: 2.38412 | Gap: 7.26926 | Gamma: 0.0312\n",
      "Iter 33/5000 | Loss: 2.37007 | Gap: 7.41438 | Gamma: 0.0312\n",
      "Iter 34/5000 | Loss: 2.36136 | Gap: 7.26134 | Gamma: 0.0312\n",
      "Iter 35/5000 | Loss: 2.34911 | Gap: 7.39914 | Gamma: 0.0312\n",
      "Iter 36/5000 | Loss: 2.34181 | Gap: 7.25591 | Gamma: 0.0312\n",
      "Iter 37/5000 | Loss: 2.33110 | Gap: 7.38630 | Gamma: 0.0312\n",
      "Iter 38/5000 | Loss: 2.32498 | Gap: 7.25163 | Gamma: 0.0312\n",
      "Iter 39/5000 | Loss: 2.31556 | Gap: 7.37469 | Gamma: 0.0312\n",
      "Iter 40/5000 | Loss: 2.31040 | Gap: 7.24789 | Gamma: 0.0312\n",
      "Iter 41/5000 | Loss: 2.30209 | Gap: 7.36392 | Gamma: 0.0312\n",
      "Iter 42/5000 | Loss: 2.29774 | Gap: 7.24457 | Gamma: 0.0312\n",
      "Iter 43/5000 | Loss: 2.29037 | Gap: 7.35416 | Gamma: 0.0312\n",
      "Iter 44/5000 | Loss: 2.28671 | Gap: 7.24182 | Gamma: 0.0312\n",
      "Iter 45/5000 | Loss: 2.28016 | Gap: 7.34562 | Gamma: 0.0312\n",
      "Iter 46/5000 | Loss: 2.27709 | Gap: 7.23973 | Gamma: 0.0312\n",
      "Iter 47/5000 | Loss: 2.27126 | Gap: 7.33831 | Gamma: 0.0312\n",
      "Iter 48/5000 | Loss: 2.26870 | Gap: 7.23819 | Gamma: 0.0312\n",
      "Iter 49/5000 | Loss: 2.26349 | Gap: 7.33199 | Gamma: 0.0312\n",
      "Iter 50/5000 | Loss: 2.26137 | Gap: 7.23704 | Gamma: 0.0312\n",
      "Iter 51/5000 | Loss: 2.25670 | Gap: 7.32642 | Gamma: 0.0312\n",
      "Iter 52/5000 | Loss: 2.25496 | Gap: 7.23613 | Gamma: 0.0312\n",
      "Iter 53/5000 | Loss: 2.25076 | Gap: 7.32147 | Gamma: 0.0312\n",
      "Iter 54/5000 | Loss: 2.24933 | Gap: 7.23540 | Gamma: 0.0312\n",
      "Iter 55/5000 | Loss: 2.24554 | Gap: 7.31707 | Gamma: 0.0312\n",
      "Iter 56/5000 | Loss: 2.24440 | Gap: 7.23485 | Gamma: 0.0312\n",
      "Iter 57/5000 | Loss: 2.24097 | Gap: 7.31319 | Gamma: 0.0312\n",
      "Iter 58/5000 | Loss: 2.24006 | Gap: 7.23445 | Gamma: 0.0312\n",
      "Iter 59/5000 | Loss: 2.23695 | Gap: 7.30979 | Gamma: 0.0312\n",
      "Iter 60/5000 | Loss: 2.23625 | Gap: 7.23419 | Gamma: 0.0312\n",
      "Iter 61/5000 | Loss: 2.23342 | Gap: 7.30679 | Gamma: 0.0312\n",
      "Iter 62/5000 | Loss: 2.23290 | Gap: 7.23402 | Gamma: 0.0312\n",
      "Iter 63/5000 | Loss: 2.23031 | Gap: 7.30415 | Gamma: 0.0312\n",
      "Iter 64/5000 | Loss: 2.22996 | Gap: 7.23392 | Gamma: 0.0312\n",
      "Iter 65/5000 | Loss: 2.22758 | Gap: 7.30180 | Gamma: 0.0312\n",
      "Iter 66/5000 | Loss: 2.22736 | Gap: 7.23387 | Gamma: 0.0312\n",
      "Iter 67/5000 | Loss: 2.22518 | Gap: 7.29973 | Gamma: 0.0312\n",
      "Iter 68/5000 | Loss: 2.22508 | Gap: 7.23387 | Gamma: 0.0312\n",
      "Iter 69/5000 | Loss: 2.22306 | Gap: 7.29789 | Gamma: 0.0312\n",
      "Iter 70/5000 | Loss: 2.16654 | Gap: 7.23391 | Gamma: 0.0156\n",
      "Iter 71/5000 | Loss: 1.78425 | Gap: 3.89290 | Gamma: 0.2500\n",
      "Iter 72/5000 | Loss: 1.75347 | Gap: 5.38645 | Gamma: 0.0156\n",
      "Iter 73/5000 | Loss: 1.68971 | Gap: 4.51764 | Gamma: 0.0312\n",
      "Iter 74/5000 | Loss: 1.66573 | Gap: 5.65811 | Gamma: 0.0156\n",
      "Iter 75/5000 | Loss: 1.63965 | Gap: 3.44380 | Gamma: 0.0312\n",
      "Iter 76/5000 | Loss: 1.61771 | Gap: 6.10156 | Gamma: 0.0156\n",
      "Iter 77/5000 | Loss: 1.59575 | Gap: 2.93477 | Gamma: 0.0312\n",
      "Iter 78/5000 | Loss: 1.57969 | Gap: 5.52749 | Gamma: 0.0156\n",
      "Iter 79/5000 | Loss: 1.56756 | Gap: 2.81228 | Gamma: 0.0312\n",
      "Iter 80/5000 | Loss: 1.55318 | Gap: 5.68912 | Gamma: 0.0156\n",
      "Iter 81/5000 | Loss: 1.54147 | Gap: 2.68722 | Gamma: 0.0312\n",
      "Iter 82/5000 | Loss: 1.52952 | Gap: 5.42913 | Gamma: 0.0156\n",
      "Iter 83/5000 | Loss: 1.52074 | Gap: 2.64796 | Gamma: 0.0312\n",
      "Iter 84/5000 | Loss: 1.50993 | Gap: 5.42288 | Gamma: 0.0156\n",
      "Iter 85/5000 | Loss: 1.50144 | Gap: 2.60484 | Gamma: 0.0312\n",
      "Iter 86/5000 | Loss: 1.49193 | Gap: 5.30227 | Gamma: 0.0156\n",
      "Iter 87/5000 | Loss: 1.48489 | Gap: 2.58556 | Gamma: 0.0312\n",
      "Iter 88/5000 | Loss: 1.47622 | Gap: 5.28856 | Gamma: 0.0156\n",
      "Iter 89/5000 | Loss: 1.46987 | Gap: 2.56597 | Gamma: 0.0312\n",
      "Iter 90/5000 | Loss: 1.46201 | Gap: 5.24761 | Gamma: 0.0156\n",
      "Iter 91/5000 | Loss: 1.45670 | Gap: 2.55408 | Gamma: 0.0312\n",
      "Iter 92/5000 | Loss: 1.44950 | Gap: 5.24420 | Gamma: 0.0156\n",
      "Iter 93/5000 | Loss: 1.44504 | Gap: 2.54375 | Gamma: 0.0312\n",
      "Iter 94/5000 | Loss: 1.43842 | Gap: 5.24163 | Gamma: 0.0156\n",
      "Iter 95/5000 | Loss: 1.43479 | Gap: 2.53593 | Gamma: 0.0312\n",
      "Iter 96/5000 | Loss: 1.42865 | Gap: 5.24589 | Gamma: 0.0156\n",
      "Iter 97/5000 | Loss: 1.42577 | Gap: 2.52958 | Gamma: 0.0312\n",
      "Iter 98/5000 | Loss: 1.42006 | Gap: 5.25279 | Gamma: 0.0156\n",
      "Iter 99/5000 | Loss: 1.41787 | Gap: 2.52453 | Gamma: 0.0312\n",
      "Iter 100/5000 | Loss: 1.41253 | Gap: 5.26160 | Gamma: 0.0156\n",
      "Iter 101/5000 | Loss: 1.41092 | Gap: 2.52025 | Gamma: 0.0312\n",
      "Iter 102/5000 | Loss: 1.40590 | Gap: 5.26938 | Gamma: 0.0156\n",
      "Iter 103/5000 | Loss: 1.40483 | Gap: 2.51679 | Gamma: 0.0312\n",
      "Iter 104/5000 | Loss: 1.40009 | Gap: 5.27754 | Gamma: 0.0156\n",
      "Iter 105/5000 | Loss: 1.39948 | Gap: 2.51393 | Gamma: 0.0312\n",
      "Iter 106/5000 | Loss: 1.39498 | Gap: 5.28496 | Gamma: 0.0156\n",
      "Iter 107/5000 | Loss: 1.39479 | Gap: 2.51147 | Gamma: 0.0312\n",
      "Iter 108/5000 | Loss: 1.39049 | Gap: 5.29186 | Gamma: 0.0156\n",
      "Iter 109/5000 | Loss: 1.37093 | Gap: 2.50940 | Gamma: 0.0156\n",
      "Iter 110/5000 | Loss: 1.34146 | Gap: 1.94565 | Gamma: 0.0625\n",
      "Iter 111/5000 | Loss: 1.31509 | Gap: 7.24935 | Gamma: 0.0156\n",
      "Iter 112/5000 | Loss: 1.29062 | Gap: 1.80020 | Gamma: 0.0312\n",
      "Iter 113/5000 | Loss: 1.28748 | Gap: 3.15549 | Gamma: 0.0156\n",
      "Iter 114/5000 | Loss: 1.27669 | Gap: 2.41506 | Gamma: 0.0156\n",
      "Iter 115/5000 | Loss: 1.26300 | Gap: 1.99552 | Gamma: 0.0156\n",
      "Iter 116/5000 | Loss: 1.24711 | Gap: 1.79589 | Gamma: 0.0312\n",
      "Iter 117/5000 | Loss: 1.23443 | Gap: 3.44132 | Gamma: 0.0078\n",
      "Iter 118/5000 | Loss: 1.19644 | Gap: 1.49692 | Gamma: 0.0625\n",
      "Iter 119/5000 | Loss: 1.16940 | Gap: 2.18103 | Gamma: 0.0312\n",
      "Iter 120/5000 | Loss: 1.15591 | Gap: 1.81485 | Gamma: 0.0312\n",
      "Iter 121/5000 | Loss: 1.14254 | Gap: 3.78317 | Gamma: 0.0078\n",
      "Iter 122/5000 | Loss: 1.09899 | Gap: 1.31550 | Gamma: 0.1250\n",
      "Iter 123/5000 | Loss: 1.08173 | Gap: 5.25113 | Gamma: 0.0078\n",
      "Iter 124/5000 | Loss: 1.07200 | Gap: 2.42641 | Gamma: 0.0312\n",
      "Iter 125/5000 | Loss: 1.06895 | Gap: 1.72321 | Gamma: 0.0039\n",
      "Iter 126/5000 | Loss: 1.02984 | Gap: 1.44095 | Gamma: 0.0625\n",
      "Iter 127/5000 | Loss: 1.02101 | Gap: 2.36550 | Gamma: 0.0078\n",
      "Iter 128/5000 | Loss: 1.00645 | Gap: 1.25988 | Gamma: 0.0625\n",
      "Iter 129/5000 | Loss: 0.99499 | Gap: 3.28330 | Gamma: 0.0078\n",
      "Iter 130/5000 | Loss: 0.96335 | Gap: 1.37470 | Gamma: 0.0625\n",
      "Iter 131/5000 | Loss: 0.95524 | Gap: 1.81022 | Gamma: 0.0156\n",
      "Iter 132/5000 | Loss: 0.94921 | Gap: 1.81490 | Gamma: 0.0078\n",
      "Iter 133/5000 | Loss: 0.92825 | Gap: 1.01435 | Gamma: 0.0625\n",
      "Iter 134/5000 | Loss: 0.91673 | Gap: 4.43739 | Gamma: 0.0078\n",
      "Iter 135/5000 | Loss: 0.90700 | Gap: 0.98107 | Gamma: 0.0625\n",
      "Iter 136/5000 | Loss: 0.89241 | Gap: 1.68012 | Gamma: 0.0312\n",
      "Iter 137/5000 | Loss: 0.88658 | Gap: 1.12927 | Gamma: 0.0312\n",
      "Iter 138/5000 | Loss: 0.87925 | Gap: 3.22149 | Gamma: 0.0078\n",
      "Iter 139/5000 | Loss: 0.85984 | Gap: 0.84799 | Gamma: 0.0625\n",
      "Iter 140/5000 | Loss: 0.85241 | Gap: 2.08472 | Gamma: 0.0078\n",
      "Iter 141/5000 | Loss: 0.84281 | Gap: 1.13694 | Gamma: 0.0312\n",
      "Iter 142/5000 | Loss: 0.83859 | Gap: 1.98432 | Gamma: 0.0078\n",
      "Iter 143/5000 | Loss: 0.83163 | Gap: 0.84875 | Gamma: 0.0312\n",
      "Iter 144/5000 | Loss: 0.82766 | Gap: 2.13711 | Gamma: 0.0078\n",
      "Iter 145/5000 | Loss: 0.82255 | Gap: 0.84541 | Gamma: 0.0312\n",
      "Iter 146/5000 | Loss: 0.81888 | Gap: 2.13427 | Gamma: 0.0078\n",
      "Iter 147/5000 | Loss: 0.81603 | Gap: 0.84447 | Gamma: 0.0312\n",
      "Iter 148/5000 | Loss: 0.81225 | Gap: 2.29140 | Gamma: 0.0078\n",
      "Iter 149/5000 | Loss: 0.80683 | Gap: 0.82567 | Gamma: 0.0312\n",
      "Iter 150/5000 | Loss: 0.80344 | Gap: 2.13435 | Gamma: 0.0078\n",
      "Iter 151/5000 | Loss: 0.79972 | Gap: 0.78996 | Gamma: 0.0312\n",
      "Iter 152/5000 | Loss: 0.79629 | Gap: 2.27276 | Gamma: 0.0078\n",
      "Iter 153/5000 | Loss: 0.79236 | Gap: 0.77490 | Gamma: 0.0312\n",
      "Iter 154/5000 | Loss: 0.78942 | Gap: 2.11063 | Gamma: 0.0078\n",
      "Iter 155/5000 | Loss: 0.78326 | Gap: 0.79424 | Gamma: 0.0156\n",
      "Iter 156/5000 | Loss: 0.77979 | Gap: 0.91413 | Gamma: 0.0078\n",
      "Iter 157/5000 | Loss: 0.77032 | Gap: 0.62831 | Gamma: 0.0312\n",
      "Iter 158/5000 | Loss: 0.76839 | Gap: 1.65075 | Gamma: 0.0078\n",
      "Iter 159/5000 | Loss: 0.76478 | Gap: 0.70216 | Gamma: 0.0156\n",
      "Iter 160/5000 | Loss: 0.76204 | Gap: 0.97966 | Gamma: 0.0078\n",
      "Iter 161/5000 | Loss: 0.75714 | Gap: 0.65324 | Gamma: 0.0156\n",
      "Iter 162/5000 | Loss: 0.75430 | Gap: 0.86648 | Gamma: 0.0078\n",
      "Iter 163/5000 | Loss: 0.75034 | Gap: 0.59006 | Gamma: 0.0312\n",
      "Iter 164/5000 | Loss: 0.74836 | Gap: 2.13969 | Gamma: 0.0078\n",
      "Iter 165/5000 | Loss: 0.74536 | Gap: 0.68630 | Gamma: 0.0156\n",
      "Iter 166/5000 | Loss: 0.74310 | Gap: 0.93036 | Gamma: 0.0078\n",
      "Iter 167/5000 | Loss: 0.73949 | Gap: 0.63214 | Gamma: 0.0156\n",
      "Iter 168/5000 | Loss: 0.73746 | Gap: 0.94064 | Gamma: 0.0078\n",
      "Iter 169/5000 | Loss: 0.73350 | Gap: 0.59205 | Gamma: 0.0156\n",
      "Iter 170/5000 | Loss: 0.73154 | Gap: 0.88238 | Gamma: 0.0078\n",
      "Iter 171/5000 | Loss: 0.72795 | Gap: 0.57621 | Gamma: 0.0156\n",
      "Iter 172/5000 | Loss: 0.72623 | Gap: 0.89345 | Gamma: 0.0078\n",
      "Iter 173/5000 | Loss: 0.72355 | Gap: 0.58060 | Gamma: 0.0156\n",
      "Iter 174/5000 | Loss: 0.72216 | Gap: 0.95539 | Gamma: 0.0078\n",
      "Iter 175/5000 | Loss: 0.72088 | Gap: 0.61423 | Gamma: 0.0156\n",
      "Iter 176/5000 | Loss: 0.71999 | Gap: 1.05493 | Gamma: 0.0078\n",
      "Iter 177/5000 | Loss: 0.71978 | Gap: 0.68145 | Gamma: 0.0156\n",
      "Iter 178/5000 | Loss: 0.71923 | Gap: 1.12124 | Gamma: 0.0078\n",
      "Iter 179/5000 | Loss: 0.71916 | Gap: 0.71343 | Gamma: 0.0156\n",
      "Iter 180/5000 | Loss: 0.71879 | Gap: 1.16179 | Gamma: 0.0078\n",
      "Iter 181/5000 | Loss: 0.71811 | Gap: 0.69734 | Gamma: 0.0156\n",
      "Iter 182/5000 | Loss: 0.71758 | Gap: 1.20233 | Gamma: 0.0078\n",
      "Iter 183/5000 | Loss: 0.71613 | Gap: 0.63985 | Gamma: 0.0156\n",
      "Iter 184/5000 | Loss: 0.71544 | Gap: 1.09913 | Gamma: 0.0078\n",
      "Iter 185/5000 | Loss: 0.71392 | Gap: 0.62639 | Gamma: 0.0156\n",
      "Iter 186/5000 | Loss: 0.71320 | Gap: 1.07245 | Gamma: 0.0078\n",
      "Iter 187/5000 | Loss: 0.71214 | Gap: 0.62584 | Gamma: 0.0156\n",
      "Iter 188/5000 | Loss: 0.71155 | Gap: 1.07733 | Gamma: 0.0078\n",
      "Iter 189/5000 | Loss: 0.71110 | Gap: 0.65641 | Gamma: 0.0156\n",
      "Iter 190/5000 | Loss: 0.71067 | Gap: 1.09512 | Gamma: 0.0078\n",
      "Iter 191/5000 | Loss: 0.71038 | Gap: 0.67681 | Gamma: 0.0156\n",
      "Iter 192/5000 | Loss: 0.71004 | Gap: 1.14672 | Gamma: 0.0078\n",
      "Iter 193/5000 | Loss: 0.70942 | Gap: 0.65894 | Gamma: 0.0156\n",
      "Iter 194/5000 | Loss: 0.70902 | Gap: 1.12988 | Gamma: 0.0078\n",
      "Iter 195/5000 | Loss: 0.70813 | Gap: 0.64434 | Gamma: 0.0156\n",
      "Iter 196/5000 | Loss: 0.70769 | Gap: 1.11038 | Gamma: 0.0078\n",
      "Iter 197/5000 | Loss: 0.70681 | Gap: 0.62817 | Gamma: 0.0156\n",
      "Iter 198/5000 | Loss: 0.70640 | Gap: 1.09639 | Gamma: 0.0078\n",
      "Iter 199/5000 | Loss: 0.70556 | Gap: 0.62983 | Gamma: 0.0156\n",
      "Iter 200/5000 | Loss: 0.70520 | Gap: 1.11840 | Gamma: 0.0078\n",
      "Iter 201/5000 | Loss: 0.70471 | Gap: 0.61935 | Gamma: 0.0156\n",
      "Iter 202/5000 | Loss: 0.70449 | Gap: 1.14715 | Gamma: 0.0078\n",
      "Iter 203/5000 | Loss: 0.70426 | Gap: 0.63581 | Gamma: 0.0156\n",
      "Iter 204/5000 | Loss: 0.70407 | Gap: 1.14434 | Gamma: 0.0078\n",
      "Iter 205/5000 | Loss: 0.70164 | Gap: 0.66039 | Gamma: 0.0078\n",
      "Iter 206/5000 | Loss: 0.69633 | Gap: 0.45436 | Gamma: 0.0312\n",
      "Iter 207/5000 | Loss: 0.69371 | Gap: 1.54667 | Gamma: 0.0039\n",
      "Iter 208/5000 | Loss: 0.69238 | Gap: 0.45929 | Gamma: 0.0312\n",
      "Iter 209/5000 | Loss: 0.68931 | Gap: 1.64741 | Gamma: 0.0039\n",
      "Iter 210/5000 | Loss: 0.68789 | Gap: 0.61514 | Gamma: 0.0156\n",
      "Iter 211/5000 | Loss: 0.68379 | Gap: 0.51833 | Gamma: 0.0312\n",
      "Iter 212/5000 | Loss: 0.68277 | Gap: 0.85568 | Gamma: 0.0156\n",
      "Iter 213/5000 | Loss: 0.68063 | Gap: 1.26688 | Gamma: 0.0039\n",
      "Iter 214/5000 | Loss: 0.67743 | Gap: 0.48331 | Gamma: 0.0312\n",
      "Iter 215/5000 | Loss: 0.67165 | Gap: 0.69142 | Gamma: 0.0312\n",
      "Iter 216/5000 | Loss: 0.66910 | Gap: 0.69113 | Gamma: 0.0312\n",
      "Iter 217/5000 | Loss: 0.66748 | Gap: 0.70160 | Gamma: 0.0078\n",
      "Iter 218/5000 | Loss: 0.66632 | Gap: 0.83752 | Gamma: 0.0078\n",
      "Iter 219/5000 | Loss: 0.66410 | Gap: 0.62285 | Gamma: 0.0078\n",
      "Iter 220/5000 | Loss: 0.66116 | Gap: 0.49016 | Gamma: 0.0156\n",
      "Iter 221/5000 | Loss: 0.66049 | Gap: 0.81945 | Gamma: 0.0078\n",
      "Iter 222/5000 | Loss: 0.65858 | Gap: 0.56573 | Gamma: 0.0078\n",
      "Iter 223/5000 | Loss: 0.65558 | Gap: 0.44544 | Gamma: 0.0156\n",
      "Iter 224/5000 | Loss: 0.65490 | Gap: 0.74344 | Gamma: 0.0078\n",
      "Iter 225/5000 | Loss: 0.65308 | Gap: 0.53432 | Gamma: 0.0078\n",
      "Iter 226/5000 | Loss: 0.65035 | Gap: 0.42163 | Gamma: 0.0156\n",
      "Iter 227/5000 | Loss: 0.64996 | Gap: 0.73224 | Gamma: 0.0078\n",
      "Iter 228/5000 | Loss: 0.64844 | Gap: 0.53689 | Gamma: 0.0078\n",
      "Iter 229/5000 | Loss: 0.64666 | Gap: 0.42705 | Gamma: 0.0156\n",
      "Iter 230/5000 | Loss: 0.64508 | Gap: 0.82435 | Gamma: 0.0039\n",
      "Iter 231/5000 | Loss: 0.64270 | Gap: 0.35502 | Gamma: 0.0312\n",
      "Iter 232/5000 | Loss: 0.64044 | Gap: 0.76319 | Gamma: 0.0156\n",
      "Iter 233/5000 | Loss: 0.63699 | Gap: 0.45960 | Gamma: 0.0312\n",
      "Iter 234/5000 | Loss: 0.63456 | Gap: 0.74247 | Gamma: 0.0078\n",
      "Iter 235/5000 | Loss: 0.63355 | Gap: 0.57525 | Gamma: 0.0078\n",
      "Iter 236/5000 | Loss: 0.63180 | Gap: 0.55106 | Gamma: 0.0078\n",
      "Iter 237/5000 | Loss: 0.63059 | Gap: 0.43118 | Gamma: 0.0156\n",
      "Iter 238/5000 | Loss: 0.62889 | Gap: 0.91526 | Gamma: 0.0039\n",
      "Iter 239/5000 | Loss: 0.62348 | Gap: 0.34812 | Gamma: 0.0312\n",
      "Iter 240/5000 | Loss: 0.62226 | Gap: 0.72897 | Gamma: 0.0078\n",
      "Iter 241/5000 | Loss: 0.62203 | Gap: 0.64731 | Gamma: 0.0078\n",
      "Iter 242/5000 | Loss: 0.62054 | Gap: 0.48847 | Gamma: 0.0078\n",
      "Iter 243/5000 | Loss: 0.61977 | Gap: 0.39660 | Gamma: 0.0156\n",
      "Iter 244/5000 | Loss: 0.61830 | Gap: 0.88140 | Gamma: 0.0039\n",
      "Iter 245/5000 | Loss: 0.61328 | Gap: 0.32300 | Gamma: 0.0312\n",
      "Iter 246/5000 | Loss: 0.61109 | Gap: 0.60014 | Gamma: 0.0078\n",
      "Iter 247/5000 | Loss: 0.60976 | Gap: 0.42167 | Gamma: 0.0078\n",
      "Iter 248/5000 | Loss: 0.60855 | Gap: 0.42578 | Gamma: 0.0078\n",
      "Iter 249/5000 | Loss: 0.60839 | Gap: 0.38227 | Gamma: 0.0156\n",
      "Iter 250/5000 | Loss: 0.60697 | Gap: 0.93162 | Gamma: 0.0039\n",
      "Iter 251/5000 | Loss: 0.60615 | Gap: 0.30159 | Gamma: 0.0312\n",
      "Iter 252/5000 | Loss: 0.60483 | Gap: 0.76190 | Gamma: 0.0078\n",
      "Iter 253/5000 | Loss: 0.60167 | Gap: 0.45171 | Gamma: 0.0156\n",
      "Iter 254/5000 | Loss: 0.59960 | Gap: 0.35871 | Gamma: 0.0156\n",
      "Iter 255/5000 | Loss: 0.59847 | Gap: 0.61045 | Gamma: 0.0039\n",
      "Iter 256/5000 | Loss: 0.59440 | Gap: 0.29351 | Gamma: 0.0625\n",
      "Iter 257/5000 | Loss: 0.59090 | Gap: 1.00531 | Gamma: 0.0078\n",
      "Iter 258/5000 | Loss: 0.58857 | Gap: 0.53759 | Gamma: 0.0156\n",
      "Iter 259/5000 | Loss: 0.58838 | Gap: 0.54667 | Gamma: 0.0078\n",
      "Iter 260/5000 | Loss: 0.58824 | Gap: 0.58358 | Gamma: 0.0078\n",
      "Iter 261/5000 | Loss: 0.58755 | Gap: 0.46924 | Gamma: 0.0078\n",
      "Iter 262/5000 | Loss: 0.58647 | Gap: 0.41973 | Gamma: 0.0078\n",
      "Iter 263/5000 | Loss: 0.58513 | Gap: 0.36108 | Gamma: 0.0078\n",
      "Iter 264/5000 | Loss: 0.58422 | Gap: 0.32594 | Gamma: 0.0156\n",
      "Iter 265/5000 | Loss: 0.58306 | Gap: 0.76280 | Gamma: 0.0039\n",
      "Iter 266/5000 | Loss: 0.57829 | Gap: 0.25156 | Gamma: 0.1250\n",
      "Iter 267/5000 | Loss: 0.57302 | Gap: 2.80672 | Gamma: 0.0039\n",
      "Iter 268/5000 | Loss: 0.57139 | Gap: 1.35583 | Gamma: 0.0078\n",
      "Iter 269/5000 | Loss: 0.56999 | Gap: 0.72485 | Gamma: 0.0156\n",
      "Iter 270/5000 | Loss: 0.56927 | Gap: 0.49651 | Gamma: 0.0039\n",
      "Iter 271/5000 | Loss: 0.56821 | Gap: 0.33456 | Gamma: 0.0078\n",
      "Iter 272/5000 | Loss: 0.56747 | Gap: 0.36825 | Gamma: 0.0078\n",
      "Iter 273/5000 | Loss: 0.56693 | Gap: 0.36809 | Gamma: 0.0078\n",
      "Iter 274/5000 | Loss: 0.56665 | Gap: 0.39767 | Gamma: 0.0078\n",
      "Iter 275/5000 | Loss: 0.56603 | Gap: 0.39460 | Gamma: 0.0078\n",
      "Iter 276/5000 | Loss: 0.56546 | Gap: 0.34721 | Gamma: 0.0078\n",
      "Iter 277/5000 | Loss: 0.56478 | Gap: 0.37358 | Gamma: 0.0078\n",
      "Iter 278/5000 | Loss: 0.56389 | Gap: 0.33361 | Gamma: 0.0078\n",
      "Iter 279/5000 | Loss: 0.56306 | Gap: 0.32163 | Gamma: 0.0078\n",
      "Iter 280/5000 | Loss: 0.56198 | Gap: 0.31576 | Gamma: 0.0078\n",
      "Iter 281/5000 | Loss: 0.56155 | Gap: 0.27751 | Gamma: 0.0156\n",
      "Iter 282/5000 | Loss: 0.56055 | Gap: 0.65523 | Gamma: 0.0039\n",
      "Iter 283/5000 | Loss: 0.55891 | Gap: 0.26285 | Gamma: 0.0156\n",
      "Iter 284/5000 | Loss: 0.55876 | Gap: 0.35437 | Gamma: 0.0156\n",
      "Iter 285/5000 | Loss: 0.55794 | Gap: 0.58461 | Gamma: 0.0039\n",
      "Iter 286/5000 | Loss: 0.55691 | Gap: 0.25599 | Gamma: 0.0312\n",
      "Iter 287/5000 | Loss: 0.55493 | Gap: 0.59623 | Gamma: 0.0078\n",
      "Iter 288/5000 | Loss: 0.55392 | Gap: 0.39947 | Gamma: 0.0078\n",
      "Iter 289/5000 | Loss: 0.55312 | Gap: 0.33606 | Gamma: 0.0078\n",
      "Iter 290/5000 | Loss: 0.55248 | Gap: 0.32843 | Gamma: 0.0078\n",
      "Iter 291/5000 | Loss: 0.55166 | Gap: 0.30816 | Gamma: 0.0078\n",
      "Iter 292/5000 | Loss: 0.55093 | Gap: 0.29045 | Gamma: 0.0078\n",
      "Iter 293/5000 | Loss: 0.55014 | Gap: 0.29697 | Gamma: 0.0078\n",
      "Iter 294/5000 | Loss: 0.54927 | Gap: 0.27561 | Gamma: 0.0078\n",
      "Iter 295/5000 | Loss: 0.54846 | Gap: 0.27092 | Gamma: 0.0078\n",
      "Iter 296/5000 | Loss: 0.54752 | Gap: 0.26449 | Gamma: 0.0078\n",
      "Iter 297/5000 | Loss: 0.54675 | Gap: 0.26176 | Gamma: 0.0078\n",
      "Iter 298/5000 | Loss: 0.54595 | Gap: 0.27355 | Gamma: 0.0078\n",
      "Iter 299/5000 | Loss: 0.54497 | Gap: 0.25970 | Gamma: 0.0078\n",
      "Iter 300/5000 | Loss: 0.54407 | Gap: 0.24175 | Gamma: 0.0078\n",
      "Iter 301/5000 | Loss: 0.54344 | Gap: 0.26469 | Gamma: 0.0078\n",
      "Iter 302/5000 | Loss: 0.54246 | Gap: 0.26413 | Gamma: 0.0078\n",
      "Iter 303/5000 | Loss: 0.54206 | Gap: 0.21946 | Gamma: 0.0156\n",
      "Iter 304/5000 | Loss: 0.54127 | Gap: 0.64229 | Gamma: 0.0039\n",
      "Iter 305/5000 | Loss: 0.54104 | Gap: 0.19329 | Gamma: 0.0312\n",
      "Iter 306/5000 | Loss: 0.53896 | Gap: 0.58859 | Gamma: 0.0078\n",
      "Iter 307/5000 | Loss: 0.53794 | Gap: 0.29975 | Gamma: 0.0156\n",
      "Iter 308/5000 | Loss: 0.53722 | Gap: 0.44447 | Gamma: 0.0039\n",
      "Iter 309/5000 | Loss: 0.53560 | Gap: 0.21129 | Gamma: 0.0156\n",
      "Iter 310/5000 | Loss: 0.53486 | Gap: 0.32986 | Gamma: 0.0078\n",
      "Iter 311/5000 | Loss: 0.53415 | Gap: 0.24926 | Gamma: 0.0078\n",
      "Iter 312/5000 | Loss: 0.53381 | Gap: 0.26474 | Gamma: 0.0078\n",
      "Iter 313/5000 | Loss: 0.53357 | Gap: 0.31484 | Gamma: 0.0078\n",
      "Iter 314/5000 | Loss: 0.53303 | Gap: 0.27294 | Gamma: 0.0078\n",
      "Iter 315/5000 | Loss: 0.53298 | Gap: 0.29647 | Gamma: 0.0078\n",
      "Iter 316/5000 | Loss: 0.53278 | Gap: 0.31959 | Gamma: 0.0078\n",
      "Iter 317/5000 | Loss: 0.53273 | Gap: 0.31625 | Gamma: 0.0078\n",
      "Iter 318/5000 | Loss: 0.53271 | Gap: 0.32262 | Gamma: 0.0078\n",
      "Iter 319/5000 | Loss: 0.53205 | Gap: 0.33906 | Gamma: 0.0039\n",
      "Iter 320/5000 | Loss: 0.52726 | Gap: 0.16891 | Gamma: 0.0625\n",
      "Iter 321/5000 | Loss: 0.52534 | Gap: 0.65734 | Gamma: 0.0078\n",
      "Iter 322/5000 | Loss: 0.52496 | Gap: 0.30463 | Gamma: 0.0156\n",
      "Iter 323/5000 | Loss: 0.52383 | Gap: 0.30901 | Gamma: 0.0078\n",
      "Iter 324/5000 | Loss: 0.52298 | Gap: 0.21155 | Gamma: 0.0156\n",
      "Iter 325/5000 | Loss: 0.52230 | Gap: 0.36448 | Gamma: 0.0039\n",
      "Iter 326/5000 | Loss: 0.52186 | Gap: 0.23200 | Gamma: 0.0039\n",
      "Iter 327/5000 | Loss: 0.52099 | Gap: 0.18992 | Gamma: 0.0156\n",
      "Iter 328/5000 | Loss: 0.52040 | Gap: 0.44267 | Gamma: 0.0039\n",
      "Iter 329/5000 | Loss: 0.52026 | Gap: 0.19229 | Gamma: 0.0156\n",
      "Iter 330/5000 | Loss: 0.51892 | Gap: 0.35260 | Gamma: 0.0078\n",
      "Iter 331/5000 | Loss: 0.51821 | Gap: 0.21912 | Gamma: 0.0078\n",
      "Iter 332/5000 | Loss: 0.51782 | Gap: 0.24133 | Gamma: 0.0078\n",
      "Iter 333/5000 | Loss: 0.51767 | Gap: 0.26451 | Gamma: 0.0078\n",
      "Iter 334/5000 | Loss: 0.51713 | Gap: 0.29358 | Gamma: 0.0039\n",
      "Iter 335/5000 | Loss: 0.51514 | Gap: 0.15114 | Gamma: 0.0625\n",
      "Iter 336/5000 | Loss: 0.51230 | Gap: 0.78358 | Gamma: 0.0078\n",
      "Iter 337/5000 | Loss: 0.51118 | Gap: 0.31183 | Gamma: 0.0078\n",
      "Iter 338/5000 | Loss: 0.51040 | Gap: 0.23062 | Gamma: 0.0078\n",
      "Iter 339/5000 | Loss: 0.50998 | Gap: 0.23705 | Gamma: 0.0039\n",
      "Iter 340/5000 | Loss: 0.50940 | Gap: 0.17242 | Gamma: 0.0078\n",
      "Iter 341/5000 | Loss: 0.50895 | Gap: 0.22545 | Gamma: 0.0078\n",
      "Iter 342/5000 | Loss: 0.50829 | Gap: 0.19547 | Gamma: 0.0078\n",
      "Iter 343/5000 | Loss: 0.50792 | Gap: 0.22380 | Gamma: 0.0039\n",
      "Iter 344/5000 | Loss: 0.50761 | Gap: 0.15557 | Gamma: 0.0156\n",
      "Iter 345/5000 | Loss: 0.50685 | Gap: 0.39968 | Gamma: 0.0039\n",
      "Iter 346/5000 | Loss: 0.50604 | Gap: 0.17652 | Gamma: 0.0156\n",
      "Iter 347/5000 | Loss: 0.50563 | Gap: 0.28088 | Gamma: 0.0078\n",
      "Iter 348/5000 | Loss: 0.50554 | Gap: 0.22736 | Gamma: 0.0078\n",
      "Iter 349/5000 | Loss: 0.50512 | Gap: 0.28585 | Gamma: 0.0039\n",
      "Iter 350/5000 | Loss: 0.50508 | Gap: 0.13493 | Gamma: 0.0625\n",
      "Iter 351/5000 | Loss: 0.50313 | Gap: 1.09840 | Gamma: 0.0039\n",
      "Iter 352/5000 | Loss: 0.50191 | Gap: 0.49367 | Gamma: 0.0078\n",
      "Iter 353/5000 | Loss: 0.50134 | Gap: 0.32446 | Gamma: 0.0039\n",
      "Iter 354/5000 | Loss: 0.50122 | Gap: 0.16435 | Gamma: 0.0156\n",
      "Iter 355/5000 | Loss: 0.50053 | Gap: 0.39138 | Gamma: 0.0039\n",
      "Iter 356/5000 | Loss: 0.49990 | Gap: 0.16397 | Gamma: 0.0156\n",
      "Iter 357/5000 | Loss: 0.49956 | Gap: 0.28377 | Gamma: 0.0078\n",
      "Iter 358/5000 | Loss: 0.49941 | Gap: 0.21779 | Gamma: 0.0078\n",
      "Iter 359/5000 | Loss: 0.49905 | Gap: 0.26189 | Gamma: 0.0039\n",
      "Iter 360/5000 | Loss: 0.49884 | Gap: 0.14804 | Gamma: 0.0078\n",
      "Iter 361/5000 | Loss: 0.49833 | Gap: 0.21508 | Gamma: 0.0078\n",
      "Iter 362/5000 | Loss: 0.49832 | Gap: 0.15294 | Gamma: 0.0312\n",
      "Iter 363/5000 | Loss: 0.49755 | Gap: 0.68316 | Gamma: 0.0039\n",
      "Iter 364/5000 | Loss: 0.49698 | Gap: 0.29729 | Gamma: 0.0039\n",
      "Iter 365/5000 | Loss: 0.49634 | Gap: 0.17380 | Gamma: 0.0156\n",
      "Iter 366/5000 | Loss: 0.49599 | Gap: 0.29646 | Gamma: 0.0039\n",
      "Iter 367/5000 | Loss: 0.49598 | Gap: 0.19729 | Gamma: 0.0078\n",
      "Iter 368/5000 | Loss: 0.49569 | Gap: 0.22974 | Gamma: 0.0078\n",
      "Iter 369/5000 | Loss: 0.49528 | Gap: 0.21768 | Gamma: 0.0039\n",
      "Iter 370/5000 | Loss: 0.49427 | Gap: 0.13158 | Gamma: 0.0156\n",
      "Iter 371/5000 | Loss: 0.49377 | Gap: 0.28512 | Gamma: 0.0039\n",
      "Iter 372/5000 | Loss: 0.49313 | Gap: 0.12120 | Gamma: 0.0625\n",
      "Iter 373/5000 | Loss: 0.49138 | Gap: 0.64179 | Gamma: 0.0078\n",
      "Iter 374/5000 | Loss: 0.49048 | Gap: 0.27935 | Gamma: 0.0078\n",
      "Iter 375/5000 | Loss: 0.49025 | Gap: 0.20251 | Gamma: 0.0078\n",
      "Iter 376/5000 | Loss: 0.48984 | Gap: 0.21075 | Gamma: 0.0039\n",
      "Iter 377/5000 | Loss: 0.48980 | Gap: 0.12899 | Gamma: 0.0156\n",
      "Iter 378/5000 | Loss: 0.48976 | Gap: 0.32312 | Gamma: 0.0078\n",
      "Iter 379/5000 | Loss: 0.48942 | Gap: 0.27627 | Gamma: 0.0039\n",
      "Iter 380/5000 | Loss: 0.48910 | Gap: 0.13600 | Gamma: 0.0156\n",
      "Iter 381/5000 | Loss: 0.48855 | Gap: 0.37240 | Gamma: 0.0039\n",
      "Iter 382/5000 | Loss: 0.48832 | Gap: 0.14139 | Gamma: 0.0156\n",
      "Iter 383/5000 | Loss: 0.48813 | Gap: 0.31210 | Gamma: 0.0078\n",
      "Iter 384/5000 | Loss: 0.48780 | Gap: 0.22125 | Gamma: 0.0039\n",
      "Iter 385/5000 | Loss: 0.48732 | Gap: 0.13521 | Gamma: 0.0078\n",
      "Iter 386/5000 | Loss: 0.48698 | Gap: 0.17988 | Gamma: 0.0078\n",
      "Iter 387/5000 | Loss: 0.48649 | Gap: 0.16233 | Gamma: 0.0078\n",
      "Iter 388/5000 | Loss: 0.48645 | Gap: 0.17167 | Gamma: 0.0078\n",
      "Iter 389/5000 | Loss: 0.48608 | Gap: 0.23529 | Gamma: 0.0039\n",
      "Iter 390/5000 | Loss: 0.48578 | Gap: 0.12313 | Gamma: 0.0156\n",
      "Iter 391/5000 | Loss: 0.48515 | Gap: 0.32428 | Gamma: 0.0039\n",
      "Iter 392/5000 | Loss: 0.48416 | Gap: 0.13408 | Gamma: 0.0156\n",
      "Iter 393/5000 | Loss: 0.48361 | Gap: 0.17639 | Gamma: 0.0078\n",
      "Iter 394/5000 | Loss: 0.48316 | Gap: 0.13122 | Gamma: 0.0156\n",
      "Iter 395/5000 | Loss: 0.48275 | Gap: 0.28131 | Gamma: 0.0039\n",
      "Iter 396/5000 | Loss: 0.48262 | Gap: 0.14141 | Gamma: 0.0078\n",
      "Iter 397/5000 | Loss: 0.48222 | Gap: 0.20449 | Gamma: 0.0039\n",
      "Iter 398/5000 | Loss: 0.48123 | Gap: 0.11032 | Gamma: 0.0312\n",
      "Iter 399/5000 | Loss: 0.48055 | Gap: 0.43731 | Gamma: 0.0039\n",
      "Iter 400/5000 | Loss: 0.48046 | Gap: 0.15659 | Gamma: 0.0078\n",
      "Iter 401/5000 | Loss: 0.48013 | Gap: 0.20332 | Gamma: 0.0039\n",
      "Iter 402/5000 | Loss: 0.48009 | Gap: 0.10524 | Gamma: 0.0312\n",
      "Iter 403/5000 | Loss: 0.47949 | Gap: 0.59578 | Gamma: 0.0039\n",
      "Iter 404/5000 | Loss: 0.47908 | Gap: 0.22168 | Gamma: 0.0039\n",
      "Iter 405/5000 | Loss: 0.47862 | Gap: 0.12921 | Gamma: 0.0078\n",
      "Iter 406/5000 | Loss: 0.47854 | Gap: 0.14491 | Gamma: 0.0078\n",
      "Iter 407/5000 | Loss: 0.47802 | Gap: 0.15310 | Gamma: 0.0078\n",
      "Iter 408/5000 | Loss: 0.47745 | Gap: 0.11267 | Gamma: 0.0156\n",
      "Iter 409/5000 | Loss: 0.47701 | Gap: 0.20547 | Gamma: 0.0078\n",
      "Iter 410/5000 | Loss: 0.47667 | Gap: 0.12126 | Gamma: 0.0156\n",
      "Iter 411/5000 | Loss: 0.47620 | Gap: 0.25429 | Gamma: 0.0039\n",
      "Iter 412/5000 | Loss: 0.47603 | Gap: 0.13810 | Gamma: 0.0078\n",
      "Iter 413/5000 | Loss: 0.47573 | Gap: 0.20855 | Gamma: 0.0039\n",
      "Iter 414/5000 | Loss: 0.47497 | Gap: 0.10473 | Gamma: 0.0156\n",
      "Iter 415/5000 | Loss: 0.47469 | Gap: 0.23887 | Gamma: 0.0039\n",
      "Iter 416/5000 | Loss: 0.47408 | Gap: 0.09533 | Gamma: 0.0156\n",
      "Iter 417/5000 | Loss: 0.47378 | Gap: 0.24981 | Gamma: 0.0039\n",
      "Iter 418/5000 | Loss: 0.47318 | Gap: 0.09476 | Gamma: 0.0156\n",
      "Iter 419/5000 | Loss: 0.47283 | Gap: 0.21698 | Gamma: 0.0039\n",
      "Iter 420/5000 | Loss: 0.47246 | Gap: 0.09833 | Gamma: 0.0078\n",
      "Iter 421/5000 | Loss: 0.47220 | Gap: 0.14004 | Gamma: 0.0039\n",
      "Iter 422/5000 | Loss: 0.47163 | Gap: 0.09199 | Gamma: 0.0156\n",
      "Iter 423/5000 | Loss: 0.47132 | Gap: 0.22175 | Gamma: 0.0039\n",
      "Iter 424/5000 | Loss: 0.47094 | Gap: 0.10331 | Gamma: 0.0078\n",
      "Iter 425/5000 | Loss: 0.47067 | Gap: 0.12493 | Gamma: 0.0078\n",
      "Iter 426/5000 | Loss: 0.47047 | Gap: 0.13088 | Gamma: 0.0078\n",
      "Iter 427/5000 | Loss: 0.47020 | Gap: 0.14882 | Gamma: 0.0039\n",
      "Iter 428/5000 | Loss: 0.46989 | Gap: 0.09867 | Gamma: 0.0078\n",
      "Iter 429/5000 | Loss: 0.46964 | Gap: 0.15285 | Gamma: 0.0039\n",
      "Iter 430/5000 | Loss: 0.46933 | Gap: 0.08498 | Gamma: 0.0312\n",
      "Iter 431/5000 | Loss: 0.46918 | Gap: 0.53784 | Gamma: 0.0039\n",
      "Iter 432/5000 | Loss: 0.46896 | Gap: 0.20454 | Gamma: 0.0039\n",
      "Iter 433/5000 | Loss: 0.46878 | Gap: 0.08896 | Gamma: 0.0156\n",
      "Iter 434/5000 | Loss: 0.46860 | Gap: 0.21831 | Gamma: 0.0078\n",
      "Iter 435/5000 | Loss: 0.46837 | Gap: 0.11798 | Gamma: 0.0078\n",
      "Iter 436/5000 | Loss: 0.46819 | Gap: 0.15734 | Gamma: 0.0039\n",
      "Iter 437/5000 | Loss: 0.46801 | Gap: 0.10829 | Gamma: 0.0039\n",
      "Iter 438/5000 | Loss: 0.46775 | Gap: 0.09838 | Gamma: 0.0078\n",
      "Iter 439/5000 | Loss: 0.46749 | Gap: 0.14172 | Gamma: 0.0039\n",
      "Iter 440/5000 | Loss: 0.46708 | Gap: 0.07983 | Gamma: 0.0312\n",
      "Iter 441/5000 | Loss: 0.46682 | Gap: 0.35933 | Gamma: 0.0078\n",
      "Iter 442/5000 | Loss: 0.46666 | Gap: 0.11869 | Gamma: 0.0078\n",
      "Iter 443/5000 | Loss: 0.46642 | Gap: 0.16359 | Gamma: 0.0039\n",
      "Iter 444/5000 | Loss: 0.46637 | Gap: 0.10355 | Gamma: 0.0078\n",
      "Iter 445/5000 | Loss: 0.46612 | Gap: 0.18378 | Gamma: 0.0039\n",
      "Iter 446/5000 | Loss: 0.46579 | Gap: 0.09436 | Gamma: 0.0078\n",
      "Iter 447/5000 | Loss: 0.46561 | Gap: 0.12496 | Gamma: 0.0078\n",
      "Iter 448/5000 | Loss: 0.46533 | Gap: 0.12021 | Gamma: 0.0078\n",
      "Iter 449/5000 | Loss: 0.46514 | Gap: 0.12636 | Gamma: 0.0039\n",
      "Iter 450/5000 | Loss: 0.46510 | Gap: 0.10076 | Gamma: 0.0078\n",
      "Iter 451/5000 | Loss: 0.46490 | Gap: 0.15132 | Gamma: 0.0078\n",
      "Iter 452/5000 | Loss: 0.46468 | Gap: 0.12788 | Gamma: 0.0039\n",
      "Iter 453/5000 | Loss: 0.46439 | Gap: 0.09318 | Gamma: 0.0078\n",
      "Iter 454/5000 | Loss: 0.46417 | Gap: 0.13674 | Gamma: 0.0039\n",
      "Iter 455/5000 | Loss: 0.46376 | Gap: 0.07903 | Gamma: 0.0156\n",
      "Iter 456/5000 | Loss: 0.46346 | Gap: 0.20959 | Gamma: 0.0039\n",
      "Iter 457/5000 | Loss: 0.46317 | Gap: 0.09225 | Gamma: 0.0078\n",
      "Iter 458/5000 | Loss: 0.46294 | Gap: 0.13441 | Gamma: 0.0039\n",
      "Iter 459/5000 | Loss: 0.46283 | Gap: 0.07288 | Gamma: 0.0312\n",
      "Iter 460/5000 | Loss: 0.46232 | Gap: 0.43625 | Gamma: 0.0039\n",
      "Iter 461/5000 | Loss: 0.46211 | Gap: 0.15560 | Gamma: 0.0039\n",
      "Iter 462/5000 | Loss: 0.46191 | Gap: 0.10874 | Gamma: 0.0039\n",
      "Iter 463/5000 | Loss: 0.46188 | Gap: 0.08514 | Gamma: 0.0078\n",
      "Iter 464/5000 | Loss: 0.46172 | Gap: 0.14254 | Gamma: 0.0078\n",
      "Iter 465/5000 | Loss: 0.46136 | Gap: 0.10111 | Gamma: 0.0078\n",
      "Iter 466/5000 | Loss: 0.46115 | Gap: 0.11004 | Gamma: 0.0039\n",
      "Iter 467/5000 | Loss: 0.46062 | Gap: 0.07771 | Gamma: 0.0156\n",
      "Iter 468/5000 | Loss: 0.46030 | Gap: 0.17138 | Gamma: 0.0039\n",
      "Iter 469/5000 | Loss: 0.45989 | Gap: 0.07880 | Gamma: 0.0156\n",
      "Iter 470/5000 | Loss: 0.45955 | Gap: 0.17698 | Gamma: 0.0039\n",
      "Iter 471/5000 | Loss: 0.45892 | Gap: 0.07507 | Gamma: 0.0312\n",
      "Iter 472/5000 | Loss: 0.45860 | Gap: 0.20155 | Gamma: 0.0078\n",
      "Iter 473/5000 | Loss: 0.45848 | Gap: 0.11895 | Gamma: 0.0039\n",
      "Iter 474/5000 | Loss: 0.45836 | Gap: 0.10296 | Gamma: 0.0039\n",
      "Iter 475/5000 | Loss: 0.45823 | Gap: 0.10248 | Gamma: 0.0039\n",
      "Iter 476/5000 | Loss: 0.45812 | Gap: 0.08576 | Gamma: 0.0078\n",
      "Iter 477/5000 | Loss: 0.45794 | Gap: 0.14263 | Gamma: 0.0039\n",
      "Iter 478/5000 | Loss: 0.45778 | Gap: 0.07578 | Gamma: 0.0078\n",
      "Iter 479/5000 | Loss: 0.45764 | Gap: 0.14690 | Gamma: 0.0039\n",
      "Iter 480/5000 | Loss: 0.45739 | Gap: 0.07345 | Gamma: 0.0078\n",
      "Iter 481/5000 | Loss: 0.45720 | Gap: 0.11263 | Gamma: 0.0039\n",
      "Iter 482/5000 | Loss: 0.45677 | Gap: 0.06620 | Gamma: 0.0156\n",
      "Iter 483/5000 | Loss: 0.45648 | Gap: 0.16851 | Gamma: 0.0039\n",
      "Iter 484/5000 | Loss: 0.45638 | Gap: 0.07042 | Gamma: 0.0156\n",
      "Iter 485/5000 | Loss: 0.45633 | Gap: 0.19184 | Gamma: 0.0078\n",
      "Iter 486/5000 | Loss: 0.45616 | Gap: 0.11166 | Gamma: 0.0078\n",
      "Iter 487/5000 | Loss: 0.45597 | Gap: 0.11586 | Gamma: 0.0039\n",
      "Iter 488/5000 | Loss: 0.45589 | Gap: 0.09230 | Gamma: 0.0039\n",
      "Iter 489/5000 | Loss: 0.45588 | Gap: 0.09355 | Gamma: 0.0078\n",
      "Iter 490/5000 | Loss: 0.45576 | Gap: 0.15875 | Gamma: 0.0039\n",
      "Iter 491/5000 | Loss: 0.45574 | Gap: 0.08567 | Gamma: 0.0078\n",
      "Iter 492/5000 | Loss: 0.45559 | Gap: 0.16150 | Gamma: 0.0039\n",
      "Iter 493/5000 | Loss: 0.45534 | Gap: 0.07274 | Gamma: 0.0078\n",
      "Iter 494/5000 | Loss: 0.45520 | Gap: 0.11549 | Gamma: 0.0039\n",
      "Iter 495/5000 | Loss: 0.45485 | Gap: 0.06764 | Gamma: 0.0156\n",
      "Iter 496/5000 | Loss: 0.45467 | Gap: 0.17773 | Gamma: 0.0039\n",
      "Iter 497/5000 | Loss: 0.45451 | Gap: 0.07986 | Gamma: 0.0078\n",
      "Iter 498/5000 | Loss: 0.45430 | Gap: 0.11536 | Gamma: 0.0039\n",
      "Iter 499/5000 | Loss: 0.45390 | Gap: 0.06416 | Gamma: 0.0156\n",
      "Iter 500/5000 | Loss: 0.45365 | Gap: 0.17012 | Gamma: 0.0039\n",
      "Iter 501/5000 | Loss: 0.45355 | Gap: 0.07029 | Gamma: 0.0156\n",
      "Iter 502/5000 | Loss: 0.45339 | Gap: 0.17114 | Gamma: 0.0078\n",
      "Iter 503/5000 | Loss: 0.45314 | Gap: 0.08543 | Gamma: 0.0078\n",
      "Iter 504/5000 | Loss: 0.45300 | Gap: 0.09045 | Gamma: 0.0078\n",
      "Iter 505/5000 | Loss: 0.45294 | Gap: 0.10205 | Gamma: 0.0078\n",
      "Iter 506/5000 | Loss: 0.45292 | Gap: 0.14263 | Gamma: 0.0039\n",
      "Iter 507/5000 | Loss: 0.45280 | Gap: 0.10247 | Gamma: 0.0039\n",
      "Iter 508/5000 | Loss: 0.45267 | Gap: 0.07665 | Gamma: 0.0078\n",
      "Iter 509/5000 | Loss: 0.45252 | Gap: 0.12618 | Gamma: 0.0039\n",
      "Iter 510/5000 | Loss: 0.45215 | Gap: 0.06108 | Gamma: 0.0156\n",
      "Iter 511/5000 | Loss: 0.45211 | Gap: 0.19146 | Gamma: 0.0039\n",
      "Iter 512/5000 | Loss: 0.45200 | Gap: 0.08158 | Gamma: 0.0039\n",
      "Iter 513/5000 | Loss: 0.45191 | Gap: 0.07531 | Gamma: 0.0039\n",
      "Iter 514/5000 | Loss: 0.45184 | Gap: 0.07726 | Gamma: 0.0078\n",
      "Iter 515/5000 | Loss: 0.45173 | Gap: 0.10815 | Gamma: 0.0078\n",
      "Iter 516/5000 | Loss: 0.45141 | Gap: 0.08629 | Gamma: 0.0078\n",
      "Iter 517/5000 | Loss: 0.45120 | Gap: 0.07656 | Gamma: 0.0078\n",
      "Iter 518/5000 | Loss: 0.45105 | Gap: 0.09033 | Gamma: 0.0078\n",
      "Iter 519/5000 | Loss: 0.45079 | Gap: 0.08737 | Gamma: 0.0078\n",
      "Iter 520/5000 | Loss: 0.45067 | Gap: 0.08249 | Gamma: 0.0078\n",
      "Iter 521/5000 | Loss: 0.45050 | Gap: 0.10829 | Gamma: 0.0039\n",
      "Iter 522/5000 | Loss: 0.45047 | Gap: 0.07167 | Gamma: 0.0078\n",
      "Iter 523/5000 | Loss: 0.45033 | Gap: 0.14591 | Gamma: 0.0039\n",
      "Iter 524/5000 | Loss: 0.45019 | Gap: 0.07129 | Gamma: 0.0078\n",
      "Iter 525/5000 | Loss: 0.45005 | Gap: 0.12067 | Gamma: 0.0039\n",
      "Iter 526/5000 | Loss: 0.44973 | Gap: 0.05908 | Gamma: 0.0156\n",
      "Iter 527/5000 | Loss: 0.44973 | Gap: 0.18797 | Gamma: 0.0039\n",
      "Iter 528/5000 | Loss: 0.44964 | Gap: 0.07655 | Gamma: 0.0039\n",
      "Iter 529/5000 | Loss: 0.44956 | Gap: 0.07376 | Gamma: 0.0039\n",
      "Iter 530/5000 | Loss: 0.44945 | Gap: 0.07227 | Gamma: 0.0078\n",
      "Iter 531/5000 | Loss: 0.44928 | Gap: 0.10029 | Gamma: 0.0039\n",
      "Iter 532/5000 | Loss: 0.44911 | Gap: 0.05909 | Gamma: 0.0156\n",
      "Iter 533/5000 | Loss: 0.44882 | Gap: 0.16710 | Gamma: 0.0039\n",
      "Iter 534/5000 | Loss: 0.44868 | Gap: 0.06645 | Gamma: 0.0156\n",
      "Iter 535/5000 | Loss: 0.44840 | Gap: 0.14873 | Gamma: 0.0039\n",
      "Iter 536/5000 | Loss: 0.44830 | Gap: 0.06527 | Gamma: 0.0156\n",
      "Iter 537/5000 | Loss: 0.44824 | Gap: 0.12696 | Gamma: 0.0078\n",
      "Iter 538/5000 | Loss: 0.44802 | Gap: 0.09941 | Gamma: 0.0078\n",
      "Iter 539/5000 | Loss: 0.44780 | Gap: 0.07586 | Gamma: 0.0078\n",
      "Iter 540/5000 | Loss: 0.44767 | Gap: 0.08678 | Gamma: 0.0039\n",
      "Iter 541/5000 | Loss: 0.44752 | Gap: 0.06064 | Gamma: 0.0078\n",
      "Iter 542/5000 | Loss: 0.44751 | Gap: 0.11889 | Gamma: 0.0039\n",
      "Iter 543/5000 | Loss: 0.44743 | Gap: 0.08461 | Gamma: 0.0039\n",
      "Iter 544/5000 | Loss: 0.44732 | Gap: 0.07199 | Gamma: 0.0039\n",
      "Iter 545/5000 | Loss: 0.44727 | Gap: 0.06279 | Gamma: 0.0078\n",
      "Iter 546/5000 | Loss: 0.44712 | Gap: 0.12025 | Gamma: 0.0039\n",
      "Iter 547/5000 | Loss: 0.44692 | Gap: 0.05337 | Gamma: 0.0156\n",
      "Iter 548/5000 | Loss: 0.44684 | Gap: 0.18456 | Gamma: 0.0039\n",
      "Iter 549/5000 | Loss: 0.44676 | Gap: 0.07386 | Gamma: 0.0078\n",
      "Iter 550/5000 | Loss: 0.44673 | Gap: 0.12148 | Gamma: 0.0039\n",
      "Iter 551/5000 | Loss: 0.44663 | Gap: 0.08994 | Gamma: 0.0039\n",
      "Iter 552/5000 | Loss: 0.44657 | Gap: 0.06701 | Gamma: 0.0078\n",
      "Iter 553/5000 | Loss: 0.44650 | Gap: 0.12966 | Gamma: 0.0039\n",
      "Iter 554/5000 | Loss: 0.44641 | Gap: 0.06339 | Gamma: 0.0078\n",
      "Iter 555/5000 | Loss: 0.44631 | Gap: 0.11232 | Gamma: 0.0039\n",
      "Iter 556/5000 | Loss: 0.44619 | Gap: 0.06067 | Gamma: 0.0039\n",
      "Iter 557/5000 | Loss: 0.44607 | Gap: 0.06481 | Gamma: 0.0039\n",
      "Iter 558/5000 | Loss: 0.44594 | Gap: 0.05611 | Gamma: 0.0078\n",
      "Iter 559/5000 | Loss: 0.44578 | Gap: 0.08721 | Gamma: 0.0039\n",
      "Iter 560/5000 | Loss: 0.44562 | Gap: 0.04988 | Gamma: 0.0156\n",
      "Iter 561/5000 | Loss: 0.44560 | Gap: 0.18473 | Gamma: 0.0039\n",
      "Iter 562/5000 | Loss: 0.44551 | Gap: 0.05983 | Gamma: 0.0039\n",
      "Iter 563/5000 | Loss: 0.44542 | Gap: 0.06005 | Gamma: 0.0039\n",
      "Iter 564/5000 | Loss: 0.44534 | Gap: 0.05421 | Gamma: 0.0078\n",
      "Iter 565/5000 | Loss: 0.44523 | Gap: 0.09232 | Gamma: 0.0039\n",
      "Iter 566/5000 | Loss: 0.44516 | Gap: 0.06641 | Gamma: 0.0020\n",
      "Iter 567/5000 | Loss: 0.44483 | Gap: 0.04604 | Gamma: 0.0156\n",
      "Iter 568/5000 | Loss: 0.44474 | Gap: 0.13353 | Gamma: 0.0039\n",
      "Iter 569/5000 | Loss: 0.44460 | Gap: 0.04761 | Gamma: 0.0078\n",
      "Iter 570/5000 | Loss: 0.44456 | Gap: 0.10826 | Gamma: 0.0039\n",
      "Iter 571/5000 | Loss: 0.44440 | Gap: 0.05375 | Gamma: 0.0156\n",
      "Iter 572/5000 | Loss: 0.44428 | Gap: 0.15009 | Gamma: 0.0039\n",
      "Iter 573/5000 | Loss: 0.44418 | Gap: 0.07433 | Gamma: 0.0039\n",
      "Iter 574/5000 | Loss: 0.44405 | Gap: 0.05679 | Gamma: 0.0078\n",
      "Iter 575/5000 | Loss: 0.44391 | Gap: 0.08608 | Gamma: 0.0039\n",
      "Iter 576/5000 | Loss: 0.44377 | Gap: 0.05142 | Gamma: 0.0078\n",
      "Iter 577/5000 | Loss: 0.44369 | Gap: 0.09601 | Gamma: 0.0039\n",
      "Iter 578/5000 | Loss: 0.44365 | Gap: 0.06401 | Gamma: 0.0039\n",
      "Iter 579/5000 | Loss: 0.44359 | Gap: 0.07420 | Gamma: 0.0039\n",
      "Iter 580/5000 | Loss: 0.44347 | Gap: 0.06132 | Gamma: 0.0039\n",
      "Iter 581/5000 | Loss: 0.44340 | Gap: 0.05230 | Gamma: 0.0078\n",
      "Iter 582/5000 | Loss: 0.44328 | Gap: 0.10099 | Gamma: 0.0039\n",
      "Iter 583/5000 | Loss: 0.44303 | Gap: 0.04450 | Gamma: 0.0156\n",
      "Iter 584/5000 | Loss: 0.44294 | Gap: 0.14348 | Gamma: 0.0039\n",
      "Iter 585/5000 | Loss: 0.44292 | Gap: 0.05902 | Gamma: 0.0078\n",
      "Iter 586/5000 | Loss: 0.44277 | Gap: 0.10127 | Gamma: 0.0039\n",
      "Iter 587/5000 | Loss: 0.44261 | Gap: 0.05008 | Gamma: 0.0078\n",
      "Iter 588/5000 | Loss: 0.44256 | Gap: 0.09063 | Gamma: 0.0039\n",
      "Iter 589/5000 | Loss: 0.44245 | Gap: 0.05818 | Gamma: 0.0039\n",
      "Iter 590/5000 | Loss: 0.44236 | Gap: 0.05435 | Gamma: 0.0039\n",
      "Iter 591/5000 | Loss: 0.44231 | Gap: 0.06138 | Gamma: 0.0039\n",
      "Iter 592/5000 | Loss: 0.44222 | Gap: 0.06063 | Gamma: 0.0039\n",
      "Iter 593/5000 | Loss: 0.44219 | Gap: 0.05075 | Gamma: 0.0078\n",
      "Iter 594/5000 | Loss: 0.44217 | Gap: 0.10567 | Gamma: 0.0039\n",
      "Iter 595/5000 | Loss: 0.44214 | Gap: 0.06341 | Gamma: 0.0039\n",
      "Iter 596/5000 | Loss: 0.44208 | Gap: 0.06528 | Gamma: 0.0039\n",
      "Iter 597/5000 | Loss: 0.44199 | Gap: 0.05666 | Gamma: 0.0039\n",
      "Iter 598/5000 | Loss: 0.44197 | Gap: 0.05251 | Gamma: 0.0078\n",
      "Iter 599/5000 | Loss: 0.44180 | Gap: 0.09203 | Gamma: 0.0039\n",
      "Iter 600/5000 | Loss: 0.44164 | Gap: 0.04526 | Gamma: 0.0078\n",
      "Iter 601/5000 | Loss: 0.44164 | Gap: 0.08885 | Gamma: 0.0039\n",
      "Iter 602/5000 | Loss: 0.44151 | Gap: 0.05432 | Gamma: 0.0078\n",
      "Iter 603/5000 | Loss: 0.44142 | Gap: 0.07813 | Gamma: 0.0039\n",
      "Iter 604/5000 | Loss: 0.44139 | Gap: 0.04939 | Gamma: 0.0078\n",
      "Iter 605/5000 | Loss: 0.44131 | Gap: 0.09557 | Gamma: 0.0039\n",
      "Iter 606/5000 | Loss: 0.44128 | Gap: 0.05822 | Gamma: 0.0039\n",
      "Iter 607/5000 | Loss: 0.44122 | Gap: 0.06529 | Gamma: 0.0039\n",
      "Iter 608/5000 | Loss: 0.44120 | Gap: 0.05165 | Gamma: 0.0078\n",
      "Iter 609/5000 | Loss: 0.44110 | Gap: 0.09404 | Gamma: 0.0039\n",
      "Iter 610/5000 | Loss: 0.44098 | Gap: 0.04266 | Gamma: 0.0078\n",
      "Iter 611/5000 | Loss: 0.44093 | Gap: 0.08837 | Gamma: 0.0039\n",
      "Iter 612/5000 | Loss: 0.44085 | Gap: 0.05353 | Gamma: 0.0039\n",
      "Iter 613/5000 | Loss: 0.44078 | Gap: 0.05513 | Gamma: 0.0039\n",
      "Iter 614/5000 | Loss: 0.44068 | Gap: 0.05548 | Gamma: 0.0039\n",
      "Iter 615/5000 | Loss: 0.44059 | Gap: 0.04598 | Gamma: 0.0078\n",
      "Iter 616/5000 | Loss: 0.44052 | Gap: 0.08640 | Gamma: 0.0039\n",
      "Iter 617/5000 | Loss: 0.44048 | Gap: 0.04605 | Gamma: 0.0078\n",
      "Iter 618/5000 | Loss: 0.44042 | Gap: 0.09930 | Gamma: 0.0039\n",
      "Iter 619/5000 | Loss: 0.44035 | Gap: 0.05346 | Gamma: 0.0039\n",
      "Iter 620/5000 | Loss: 0.44029 | Gap: 0.05786 | Gamma: 0.0039\n",
      "Iter 621/5000 | Loss: 0.44022 | Gap: 0.05672 | Gamma: 0.0039\n",
      "Iter 622/5000 | Loss: 0.44012 | Gap: 0.05187 | Gamma: 0.0039\n",
      "Iter 623/5000 | Loss: 0.44010 | Gap: 0.04469 | Gamma: 0.0078\n",
      "Iter 624/5000 | Loss: 0.44000 | Gap: 0.09365 | Gamma: 0.0039\n",
      "Iter 625/5000 | Loss: 0.43994 | Gap: 0.04733 | Gamma: 0.0078\n",
      "Iter 626/5000 | Loss: 0.43986 | Gap: 0.09051 | Gamma: 0.0039\n",
      "Iter 627/5000 | Loss: 0.43971 | Gap: 0.04366 | Gamma: 0.0078\n",
      "Iter 628/5000 | Loss: 0.43967 | Gap: 0.06906 | Gamma: 0.0039\n",
      "Iter 629/5000 | Loss: 0.43963 | Gap: 0.05466 | Gamma: 0.0039\n",
      "Iter 630/5000 | Loss: 0.43959 | Gap: 0.05955 | Gamma: 0.0039\n",
      "Iter 631/5000 | Loss: 0.43955 | Gap: 0.05951 | Gamma: 0.0039\n",
      "Iter 632/5000 | Loss: 0.43945 | Gap: 0.05265 | Gamma: 0.0039\n",
      "Iter 633/5000 | Loss: 0.43932 | Gap: 0.03953 | Gamma: 0.0078\n",
      "Iter 634/5000 | Loss: 0.43922 | Gap: 0.06463 | Gamma: 0.0039\n",
      "Iter 635/5000 | Loss: 0.43914 | Gap: 0.03931 | Gamma: 0.0078\n",
      "Iter 636/5000 | Loss: 0.43911 | Gap: 0.08417 | Gamma: 0.0039\n",
      "Iter 637/5000 | Loss: 0.43909 | Gap: 0.04665 | Gamma: 0.0078\n",
      "Iter 638/5000 | Loss: 0.43905 | Gap: 0.09005 | Gamma: 0.0039\n",
      "Iter 639/5000 | Loss: 0.43903 | Gap: 0.05474 | Gamma: 0.0039\n",
      "Iter 640/5000 | Loss: 0.43895 | Gap: 0.05968 | Gamma: 0.0039\n",
      "Iter 641/5000 | Loss: 0.43887 | Gap: 0.04534 | Gamma: 0.0039\n",
      "Iter 642/5000 | Loss: 0.43880 | Gap: 0.04594 | Gamma: 0.0039\n",
      "Iter 643/5000 | Loss: 0.43876 | Gap: 0.04811 | Gamma: 0.0039\n",
      "Iter 644/5000 | Loss: 0.43871 | Gap: 0.05263 | Gamma: 0.0039\n",
      "Iter 645/5000 | Loss: 0.43863 | Gap: 0.04748 | Gamma: 0.0039\n",
      "Iter 646/5000 | Loss: 0.43856 | Gap: 0.03925 | Gamma: 0.0078\n",
      "Iter 647/5000 | Loss: 0.43852 | Gap: 0.07893 | Gamma: 0.0039\n",
      "Iter 648/5000 | Loss: 0.43851 | Gap: 0.04205 | Gamma: 0.0078\n",
      "Iter 649/5000 | Loss: 0.43841 | Gap: 0.08480 | Gamma: 0.0039\n",
      "Iter 650/5000 | Loss: 0.43830 | Gap: 0.03992 | Gamma: 0.0078\n",
      "Iter 651/5000 | Loss: 0.43826 | Gap: 0.07136 | Gamma: 0.0039\n",
      "Iter 652/5000 | Loss: 0.43819 | Gap: 0.04582 | Gamma: 0.0039\n",
      "Iter 653/5000 | Loss: 0.43811 | Gap: 0.04739 | Gamma: 0.0039\n",
      "Iter 654/5000 | Loss: 0.43809 | Gap: 0.04214 | Gamma: 0.0078\n",
      "Iter 655/5000 | Loss: 0.43801 | Gap: 0.10326 | Gamma: 0.0020\n",
      "Iter 656/5000 | Loss: 0.43793 | Gap: 0.04616 | Gamma: 0.0039\n",
      "Iter 657/5000 | Loss: 0.43786 | Gap: 0.04590 | Gamma: 0.0039\n",
      "Iter 658/5000 | Loss: 0.43779 | Gap: 0.04718 | Gamma: 0.0039\n",
      "Iter 659/5000 | Loss: 0.43773 | Gap: 0.04437 | Gamma: 0.0039\n",
      "Iter 660/5000 | Loss: 0.43770 | Gap: 0.04659 | Gamma: 0.0039\n",
      "Iter 661/5000 | Loss: 0.43763 | Gap: 0.05349 | Gamma: 0.0039\n",
      "Iter 662/5000 | Loss: 0.43757 | Gap: 0.04328 | Gamma: 0.0039\n",
      "Iter 663/5000 | Loss: 0.43751 | Gap: 0.04611 | Gamma: 0.0039\n",
      "Iter 664/5000 | Loss: 0.43744 | Gap: 0.04615 | Gamma: 0.0039\n",
      "Iter 665/5000 | Loss: 0.43737 | Gap: 0.04237 | Gamma: 0.0039\n",
      "Iter 666/5000 | Loss: 0.43735 | Gap: 0.04768 | Gamma: 0.0039\n",
      "Iter 667/5000 | Loss: 0.43730 | Gap: 0.05899 | Gamma: 0.0020\n",
      "Iter 668/5000 | Loss: 0.43717 | Gap: 0.03178 | Gamma: 0.0156\n",
      "Iter 669/5000 | Loss: 0.43710 | Gap: 0.12718 | Gamma: 0.0039\n",
      "Iter 670/5000 | Loss: 0.43707 | Gap: 0.04126 | Gamma: 0.0078\n",
      "Iter 671/5000 | Loss: 0.43701 | Gap: 0.07308 | Gamma: 0.0039\n",
      "Iter 672/5000 | Loss: 0.43692 | Gap: 0.03626 | Gamma: 0.0078\n",
      "Iter 673/5000 | Loss: 0.43690 | Gap: 0.06182 | Gamma: 0.0039\n",
      "Iter 674/5000 | Loss: 0.43682 | Gap: 0.04025 | Gamma: 0.0039\n",
      "Iter 675/5000 | Loss: 0.43682 | Gap: 0.03972 | Gamma: 0.0039\n",
      "Iter 676/5000 | Loss: 0.43675 | Gap: 0.05079 | Gamma: 0.0039\n",
      "Iter 677/5000 | Loss: 0.43670 | Gap: 0.04157 | Gamma: 0.0039\n",
      "Iter 678/5000 | Loss: 0.43666 | Gap: 0.05228 | Gamma: 0.0020\n",
      "Iter 679/5000 | Loss: 0.43662 | Gap: 0.03305 | Gamma: 0.0078\n",
      "Iter 680/5000 | Loss: 0.43658 | Gap: 0.11535 | Gamma: 0.0020\n",
      "Iter 681/5000 | Loss: 0.43655 | Gap: 0.03939 | Gamma: 0.0020\n",
      "Iter 682/5000 | Loss: 0.43649 | Gap: 0.03516 | Gamma: 0.0039\n",
      "Iter 683/5000 | Loss: 0.43644 | Gap: 0.04797 | Gamma: 0.0020\n",
      "Iter 684/5000 | Loss: 0.43643 | Gap: 0.03070 | Gamma: 0.0156\n",
      "Iter 685/5000 | Loss: 0.43632 | Gap: 0.14110 | Gamma: 0.0039\n",
      "Iter 686/5000 | Loss: 0.43622 | Gap: 0.04837 | Gamma: 0.0039\n",
      "Iter 687/5000 | Loss: 0.43618 | Gap: 0.04748 | Gamma: 0.0020\n",
      "Iter 688/5000 | Loss: 0.43613 | Gap: 0.03363 | Gamma: 0.0078\n",
      "Iter 689/5000 | Loss: 0.43605 | Gap: 0.09028 | Gamma: 0.0020\n",
      "Iter 690/5000 | Loss: 0.43595 | Gap: 0.03551 | Gamma: 0.0078\n",
      "Iter 691/5000 | Loss: 0.43590 | Gap: 0.05463 | Gamma: 0.0039\n",
      "Iter 692/5000 | Loss: 0.43585 | Gap: 0.03890 | Gamma: 0.0039\n",
      "Iter 693/5000 | Loss: 0.43581 | Gap: 0.04538 | Gamma: 0.0020\n",
      "Iter 694/5000 | Loss: 0.43578 | Gap: 0.03295 | Gamma: 0.0039\n",
      "Iter 695/5000 | Loss: 0.43575 | Gap: 0.06385 | Gamma: 0.0020\n",
      "Iter 696/5000 | Loss: 0.43573 | Gap: 0.03088 | Gamma: 0.0078\n",
      "Iter 697/5000 | Loss: 0.43565 | Gap: 0.09762 | Gamma: 0.0020\n",
      "Iter 698/5000 | Loss: 0.43564 | Gap: 0.03137 | Gamma: 0.0078\n",
      "Iter 699/5000 | Loss: 0.43562 | Gap: 0.08396 | Gamma: 0.0039\n",
      "Iter 700/5000 | Loss: 0.43560 | Gap: 0.04526 | Gamma: 0.0039\n",
      "Iter 701/5000 | Loss: 0.43556 | Gap: 0.05874 | Gamma: 0.0020\n",
      "Iter 702/5000 | Loss: 0.43551 | Gap: 0.03242 | Gamma: 0.0039\n",
      "Iter 703/5000 | Loss: 0.43548 | Gap: 0.04835 | Gamma: 0.0020\n",
      "Iter 704/5000 | Loss: 0.43540 | Gap: 0.02965 | Gamma: 0.0078\n",
      "Iter 705/5000 | Loss: 0.43533 | Gap: 0.07762 | Gamma: 0.0020\n",
      "Iter 706/5000 | Loss: 0.43523 | Gap: 0.02970 | Gamma: 0.0078\n",
      "Iter 707/5000 | Loss: 0.43518 | Gap: 0.06438 | Gamma: 0.0020\n",
      "Iter 708/5000 | Loss: 0.43512 | Gap: 0.03099 | Gamma: 0.0039\n",
      "Iter 709/5000 | Loss: 0.43512 | Gap: 0.04098 | Gamma: 0.0039\n",
      "Iter 710/5000 | Loss: 0.43509 | Gap: 0.04690 | Gamma: 0.0039\n",
      "Iter 711/5000 | Loss: 0.43502 | Gap: 0.03721 | Gamma: 0.0039\n",
      "Iter 712/5000 | Loss: 0.43499 | Gap: 0.03842 | Gamma: 0.0039\n",
      "Iter 713/5000 | Loss: 0.43495 | Gap: 0.04278 | Gamma: 0.0039\n",
      "Iter 714/5000 | Loss: 0.43493 | Gap: 0.04284 | Gamma: 0.0039\n",
      "Iter 715/5000 | Loss: 0.43489 | Gap: 0.04958 | Gamma: 0.0020\n",
      "Iter 716/5000 | Loss: 0.43482 | Gap: 0.02805 | Gamma: 0.0078\n",
      "Iter 717/5000 | Loss: 0.43477 | Gap: 0.08743 | Gamma: 0.0020\n",
      "Iter 718/5000 | Loss: 0.43472 | Gap: 0.03028 | Gamma: 0.0039\n",
      "Iter 719/5000 | Loss: 0.43469 | Gap: 0.04454 | Gamma: 0.0020\n",
      "Iter 720/5000 | Loss: 0.43464 | Gap: 0.02855 | Gamma: 0.0039\n",
      "Iter 721/5000 | Loss: 0.43463 | Gap: 0.04213 | Gamma: 0.0039\n",
      "Iter 722/5000 | Loss: 0.43457 | Gap: 0.03940 | Gamma: 0.0039\n",
      "Iter 723/5000 | Loss: 0.43451 | Gap: 0.03383 | Gamma: 0.0039\n",
      "Iter 724/5000 | Loss: 0.43448 | Gap: 0.03574 | Gamma: 0.0039\n",
      "Iter 725/5000 | Loss: 0.43445 | Gap: 0.03894 | Gamma: 0.0039\n",
      "Iter 726/5000 | Loss: 0.43438 | Gap: 0.03869 | Gamma: 0.0039\n",
      "Iter 727/5000 | Loss: 0.43434 | Gap: 0.03348 | Gamma: 0.0039\n",
      "Iter 728/5000 | Loss: 0.43429 | Gap: 0.03673 | Gamma: 0.0039\n",
      "Iter 729/5000 | Loss: 0.43424 | Gap: 0.03285 | Gamma: 0.0039\n",
      "Iter 730/5000 | Loss: 0.43421 | Gap: 0.04167 | Gamma: 0.0020\n",
      "Iter 731/5000 | Loss: 0.43416 | Gap: 0.02785 | Gamma: 0.0039\n",
      "Iter 732/5000 | Loss: 0.43413 | Gap: 0.04193 | Gamma: 0.0020\n",
      "Iter 733/5000 | Loss: 0.43412 | Gap: 0.02967 | Gamma: 0.0039\n",
      "Iter 734/5000 | Loss: 0.43409 | Gap: 0.05912 | Gamma: 0.0020\n",
      "Iter 735/5000 | Loss: 0.43406 | Gap: 0.02640 | Gamma: 0.0039\n",
      "Iter 736/5000 | Loss: 0.43403 | Gap: 0.05017 | Gamma: 0.0020\n",
      "Iter 737/5000 | Loss: 0.43397 | Gap: 0.02467 | Gamma: 0.0078\n",
      "Iter 738/5000 | Loss: 0.43393 | Gap: 0.06862 | Gamma: 0.0020\n",
      "Iter 739/5000 | Loss: 0.43388 | Gap: 0.02410 | Gamma: 0.0039\n",
      "Iter 740/5000 | Loss: 0.43386 | Gap: 0.04175 | Gamma: 0.0020\n",
      "Iter 741/5000 | Loss: 0.43383 | Gap: 0.02402 | Gamma: 0.0078\n",
      "Iter 742/5000 | Loss: 0.43378 | Gap: 0.08044 | Gamma: 0.0020\n",
      "Iter 743/5000 | Loss: 0.43376 | Gap: 0.02808 | Gamma: 0.0020\n",
      "Iter 744/5000 | Loss: 0.43374 | Gap: 0.02897 | Gamma: 0.0039\n",
      "Iter 745/5000 | Loss: 0.43370 | Gap: 0.04741 | Gamma: 0.0020\n",
      "Iter 746/5000 | Loss: 0.43365 | Gap: 0.02416 | Gamma: 0.0078\n",
      "Iter 747/5000 | Loss: 0.43361 | Gap: 0.07400 | Gamma: 0.0020\n",
      "Iter 748/5000 | Loss: 0.43359 | Gap: 0.02612 | Gamma: 0.0039\n",
      "Iter 749/5000 | Loss: 0.43355 | Gap: 0.04427 | Gamma: 0.0020\n",
      "Iter 750/5000 | Loss: 0.43353 | Gap: 0.02902 | Gamma: 0.0020\n",
      "Iter 751/5000 | Loss: 0.43350 | Gap: 0.02753 | Gamma: 0.0039\n",
      "Iter 752/5000 | Loss: 0.43347 | Gap: 0.04313 | Gamma: 0.0020\n",
      "Iter 753/5000 | Loss: 0.43342 | Gap: 0.02462 | Gamma: 0.0039\n",
      "Iter 754/5000 | Loss: 0.43341 | Gap: 0.04239 | Gamma: 0.0020\n",
      "Iter 755/5000 | Loss: 0.43340 | Gap: 0.02894 | Gamma: 0.0039\n",
      "Iter 756/5000 | Loss: 0.43337 | Gap: 0.04879 | Gamma: 0.0020\n",
      "Iter 757/5000 | Loss: 0.43333 | Gap: 0.02338 | Gamma: 0.0039\n",
      "Iter 758/5000 | Loss: 0.43331 | Gap: 0.03817 | Gamma: 0.0020\n",
      "Iter 759/5000 | Loss: 0.43328 | Gap: 0.02346 | Gamma: 0.0039\n",
      "Iter 760/5000 | Loss: 0.43325 | Gap: 0.04339 | Gamma: 0.0020\n",
      "Iter 761/5000 | Loss: 0.43323 | Gap: 0.02405 | Gamma: 0.0039\n",
      "Iter 762/5000 | Loss: 0.43319 | Gap: 0.04057 | Gamma: 0.0020\n",
      "Iter 763/5000 | Loss: 0.43319 | Gap: 0.02239 | Gamma: 0.0078\n",
      "Iter 764/5000 | Loss: 0.43312 | Gap: 0.07461 | Gamma: 0.0020\n",
      "Iter 765/5000 | Loss: 0.43308 | Gap: 0.03079 | Gamma: 0.0039\n",
      "Iter 766/5000 | Loss: 0.43307 | Gap: 0.03920 | Gamma: 0.0020\n",
      "Iter 767/5000 | Loss: 0.43306 | Gap: 0.03259 | Gamma: 0.0039\n",
      "Iter 768/5000 | Loss: 0.43304 | Gap: 0.04238 | Gamma: 0.0039\n",
      "Iter 769/5000 | Loss: 0.43298 | Gap: 0.03062 | Gamma: 0.0039\n",
      "Iter 770/5000 | Loss: 0.43295 | Gap: 0.02883 | Gamma: 0.0020\n",
      "Iter 771/5000 | Loss: 0.43293 | Gap: 0.02626 | Gamma: 0.0039\n",
      "Iter 772/5000 | Loss: 0.43292 | Gap: 0.05171 | Gamma: 0.0020\n",
      "Iter 773/5000 | Loss: 0.43289 | Gap: 0.02920 | Gamma: 0.0020\n",
      "Iter 774/5000 | Loss: 0.43288 | Gap: 0.02655 | Gamma: 0.0039\n",
      "Iter 775/5000 | Loss: 0.43286 | Gap: 0.05170 | Gamma: 0.0020\n",
      "Iter 776/5000 | Loss: 0.43284 | Gap: 0.02573 | Gamma: 0.0039\n",
      "Iter 777/5000 | Loss: 0.43282 | Gap: 0.04484 | Gamma: 0.0020\n",
      "Iter 778/5000 | Loss: 0.43278 | Gap: 0.02158 | Gamma: 0.0078\n",
      "Iter 779/5000 | Loss: 0.43273 | Gap: 0.06357 | Gamma: 0.0020\n",
      "Iter 780/5000 | Loss: 0.43272 | Gap: 0.02303 | Gamma: 0.0039\n",
      "Iter 781/5000 | Loss: 0.43270 | Gap: 0.04804 | Gamma: 0.0020\n",
      "Iter 782/5000 | Loss: 0.43268 | Gap: 0.02370 | Gamma: 0.0039\n",
      "Iter 783/5000 | Loss: 0.43265 | Gap: 0.04668 | Gamma: 0.0020\n",
      "Iter 784/5000 | Loss: 0.43262 | Gap: 0.02311 | Gamma: 0.0039\n",
      "Iter 785/5000 | Loss: 0.43260 | Gap: 0.04760 | Gamma: 0.0020\n",
      "Iter 786/5000 | Loss: 0.43257 | Gap: 0.02414 | Gamma: 0.0039\n",
      "Iter 787/5000 | Loss: 0.43255 | Gap: 0.04017 | Gamma: 0.0020\n",
      "Iter 788/5000 | Loss: 0.43253 | Gap: 0.02300 | Gamma: 0.0039\n",
      "Iter 789/5000 | Loss: 0.43251 | Gap: 0.04740 | Gamma: 0.0020\n",
      "Iter 790/5000 | Loss: 0.43247 | Gap: 0.02303 | Gamma: 0.0039\n",
      "Iter 791/5000 | Loss: 0.43244 | Gap: 0.03840 | Gamma: 0.0020\n",
      "Iter 792/5000 | Loss: 0.43241 | Gap: 0.02414 | Gamma: 0.0039\n",
      "Iter 793/5000 | Loss: 0.43238 | Gap: 0.03605 | Gamma: 0.0020\n",
      "Iter 794/5000 | Loss: 0.43234 | Gap: 0.02213 | Gamma: 0.0039\n",
      "Iter 795/5000 | Loss: 0.43232 | Gap: 0.03603 | Gamma: 0.0020\n",
      "Iter 796/5000 | Loss: 0.43228 | Gap: 0.02133 | Gamma: 0.0078\n",
      "Iter 797/5000 | Loss: 0.43223 | Gap: 0.06526 | Gamma: 0.0020\n",
      "Iter 798/5000 | Loss: 0.43219 | Gap: 0.02179 | Gamma: 0.0039\n",
      "Iter 799/5000 | Loss: 0.43219 | Gap: 0.03607 | Gamma: 0.0020\n",
      "Iter 800/5000 | Loss: 0.43217 | Gap: 0.02988 | Gamma: 0.0020\n",
      "Iter 801/5000 | Loss: 0.43216 | Gap: 0.02905 | Gamma: 0.0020\n",
      "Iter 802/5000 | Loss: 0.43212 | Gap: 0.02468 | Gamma: 0.0039\n",
      "Iter 803/5000 | Loss: 0.43209 | Gap: 0.02898 | Gamma: 0.0020\n",
      "Iter 804/5000 | Loss: 0.43209 | Gap: 0.02009 | Gamma: 0.0078\n",
      "Iter 805/5000 | Loss: 0.43207 | Gap: 0.08896 | Gamma: 0.0020\n",
      "Iter 806/5000 | Loss: 0.43205 | Gap: 0.02570 | Gamma: 0.0020\n",
      "Iter 807/5000 | Loss: 0.43203 | Gap: 0.02574 | Gamma: 0.0020\n",
      "Iter 808/5000 | Loss: 0.43202 | Gap: 0.02774 | Gamma: 0.0020\n",
      "Iter 809/5000 | Loss: 0.43199 | Gap: 0.02710 | Gamma: 0.0020\n",
      "Iter 810/5000 | Loss: 0.43197 | Gap: 0.02334 | Gamma: 0.0020\n",
      "Iter 811/5000 | Loss: 0.43196 | Gap: 0.02730 | Gamma: 0.0020\n",
      "Iter 812/5000 | Loss: 0.43194 | Gap: 0.02410 | Gamma: 0.0039\n",
      "Iter 813/5000 | Loss: 0.43191 | Gap: 0.04280 | Gamma: 0.0020\n",
      "Iter 814/5000 | Loss: 0.43190 | Gap: 0.02311 | Gamma: 0.0039\n",
      "Iter 815/5000 | Loss: 0.43187 | Gap: 0.04392 | Gamma: 0.0020\n",
      "Iter 816/5000 | Loss: 0.43182 | Gap: 0.02001 | Gamma: 0.0078\n",
      "Iter 817/5000 | Loss: 0.43180 | Gap: 0.06416 | Gamma: 0.0020\n",
      "Iter 818/5000 | Loss: 0.43178 | Gap: 0.02627 | Gamma: 0.0020\n",
      "Iter 819/5000 | Loss: 0.43176 | Gap: 0.02627 | Gamma: 0.0020\n",
      "Iter 820/5000 | Loss: 0.43174 | Gap: 0.02674 | Gamma: 0.0020\n",
      "Iter 821/5000 | Loss: 0.43172 | Gap: 0.02248 | Gamma: 0.0039\n",
      "Iter 822/5000 | Loss: 0.43170 | Gap: 0.03847 | Gamma: 0.0020\n",
      "Iter 823/5000 | Loss: 0.43165 | Gap: 0.01836 | Gamma: 0.0078\n",
      "Iter 824/5000 | Loss: 0.43164 | Gap: 0.05808 | Gamma: 0.0020\n",
      "Iter 825/5000 | Loss: 0.43163 | Gap: 0.02128 | Gamma: 0.0020\n",
      "Iter 826/5000 | Loss: 0.43162 | Gap: 0.02978 | Gamma: 0.0020\n",
      "Iter 827/5000 | Loss: 0.43161 | Gap: 0.02366 | Gamma: 0.0039\n",
      "Iter 828/5000 | Loss: 0.43158 | Gap: 0.03715 | Gamma: 0.0020\n",
      "Iter 829/5000 | Loss: 0.43156 | Gap: 0.01943 | Gamma: 0.0020\n",
      "Iter 830/5000 | Loss: 0.43155 | Gap: 0.03561 | Gamma: 0.0010\n",
      "Iter 831/5000 | Loss: 0.43151 | Gap: 0.01903 | Gamma: 0.0039\n",
      "Iter 832/5000 | Loss: 0.43149 | Gap: 0.04057 | Gamma: 0.0010\n",
      "Iter 833/5000 | Loss: 0.43147 | Gap: 0.01720 | Gamma: 0.0156\n",
      "Iter 834/5000 | Loss: 0.43142 | Gap: 0.11766 | Gamma: 0.0020\n",
      "Iter 835/5000 | Loss: 0.43140 | Gap: 0.04625 | Gamma: 0.0020\n",
      "Iter 836/5000 | Loss: 0.43138 | Gap: 0.02350 | Gamma: 0.0020\n",
      "Iter 837/5000 | Loss: 0.43137 | Gap: 0.02629 | Gamma: 0.0020\n",
      "Iter 838/5000 | Loss: 0.43136 | Gap: 0.02535 | Gamma: 0.0020\n",
      "Iter 839/5000 | Loss: 0.43134 | Gap: 0.02693 | Gamma: 0.0020\n",
      "Iter 840/5000 | Loss: 0.43132 | Gap: 0.02298 | Gamma: 0.0020\n",
      "Iter 841/5000 | Loss: 0.43131 | Gap: 0.02489 | Gamma: 0.0020\n",
      "Iter 842/5000 | Loss: 0.43130 | Gap: 0.02934 | Gamma: 0.0020\n",
      "Iter 843/5000 | Loss: 0.43128 | Gap: 0.02656 | Gamma: 0.0020\n",
      "Iter 844/5000 | Loss: 0.43127 | Gap: 0.02494 | Gamma: 0.0020\n",
      "Iter 845/5000 | Loss: 0.43126 | Gap: 0.03099 | Gamma: 0.0020\n",
      "Iter 846/5000 | Loss: 0.43123 | Gap: 0.02581 | Gamma: 0.0020\n",
      "Iter 847/5000 | Loss: 0.43121 | Gap: 0.02100 | Gamma: 0.0039\n",
      "Iter 848/5000 | Loss: 0.43119 | Gap: 0.03765 | Gamma: 0.0020\n",
      "Iter 849/5000 | Loss: 0.43116 | Gap: 0.01964 | Gamma: 0.0039\n",
      "Iter 850/5000 | Loss: 0.43115 | Gap: 0.03616 | Gamma: 0.0020\n",
      "Iter 851/5000 | Loss: 0.43115 | Gap: 0.02294 | Gamma: 0.0039\n",
      "Iter 852/5000 | Loss: 0.43113 | Gap: 0.04096 | Gamma: 0.0020\n",
      "Iter 853/5000 | Loss: 0.43112 | Gap: 0.02187 | Gamma: 0.0039\n",
      "Iter 854/5000 | Loss: 0.43110 | Gap: 0.04218 | Gamma: 0.0020\n",
      "Iter 855/5000 | Loss: 0.43109 | Gap: 0.02599 | Gamma: 0.0020\n",
      "Iter 856/5000 | Loss: 0.43108 | Gap: 0.02726 | Gamma: 0.0020\n",
      "Iter 857/5000 | Loss: 0.43106 | Gap: 0.02598 | Gamma: 0.0020\n",
      "Iter 858/5000 | Loss: 0.43105 | Gap: 0.02124 | Gamma: 0.0039\n",
      "Iter 859/5000 | Loss: 0.43102 | Gap: 0.04291 | Gamma: 0.0020\n",
      "Iter 860/5000 | Loss: 0.43099 | Gap: 0.02020 | Gamma: 0.0039\n",
      "Iter 861/5000 | Loss: 0.43098 | Gap: 0.03937 | Gamma: 0.0020\n",
      "Iter 862/5000 | Loss: 0.43096 | Gap: 0.02490 | Gamma: 0.0020\n",
      "Iter 863/5000 | Loss: 0.43094 | Gap: 0.02234 | Gamma: 0.0020\n",
      "Iter 864/5000 | Loss: 0.43093 | Gap: 0.02315 | Gamma: 0.0020\n",
      "Iter 865/5000 | Loss: 0.43090 | Gap: 0.02580 | Gamma: 0.0020\n",
      "Iter 866/5000 | Loss: 0.43090 | Gap: 0.01883 | Gamma: 0.0078\n",
      "Iter 867/5000 | Loss: 0.43085 | Gap: 0.06784 | Gamma: 0.0020\n",
      "Iter 868/5000 | Loss: 0.43083 | Gap: 0.01996 | Gamma: 0.0039\n",
      "Iter 869/5000 | Loss: 0.43081 | Gap: 0.03912 | Gamma: 0.0010\n",
      "Iter 870/5000 | Loss: 0.43079 | Gap: 0.01711 | Gamma: 0.0078\n",
      "Iter 871/5000 | Loss: 0.43075 | Gap: 0.07110 | Gamma: 0.0010\n",
      "Iter 872/5000 | Loss: 0.43073 | Gap: 0.03324 | Gamma: 0.0020\n",
      "Iter 873/5000 | Loss: 0.43073 | Gap: 0.02414 | Gamma: 0.0020\n",
      "Iter 874/5000 | Loss: 0.43072 | Gap: 0.02416 | Gamma: 0.0039\n",
      "Iter 875/5000 | Loss: 0.43069 | Gap: 0.03318 | Gamma: 0.0020\n",
      "Iter 876/5000 | Loss: 0.43068 | Gap: 0.02030 | Gamma: 0.0020\n",
      "Iter 877/5000 | Loss: 0.43067 | Gap: 0.02447 | Gamma: 0.0020\n",
      "Iter 878/5000 | Loss: 0.43066 | Gap: 0.02577 | Gamma: 0.0020\n",
      "Iter 879/5000 | Loss: 0.43066 | Gap: 0.02952 | Gamma: 0.0020\n",
      "Iter 880/5000 | Loss: 0.43063 | Gap: 0.02596 | Gamma: 0.0020\n",
      "Iter 881/5000 | Loss: 0.43063 | Gap: 0.02011 | Gamma: 0.0039\n",
      "Iter 882/5000 | Loss: 0.43063 | Gap: 0.05039 | Gamma: 0.0020\n",
      "Iter 883/5000 | Loss: 0.43061 | Gap: 0.02503 | Gamma: 0.0020\n",
      "Iter 884/5000 | Loss: 0.43061 | Gap: 0.02281 | Gamma: 0.0039\n",
      "Iter 885/5000 | Loss: 0.43059 | Gap: 0.04132 | Gamma: 0.0020\n",
      "Iter 886/5000 | Loss: 0.43058 | Gap: 0.02101 | Gamma: 0.0039\n",
      "Iter 887/5000 | Loss: 0.43056 | Gap: 0.04317 | Gamma: 0.0020\n",
      "Iter 888/5000 | Loss: 0.43054 | Gap: 0.02206 | Gamma: 0.0039\n",
      "Iter 889/5000 | Loss: 0.43053 | Gap: 0.04063 | Gamma: 0.0020\n",
      "Iter 890/5000 | Loss: 0.43051 | Gap: 0.02292 | Gamma: 0.0020\n",
      "Iter 891/5000 | Loss: 0.43048 | Gap: 0.02247 | Gamma: 0.0020\n",
      "Iter 892/5000 | Loss: 0.43046 | Gap: 0.02177 | Gamma: 0.0020\n",
      "Iter 893/5000 | Loss: 0.43045 | Gap: 0.02023 | Gamma: 0.0039\n",
      "Iter 894/5000 | Loss: 0.43043 | Gap: 0.04071 | Gamma: 0.0020\n",
      "Iter 895/5000 | Loss: 0.43043 | Gap: 0.02095 | Gamma: 0.0039\n",
      "Iter 896/5000 | Loss: 0.43042 | Gap: 0.04283 | Gamma: 0.0020\n",
      "Iter 897/5000 | Loss: 0.43039 | Gap: 0.02253 | Gamma: 0.0039\n",
      "Iter 898/5000 | Loss: 0.43037 | Gap: 0.03493 | Gamma: 0.0020\n",
      "Iter 899/5000 | Loss: 0.43033 | Gap: 0.01967 | Gamma: 0.0039\n",
      "Iter 900/5000 | Loss: 0.43032 | Gap: 0.03226 | Gamma: 0.0020\n",
      "Iter 901/5000 | Loss: 0.43032 | Gap: 0.02237 | Gamma: 0.0039\n",
      "Iter 902/5000 | Loss: 0.43030 | Gap: 0.04418 | Gamma: 0.0020\n",
      "Iter 903/5000 | Loss: 0.43029 | Gap: 0.02449 | Gamma: 0.0020\n",
      "Iter 904/5000 | Loss: 0.43028 | Gap: 0.02778 | Gamma: 0.0020\n",
      "Iter 905/5000 | Loss: 0.43026 | Gap: 0.02755 | Gamma: 0.0020\n",
      "Iter 906/5000 | Loss: 0.43025 | Gap: 0.02097 | Gamma: 0.0039\n",
      "Iter 907/5000 | Loss: 0.43023 | Gap: 0.04075 | Gamma: 0.0020\n",
      "Iter 908/5000 | Loss: 0.43020 | Gap: 0.01792 | Gamma: 0.0078\n",
      "Iter 909/5000 | Loss: 0.43018 | Gap: 0.07061 | Gamma: 0.0020\n",
      "Iter 910/5000 | Loss: 0.43017 | Gap: 0.02262 | Gamma: 0.0020\n",
      "Iter 911/5000 | Loss: 0.43015 | Gap: 0.02159 | Gamma: 0.0039\n",
      "Iter 912/5000 | Loss: 0.43013 | Gap: 0.03230 | Gamma: 0.0020\n",
      "Iter 913/5000 | Loss: 0.43012 | Gap: 0.01889 | Gamma: 0.0039\n",
      "Iter 914/5000 | Loss: 0.43011 | Gap: 0.04056 | Gamma: 0.0020\n",
      "Iter 915/5000 | Loss: 0.43009 | Gap: 0.02001 | Gamma: 0.0039\n",
      "Iter 916/5000 | Loss: 0.43009 | Gap: 0.04200 | Gamma: 0.0020\n",
      "Iter 917/5000 | Loss: 0.43007 | Gap: 0.02287 | Gamma: 0.0020\n",
      "Iter 918/5000 | Loss: 0.43006 | Gap: 0.02564 | Gamma: 0.0020\n",
      "Iter 919/5000 | Loss: 0.43006 | Gap: 0.02276 | Gamma: 0.0039\n",
      "Iter 920/5000 | Loss: 0.43003 | Gap: 0.03605 | Gamma: 0.0020\n",
      "Iter 921/5000 | Loss: 0.42999 | Gap: 0.01656 | Gamma: 0.0078\n",
      "Iter 922/5000 | Loss: 0.42998 | Gap: 0.05850 | Gamma: 0.0020\n",
      "Iter 923/5000 | Loss: 0.42996 | Gap: 0.01777 | Gamma: 0.0039\n",
      "Iter 924/5000 | Loss: 0.42994 | Gap: 0.03911 | Gamma: 0.0010\n",
      "Iter 925/5000 | Loss: 0.42993 | Gap: 0.02100 | Gamma: 0.0020\n",
      "Iter 926/5000 | Loss: 0.42991 | Gap: 0.02335 | Gamma: 0.0020\n",
      "Iter 927/5000 | Loss: 0.42990 | Gap: 0.02300 | Gamma: 0.0020\n",
      "Iter 928/5000 | Loss: 0.42988 | Gap: 0.02370 | Gamma: 0.0020\n",
      "Iter 929/5000 | Loss: 0.42986 | Gap: 0.02034 | Gamma: 0.0020\n",
      "Iter 930/5000 | Loss: 0.42985 | Gap: 0.02048 | Gamma: 0.0020\n",
      "Iter 931/5000 | Loss: 0.42983 | Gap: 0.02137 | Gamma: 0.0020\n",
      "Iter 932/5000 | Loss: 0.42981 | Gap: 0.02044 | Gamma: 0.0020\n",
      "Iter 933/5000 | Loss: 0.42981 | Gap: 0.01969 | Gamma: 0.0039\n",
      "Iter 934/5000 | Loss: 0.42979 | Gap: 0.03744 | Gamma: 0.0020\n",
      "Iter 935/5000 | Loss: 0.42978 | Gap: 0.01870 | Gamma: 0.0039\n",
      "Iter 936/5000 | Loss: 0.42978 | Gap: 0.04314 | Gamma: 0.0020\n",
      "Iter 937/5000 | Loss: 0.42977 | Gap: 0.02164 | Gamma: 0.0039\n",
      "Iter 938/5000 | Loss: 0.42976 | Gap: 0.04212 | Gamma: 0.0020\n",
      "Iter 939/5000 | Loss: 0.42976 | Gap: 0.02393 | Gamma: 0.0020\n",
      "Iter 940/5000 | Loss: 0.42974 | Gap: 0.03249 | Gamma: 0.0010\n",
      "Iter 941/5000 | Loss: 0.42974 | Gap: 0.01567 | Gamma: 0.0156\n",
      "Iter 942/5000 | Loss: 0.42964 | Gap: 0.11606 | Gamma: 0.0020\n",
      "Iter 943/5000 | Loss: 0.42962 | Gap: 0.02973 | Gamma: 0.0020\n",
      "Iter 944/5000 | Loss: 0.42961 | Gap: 0.02208 | Gamma: 0.0010\n",
      "Iter 945/5000 | Loss: 0.42959 | Gap: 0.01510 | Gamma: 0.0039\n",
      "Iter 946/5000 | Loss: 0.42959 | Gap: 0.03626 | Gamma: 0.0020\n",
      "Iter 947/5000 | Loss: 0.42958 | Gap: 0.01851 | Gamma: 0.0039\n",
      "Iter 948/5000 | Loss: 0.42956 | Gap: 0.04024 | Gamma: 0.0010\n",
      "Iter 949/5000 | Loss: 0.42953 | Gap: 0.01675 | Gamma: 0.0039\n",
      "Iter 950/5000 | Loss: 0.42952 | Gap: 0.02450 | Gamma: 0.0020\n",
      "Iter 951/5000 | Loss: 0.42951 | Gap: 0.02329 | Gamma: 0.0020\n",
      "Iter 952/5000 | Loss: 0.42951 | Gap: 0.02460 | Gamma: 0.0020\n",
      "Iter 953/5000 | Loss: 0.42950 | Gap: 0.03013 | Gamma: 0.0010\n",
      "Iter 954/5000 | Loss: 0.42948 | Gap: 0.01618 | Gamma: 0.0039\n",
      "Iter 955/5000 | Loss: 0.42947 | Gap: 0.04679 | Gamma: 0.0010\n",
      "Iter 956/5000 | Loss: 0.42943 | Gap: 0.01562 | Gamma: 0.0078\n",
      "Iter 957/5000 | Loss: 0.42943 | Gap: 0.05334 | Gamma: 0.0020\n",
      "Iter 958/5000 | Loss: 0.42942 | Gap: 0.02414 | Gamma: 0.0020\n",
      "Iter 959/5000 | Loss: 0.42940 | Gap: 0.02046 | Gamma: 0.0020\n",
      "Iter 960/5000 | Loss: 0.42939 | Gap: 0.01973 | Gamma: 0.0020\n",
      "Iter 961/5000 | Loss: 0.42939 | Gap: 0.02013 | Gamma: 0.0039\n",
      "Iter 962/5000 | Loss: 0.42937 | Gap: 0.03888 | Gamma: 0.0020\n",
      "Iter 963/5000 | Loss: 0.42936 | Gap: 0.02034 | Gamma: 0.0020\n",
      "Iter 964/5000 | Loss: 0.42935 | Gap: 0.02707 | Gamma: 0.0010\n",
      "Iter 965/5000 | Loss: 0.42934 | Gap: 0.01785 | Gamma: 0.0020\n",
      "Iter 966/5000 | Loss: 0.42932 | Gap: 0.02505 | Gamma: 0.0010\n",
      "Iter 967/5000 | Loss: 0.42932 | Gap: 0.01433 | Gamma: 0.0156\n",
      "Iter 968/5000 | Loss: 0.42928 | Gap: 0.17567 | Gamma: 0.0010\n",
      "Iter 969/5000 | Loss: 0.42928 | Gap: 0.03066 | Gamma: 0.0005\n",
      "Iter 970/5000 | Loss: 0.42927 | Gap: 0.02493 | Gamma: 0.0010\n",
      "Iter 971/5000 | Loss: 0.42926 | Gap: 0.01932 | Gamma: 0.0010\n",
      "Iter 972/5000 | Loss: 0.42925 | Gap: 0.01617 | Gamma: 0.0020\n",
      "Iter 973/5000 | Loss: 0.42924 | Gap: 0.02166 | Gamma: 0.0010\n",
      "Iter 974/5000 | Loss: 0.42923 | Gap: 0.01635 | Gamma: 0.0010\n",
      "Iter 975/5000 | Loss: 0.42923 | Gap: 0.01806 | Gamma: 0.0020\n",
      "Iter 976/5000 | Loss: 0.42922 | Gap: 0.02546 | Gamma: 0.0020\n",
      "Iter 977/5000 | Loss: 0.42919 | Gap: 0.01681 | Gamma: 0.0039\n",
      "Iter 978/5000 | Loss: 0.42918 | Gap: 0.02436 | Gamma: 0.0020\n",
      "Iter 979/5000 | Loss: 0.42917 | Gap: 0.02019 | Gamma: 0.0020\n",
      "Iter 980/5000 | Loss: 0.42916 | Gap: 0.02128 | Gamma: 0.0020\n",
      "Iter 981/5000 | Loss: 0.42914 | Gap: 0.01714 | Gamma: 0.0020\n",
      "Iter 982/5000 | Loss: 0.42914 | Gap: 0.02090 | Gamma: 0.0020\n",
      "Iter 983/5000 | Loss: 0.42912 | Gap: 0.02384 | Gamma: 0.0020\n",
      "Iter 984/5000 | Loss: 0.42911 | Gap: 0.01823 | Gamma: 0.0020\n",
      "Iter 985/5000 | Loss: 0.42909 | Gap: 0.01924 | Gamma: 0.0020\n",
      "Iter 986/5000 | Loss: 0.42908 | Gap: 0.01973 | Gamma: 0.0020\n",
      "Iter 987/5000 | Loss: 0.42907 | Gap: 0.01970 | Gamma: 0.0020\n",
      "Iter 988/5000 | Loss: 0.42907 | Gap: 0.01818 | Gamma: 0.0039\n",
      "Iter 989/5000 | Loss: 0.42905 | Gap: 0.03664 | Gamma: 0.0020\n",
      "Iter 990/5000 | Loss: 0.42903 | Gap: 0.01620 | Gamma: 0.0039\n",
      "Iter 991/5000 | Loss: 0.42902 | Gap: 0.03104 | Gamma: 0.0020\n",
      "Iter 992/5000 | Loss: 0.42900 | Gap: 0.01897 | Gamma: 0.0020\n",
      "Iter 993/5000 | Loss: 0.42900 | Gap: 0.01895 | Gamma: 0.0020\n",
      "Iter 994/5000 | Loss: 0.42899 | Gap: 0.02261 | Gamma: 0.0020\n",
      "Iter 995/5000 | Loss: 0.42898 | Gap: 0.01780 | Gamma: 0.0039\n",
      "Iter 996/5000 | Loss: 0.42897 | Gap: 0.03833 | Gamma: 0.0010\n",
      "Iter 997/5000 | Loss: 0.42896 | Gap: 0.02221 | Gamma: 0.0010\n",
      "Iter 998/5000 | Loss: 0.42895 | Gap: 0.01751 | Gamma: 0.0020\n",
      "Iter 999/5000 | Loss: 0.42894 | Gap: 0.02471 | Gamma: 0.0010\n",
      "Iter 1000/5000 | Loss: 0.42891 | Gap: 0.01283 | Gamma: 0.0156\n",
      "Iter 1001/5000 | Loss: 0.42890 | Gap: 0.10353 | Gamma: 0.0020\n",
      "Iter 1002/5000 | Loss: 0.42889 | Gap: 0.03885 | Gamma: 0.0005\n",
      "Iter 1003/5000 | Loss: 0.42889 | Gap: 0.01790 | Gamma: 0.0020\n",
      "Iter 1004/5000 | Loss: 0.42889 | Gap: 0.03338 | Gamma: 0.0010\n",
      "Iter 1005/5000 | Loss: 0.42888 | Gap: 0.01894 | Gamma: 0.0010\n",
      "Iter 1006/5000 | Loss: 0.42888 | Gap: 0.01798 | Gamma: 0.0010\n",
      "Iter 1007/5000 | Loss: 0.42887 | Gap: 0.01596 | Gamma: 0.0020\n",
      "Iter 1008/5000 | Loss: 0.42886 | Gap: 0.02405 | Gamma: 0.0020\n",
      "Iter 1009/5000 | Loss: 0.42885 | Gap: 0.02069 | Gamma: 0.0020\n",
      "Iter 1010/5000 | Loss: 0.42884 | Gap: 0.01927 | Gamma: 0.0020\n",
      "Iter 1011/5000 | Loss: 0.42883 | Gap: 0.02139 | Gamma: 0.0020\n",
      "Iter 1012/5000 | Loss: 0.42883 | Gap: 0.02206 | Gamma: 0.0020\n",
      "Iter 1013/5000 | Loss: 0.42882 | Gap: 0.02727 | Gamma: 0.0010\n",
      "Iter 1014/5000 | Loss: 0.42879 | Gap: 0.01468 | Gamma: 0.0039\n",
      "Iter 1015/5000 | Loss: 0.42879 | Gap: 0.03193 | Gamma: 0.0020\n",
      "Iter 1016/5000 | Loss: 0.42877 | Gap: 0.02070 | Gamma: 0.0020\n",
      "Iter 1017/5000 | Loss: 0.42876 | Gap: 0.01560 | Gamma: 0.0039\n",
      "Iter 1018/5000 | Loss: 0.42875 | Gap: 0.03331 | Gamma: 0.0020\n",
      "Iter 1019/5000 | Loss: 0.42874 | Gap: 0.01691 | Gamma: 0.0039\n",
      "Iter 1020/5000 | Loss: 0.42873 | Gap: 0.03718 | Gamma: 0.0020\n",
      "Iter 1021/5000 | Loss: 0.42871 | Gap: 0.01717 | Gamma: 0.0039\n",
      "Iter 1022/5000 | Loss: 0.42870 | Gap: 0.03439 | Gamma: 0.0010\n",
      "Iter 1023/5000 | Loss: 0.42869 | Gap: 0.01481 | Gamma: 0.0078\n",
      "Iter 1024/5000 | Loss: 0.42867 | Gap: 0.05166 | Gamma: 0.0020\n",
      "Iter 1025/5000 | Loss: 0.42865 | Gap: 0.01903 | Gamma: 0.0020\n",
      "Iter 1026/5000 | Loss: 0.42865 | Gap: 0.01965 | Gamma: 0.0010\n",
      "Iter 1027/5000 | Loss: 0.42864 | Gap: 0.01506 | Gamma: 0.0020\n",
      "Iter 1028/5000 | Loss: 0.42863 | Gap: 0.02340 | Gamma: 0.0010\n",
      "Iter 1029/5000 | Loss: 0.42861 | Gap: 0.01335 | Gamma: 0.0039\n",
      "Iter 1030/5000 | Loss: 0.42859 | Gap: 0.03556 | Gamma: 0.0010\n",
      "Iter 1031/5000 | Loss: 0.42858 | Gap: 0.01458 | Gamma: 0.0039\n",
      "Iter 1032/5000 | Loss: 0.42857 | Gap: 0.03453 | Gamma: 0.0010\n",
      "Iter 1033/5000 | Loss: 0.42855 | Gap: 0.01501 | Gamma: 0.0039\n",
      "Iter 1034/5000 | Loss: 0.42854 | Gap: 0.02464 | Gamma: 0.0020\n",
      "Iter 1035/5000 | Loss: 0.42853 | Gap: 0.01640 | Gamma: 0.0020\n",
      "Iter 1036/5000 | Loss: 0.42852 | Gap: 0.01791 | Gamma: 0.0010\n",
      "Iter 1037/5000 | Loss: 0.42852 | Gap: 0.01524 | Gamma: 0.0010\n",
      "Iter 1038/5000 | Loss: 0.42851 | Gap: 0.01600 | Gamma: 0.0010\n",
      "Iter 1039/5000 | Loss: 0.42851 | Gap: 0.01268 | Gamma: 0.0039\n",
      "Iter 1040/5000 | Loss: 0.42850 | Gap: 0.04669 | Gamma: 0.0010\n",
      "Iter 1041/5000 | Loss: 0.42850 | Gap: 0.01370 | Gamma: 0.0020\n",
      "Iter 1042/5000 | Loss: 0.42849 | Gap: 0.02795 | Gamma: 0.0010\n",
      "Iter 1043/5000 | Loss: 0.42848 | Gap: 0.01182 | Gamma: 0.0078\n",
      "Iter 1044/5000 | Loss: 0.42845 | Gap: 0.07375 | Gamma: 0.0010\n",
      "Iter 1045/5000 | Loss: 0.42844 | Gap: 0.02503 | Gamma: 0.0010\n",
      "Iter 1046/5000 | Loss: 0.42844 | Gap: 0.01456 | Gamma: 0.0020\n",
      "Iter 1047/5000 | Loss: 0.42843 | Gap: 0.02597 | Gamma: 0.0010\n",
      "Iter 1048/5000 | Loss: 0.42842 | Gap: 0.01329 | Gamma: 0.0020\n",
      "Iter 1049/5000 | Loss: 0.42841 | Gap: 0.02080 | Gamma: 0.0010\n",
      "Iter 1050/5000 | Loss: 0.42840 | Gap: 0.01259 | Gamma: 0.0039\n",
      "Iter 1051/5000 | Loss: 0.42838 | Gap: 0.04053 | Gamma: 0.0010\n",
      "Iter 1052/5000 | Loss: 0.42838 | Gap: 0.01301 | Gamma: 0.0039\n",
      "Iter 1053/5000 | Loss: 0.42837 | Gap: 0.03697 | Gamma: 0.0010\n",
      "Iter 1054/5000 | Loss: 0.42836 | Gap: 0.01687 | Gamma: 0.0020\n",
      "Iter 1055/5000 | Loss: 0.42835 | Gap: 0.02101 | Gamma: 0.0010\n",
      "Iter 1056/5000 | Loss: 0.42834 | Gap: 0.01237 | Gamma: 0.0039\n",
      "Iter 1057/5000 | Loss: 0.42833 | Gap: 0.04442 | Gamma: 0.0010\n",
      "Iter 1058/5000 | Loss: 0.42832 | Gap: 0.01440 | Gamma: 0.0039\n",
      "Iter 1059/5000 | Loss: 0.42831 | Gap: 0.03111 | Gamma: 0.0010\n",
      "Iter 1060/5000 | Loss: 0.42830 | Gap: 0.01531 | Gamma: 0.0020\n",
      "Iter 1061/5000 | Loss: 0.42829 | Gap: 0.02624 | Gamma: 0.0010\n",
      "Iter 1062/5000 | Loss: 0.42827 | Gap: 0.01311 | Gamma: 0.0039\n",
      "Iter 1063/5000 | Loss: 0.42826 | Gap: 0.02912 | Gamma: 0.0010\n",
      "Iter 1064/5000 | Loss: 0.42824 | Gap: 0.01205 | Gamma: 0.0039\n",
      "Iter 1065/5000 | Loss: 0.42824 | Gap: 0.03475 | Gamma: 0.0010\n",
      "Iter 1066/5000 | Loss: 0.42823 | Gap: 0.01458 | Gamma: 0.0010\n",
      "Iter 1067/5000 | Loss: 0.42823 | Gap: 0.01290 | Gamma: 0.0020\n",
      "Iter 1068/5000 | Loss: 0.42822 | Gap: 0.02167 | Gamma: 0.0010\n",
      "Iter 1069/5000 | Loss: 0.42821 | Gap: 0.01126 | Gamma: 0.0020\n",
      "Iter 1070/5000 | Loss: 0.42820 | Gap: 0.02066 | Gamma: 0.0010\n",
      "Iter 1071/5000 | Loss: 0.42820 | Gap: 0.01240 | Gamma: 0.0020\n",
      "Iter 1072/5000 | Loss: 0.42819 | Gap: 0.02443 | Gamma: 0.0010\n",
      "Iter 1073/5000 | Loss: 0.42817 | Gap: 0.01174 | Gamma: 0.0039\n",
      "Iter 1074/5000 | Loss: 0.42816 | Gap: 0.03153 | Gamma: 0.0010\n",
      "Iter 1075/5000 | Loss: 0.42816 | Gap: 0.01184 | Gamma: 0.0039\n",
      "Iter 1076/5000 | Loss: 0.42815 | Gap: 0.03642 | Gamma: 0.0010\n",
      "Iter 1077/5000 | Loss: 0.42813 | Gap: 0.01290 | Gamma: 0.0039\n",
      "Iter 1078/5000 | Loss: 0.42812 | Gap: 0.02439 | Gamma: 0.0010\n",
      "Iter 1079/5000 | Loss: 0.42812 | Gap: 0.01334 | Gamma: 0.0010\n",
      "Iter 1080/5000 | Loss: 0.42811 | Gap: 0.01510 | Gamma: 0.0010\n",
      "Iter 1081/5000 | Loss: 0.42810 | Gap: 0.01191 | Gamma: 0.0020\n",
      "Iter 1082/5000 | Loss: 0.42809 | Gap: 0.01909 | Gamma: 0.0010\n",
      "Iter 1083/5000 | Loss: 0.42808 | Gap: 0.01048 | Gamma: 0.0039\n",
      "Iter 1084/5000 | Loss: 0.42807 | Gap: 0.03544 | Gamma: 0.0010\n",
      "Iter 1085/5000 | Loss: 0.42807 | Gap: 0.01255 | Gamma: 0.0010\n",
      "Iter 1086/5000 | Loss: 0.42807 | Gap: 0.01456 | Gamma: 0.0010\n",
      "Iter 1087/5000 | Loss: 0.42806 | Gap: 0.01274 | Gamma: 0.0020\n",
      "Iter 1088/5000 | Loss: 0.42806 | Gap: 0.02273 | Gamma: 0.0010\n",
      "Iter 1089/5000 | Loss: 0.42806 | Gap: 0.01542 | Gamma: 0.0010\n",
      "Iter 1090/5000 | Loss: 0.42805 | Gap: 0.01462 | Gamma: 0.0010\n",
      "Iter 1091/5000 | Loss: 0.42804 | Gap: 0.01187 | Gamma: 0.0020\n",
      "Iter 1092/5000 | Loss: 0.42804 | Gap: 0.02142 | Gamma: 0.0010\n",
      "Iter 1093/5000 | Loss: 0.42803 | Gap: 0.01161 | Gamma: 0.0039\n",
      "Iter 1094/5000 | Loss: 0.42802 | Gap: 0.03683 | Gamma: 0.0010\n",
      "Iter 1095/5000 | Loss: 0.42800 | Gap: 0.01137 | Gamma: 0.0039\n",
      "Iter 1096/5000 | Loss: 0.42799 | Gap: 0.02744 | Gamma: 0.0010\n",
      "Iter 1097/5000 | Loss: 0.42798 | Gap: 0.01124 | Gamma: 0.0039\n",
      "Iter 1098/5000 | Loss: 0.42797 | Gap: 0.02718 | Gamma: 0.0010\n",
      "Iter 1099/5000 | Loss: 0.42797 | Gap: 0.01263 | Gamma: 0.0010\n",
      "Iter 1100/5000 | Loss: 0.42797 | Gap: 0.01625 | Gamma: 0.0005\n",
      "Iter 1101/5000 | Loss: 0.42796 | Gap: 0.01032 | Gamma: 0.0039\n",
      "Iter 1102/5000 | Loss: 0.42795 | Gap: 0.03721 | Gamma: 0.0010\n",
      "Iter 1103/5000 | Loss: 0.42794 | Gap: 0.01133 | Gamma: 0.0020\n",
      "Iter 1104/5000 | Loss: 0.42793 | Gap: 0.01691 | Gamma: 0.0010\n",
      "Iter 1105/5000 | Loss: 0.42793 | Gap: 0.01272 | Gamma: 0.0010\n",
      "Iter 1106/5000 | Loss: 0.42793 | Gap: 0.01188 | Gamma: 0.0020\n",
      "Iter 1107/5000 | Loss: 0.42793 | Gap: 0.02745 | Gamma: 0.0010\n",
      "Iter 1108/5000 | Loss: 0.42793 | Gap: 0.01609 | Gamma: 0.0010\n",
      "Iter 1109/5000 | Loss: 0.42792 | Gap: 0.01538 | Gamma: 0.0010\n",
      "Iter 1110/5000 | Loss: 0.42791 | Gap: 0.01120 | Gamma: 0.0020\n",
      "Iter 1111/5000 | Loss: 0.42791 | Gap: 0.01868 | Gamma: 0.0010\n",
      "Iter 1112/5000 | Loss: 0.42790 | Gap: 0.01302 | Gamma: 0.0020\n",
      "Iter 1113/5000 | Loss: 0.42789 | Gap: 0.01866 | Gamma: 0.0010\n",
      "Iter 1114/5000 | Loss: 0.42787 | Gap: 0.01010 | Gamma: 0.0039\n",
      "Iter 1115/5000 | Loss: 0.42786 | Gap: 0.02849 | Gamma: 0.0010\n",
      "Iter 1116/5000 | Loss: 0.42785 | Gap: 0.01062 | Gamma: 0.0020\n",
      "Iter 1117/5000 | Loss: 0.42785 | Gap: 0.01594 | Gamma: 0.0010\n",
      "Iter 1118/5000 | Loss: 0.42784 | Gap: 0.01005 | Gamma: 0.0020\n",
      "Iter 1119/5000 | Loss: 0.42783 | Gap: 0.01678 | Gamma: 0.0010\n",
      "Iter 1120/5000 | Loss: 0.42783 | Gap: 0.01046 | Gamma: 0.0020\n",
      "Iter 1121/5000 | Loss: 0.42782 | Gap: 0.02073 | Gamma: 0.0010\n",
      "Iter 1122/5000 | Loss: 0.42782 | Gap: 0.01232 | Gamma: 0.0010\n",
      "Iter 1123/5000 | Loss: 0.42781 | Gap: 0.01279 | Gamma: 0.0010\n",
      "Iter 1124/5000 | Loss: 0.42781 | Gap: 0.01105 | Gamma: 0.0020\n",
      "Iter 1125/5000 | Loss: 0.42780 | Gap: 0.01764 | Gamma: 0.0010\n",
      "Iter 1126/5000 | Loss: 0.42780 | Gap: 0.01174 | Gamma: 0.0010\n",
      "Iter 1127/5000 | Loss: 0.42780 | Gap: 0.01281 | Gamma: 0.0010\n",
      "Iter 1128/5000 | Loss: 0.42779 | Gap: 0.01558 | Gamma: 0.0010\n",
      "Iter 1129/5000 | Loss: 0.42779 | Gap: 0.01122 | Gamma: 0.0020\n",
      "Iter 1130/5000 | Loss: 0.42778 | Gap: 0.01895 | Gamma: 0.0010\n",
      "Iter 1131/5000 | Loss: 0.42776 | Gap: 0.00933 | Gamma: 0.0039\n",
      "Iter 1132/5000 | Loss: 0.42775 | Gap: 0.02611 | Gamma: 0.0010\n",
      "Iter 1133/5000 | Loss: 0.42775 | Gap: 0.01019 | Gamma: 0.0020\n",
      "Iter 1134/5000 | Loss: 0.42774 | Gap: 0.02246 | Gamma: 0.0005\n",
      "Iter 1135/5000 | Loss: 0.42774 | Gap: 0.00934 | Gamma: 0.0039\n",
      "Iter 1136/5000 | Loss: 0.42773 | Gap: 0.03200 | Gamma: 0.0010\n",
      "Iter 1137/5000 | Loss: 0.42773 | Gap: 0.01359 | Gamma: 0.0010\n",
      "Iter 1138/5000 | Loss: 0.42772 | Gap: 0.00998 | Gamma: 0.0020\n",
      "Iter 1139/5000 | Loss: 0.42772 | Gap: 0.01850 | Gamma: 0.0010\n",
      "Iter 1140/5000 | Loss: 0.42771 | Gap: 0.01024 | Gamma: 0.0020\n",
      "Iter 1141/5000 | Loss: 0.42771 | Gap: 0.02165 | Gamma: 0.0010\n",
      "Iter 1142/5000 | Loss: 0.42770 | Gap: 0.01186 | Gamma: 0.0010\n",
      "Iter 1143/5000 | Loss: 0.42770 | Gap: 0.01360 | Gamma: 0.0010\n",
      "Iter 1144/5000 | Loss: 0.42770 | Gap: 0.01486 | Gamma: 0.0010\n",
      "Iter 1145/5000 | Loss: 0.42769 | Gap: 0.01366 | Gamma: 0.0010\n",
      "Iter 1146/5000 | Loss: 0.42769 | Gap: 0.01128 | Gamma: 0.0020\n",
      "Iter 1147/5000 | Loss: 0.42769 | Gap: 0.02175 | Gamma: 0.0010\n",
      "Iter 1148/5000 | Loss: 0.42768 | Gap: 0.00983 | Gamma: 0.0020\n",
      "Iter 1149/5000 | Loss: 0.42768 | Gap: 0.02123 | Gamma: 0.0010\n",
      "Iter 1150/5000 | Loss: 0.42767 | Gap: 0.01071 | Gamma: 0.0020\n",
      "Iter 1151/5000 | Loss: 0.42767 | Gap: 0.01875 | Gamma: 0.0010\n",
      "Iter 1152/5000 | Loss: 0.42767 | Gap: 0.01233 | Gamma: 0.0020\n",
      "Iter 1153/5000 | Loss: 0.42766 | Gap: 0.02139 | Gamma: 0.0010\n",
      "Iter 1154/5000 | Loss: 0.42765 | Gap: 0.01053 | Gamma: 0.0020\n",
      "Iter 1155/5000 | Loss: 0.42765 | Gap: 0.02076 | Gamma: 0.0010\n",
      "Iter 1156/5000 | Loss: 0.42764 | Gap: 0.01193 | Gamma: 0.0010\n",
      "Iter 1157/5000 | Loss: 0.42764 | Gap: 0.01334 | Gamma: 0.0010\n",
      "Iter 1158/5000 | Loss: 0.42763 | Gap: 0.01117 | Gamma: 0.0020\n",
      "Iter 1159/5000 | Loss: 0.42763 | Gap: 0.02044 | Gamma: 0.0010\n",
      "Iter 1160/5000 | Loss: 0.42762 | Gap: 0.01036 | Gamma: 0.0020\n",
      "Iter 1161/5000 | Loss: 0.42762 | Gap: 0.01879 | Gamma: 0.0010\n",
      "Iter 1162/5000 | Loss: 0.42761 | Gap: 0.01091 | Gamma: 0.0020\n",
      "Iter 1163/5000 | Loss: 0.42760 | Gap: 0.01650 | Gamma: 0.0010\n",
      "Iter 1164/5000 | Loss: 0.42760 | Gap: 0.01195 | Gamma: 0.0020\n",
      "Iter 1165/5000 | Loss: 0.42759 | Gap: 0.01963 | Gamma: 0.0010\n",
      "Iter 1166/5000 | Loss: 0.42758 | Gap: 0.01070 | Gamma: 0.0020\n",
      "Iter 1167/5000 | Loss: 0.42758 | Gap: 0.01650 | Gamma: 0.0010\n",
      "Iter 1168/5000 | Loss: 0.42757 | Gap: 0.01008 | Gamma: 0.0039\n",
      "Iter 1169/5000 | Loss: 0.42756 | Gap: 0.03575 | Gamma: 0.0010\n",
      "Iter 1170/5000 | Loss: 0.42756 | Gap: 0.01242 | Gamma: 0.0020\n",
      "Iter 1171/5000 | Loss: 0.42756 | Gap: 0.02579 | Gamma: 0.0010\n",
      "Iter 1172/5000 | Loss: 0.42755 | Gap: 0.01244 | Gamma: 0.0020\n",
      "Iter 1173/5000 | Loss: 0.42755 | Gap: 0.02082 | Gamma: 0.0010\n",
      "Iter 1174/5000 | Loss: 0.42754 | Gap: 0.01131 | Gamma: 0.0020\n",
      "Iter 1175/5000 | Loss: 0.42754 | Gap: 0.01785 | Gamma: 0.0010\n",
      "Iter 1176/5000 | Loss: 0.42753 | Gap: 0.00946 | Gamma: 0.0020\n",
      "Iter 1177/5000 | Loss: 0.42753 | Gap: 0.01954 | Gamma: 0.0010\n",
      "Iter 1178/5000 | Loss: 0.42752 | Gap: 0.01168 | Gamma: 0.0020\n",
      "Iter 1179/5000 | Loss: 0.42751 | Gap: 0.01872 | Gamma: 0.0010\n",
      "Iter 1180/5000 | Loss: 0.42751 | Gap: 0.00978 | Gamma: 0.0039\n",
      "Iter 1181/5000 | Loss: 0.42750 | Gap: 0.03139 | Gamma: 0.0010\n",
      "Iter 1182/5000 | Loss: 0.42750 | Gap: 0.00975 | Gamma: 0.0039\n",
      "Iter 1183/5000 | Loss: 0.42749 | Gap: 0.03498 | Gamma: 0.0010\n",
      "Iter 1184/5000 | Loss: 0.42748 | Gap: 0.01593 | Gamma: 0.0010\n",
      "Iter 1185/5000 | Loss: 0.42748 | Gap: 0.01303 | Gamma: 0.0010\n",
      "Iter 1186/5000 | Loss: 0.42747 | Gap: 0.01198 | Gamma: 0.0010\n",
      "Iter 1187/5000 | Loss: 0.42746 | Gap: 0.01034 | Gamma: 0.0020\n",
      "Iter 1188/5000 | Loss: 0.42746 | Gap: 0.01848 | Gamma: 0.0010\n",
      "Iter 1189/5000 | Loss: 0.42746 | Gap: 0.01179 | Gamma: 0.0010\n",
      "Iter 1190/5000 | Loss: 0.42745 | Gap: 0.01335 | Gamma: 0.0010\n",
      "Iter 1191/5000 | Loss: 0.42745 | Gap: 0.01056 | Gamma: 0.0020\n",
      "Iter 1192/5000 | Loss: 0.42744 | Gap: 0.01900 | Gamma: 0.0010\n",
      "Iter 1193/5000 | Loss: 0.42744 | Gap: 0.01075 | Gamma: 0.0020\n",
      "Iter 1194/5000 | Loss: 0.42744 | Gap: 0.02371 | Gamma: 0.0010\n",
      "Iter 1195/5000 | Loss: 0.42743 | Gap: 0.01144 | Gamma: 0.0020\n",
      "Iter 1196/5000 | Loss: 0.42743 | Gap: 0.01868 | Gamma: 0.0010\n",
      "Iter 1197/5000 | Loss: 0.42741 | Gap: 0.00931 | Gamma: 0.0039\n",
      "Iter 1198/5000 | Loss: 0.42741 | Gap: 0.02827 | Gamma: 0.0010\n",
      "Iter 1199/5000 | Loss: 0.42741 | Gap: 0.01049 | Gamma: 0.0020\n",
      "Iter 1200/5000 | Loss: 0.42740 | Gap: 0.02081 | Gamma: 0.0010\n",
      "Iter 1201/5000 | Loss: 0.42739 | Gap: 0.00989 | Gamma: 0.0020\n",
      "Iter 1202/5000 | Loss: 0.42739 | Gap: 0.01833 | Gamma: 0.0005\n",
      "Iter 1203/5000 | Loss: 0.42738 | Gap: 0.01000 | Gamma: 0.0020\n",
      "Iter 1204/5000 | Loss: 0.42737 | Gap: 0.01724 | Gamma: 0.0010\n",
      "Iter 1205/5000 | Loss: 0.42737 | Gap: 0.01070 | Gamma: 0.0020\n",
      "Iter 1206/5000 | Loss: 0.42736 | Gap: 0.01772 | Gamma: 0.0010\n",
      "Iter 1207/5000 | Loss: 0.42736 | Gap: 0.01219 | Gamma: 0.0010\n",
      "Iter 1208/5000 | Loss: 0.42736 | Gap: 0.01554 | Gamma: 0.0010\n",
      "Iter 1209/5000 | Loss: 0.42736 | Gap: 0.01265 | Gamma: 0.0020\n",
      "Iter 1210/5000 | Loss: 0.42735 | Gap: 0.01987 | Gamma: 0.0010\n",
      "Iter 1211/5000 | Loss: 0.42735 | Gap: 0.00900 | Gamma: 0.0039\n",
      "Iter 1212/5000 | Loss: 0.42734 | Gap: 0.03587 | Gamma: 0.0010\n",
      "Iter 1213/5000 | Loss: 0.42734 | Gap: 0.01182 | Gamma: 0.0010\n",
      "Iter 1214/5000 | Loss: 0.42733 | Gap: 0.01350 | Gamma: 0.0010\n",
      "Iter 1215/5000 | Loss: 0.42733 | Gap: 0.01336 | Gamma: 0.0010\n",
      "Iter 1216/5000 | Loss: 0.42732 | Gap: 0.01073 | Gamma: 0.0020\n",
      "Iter 1217/5000 | Loss: 0.42732 | Gap: 0.01816 | Gamma: 0.0010\n",
      "Iter 1218/5000 | Loss: 0.42731 | Gap: 0.01002 | Gamma: 0.0020\n",
      "Iter 1219/5000 | Loss: 0.42731 | Gap: 0.01979 | Gamma: 0.0010\n",
      "Iter 1220/5000 | Loss: 0.42731 | Gap: 0.00986 | Gamma: 0.0039\n",
      "Iter 1221/5000 | Loss: 0.42730 | Gap: 0.03642 | Gamma: 0.0010\n",
      "Iter 1222/5000 | Loss: 0.42730 | Gap: 0.01199 | Gamma: 0.0010\n",
      "Iter 1223/5000 | Loss: 0.42730 | Gap: 0.01243 | Gamma: 0.0010\n",
      "Iter 1224/5000 | Loss: 0.42729 | Gap: 0.01515 | Gamma: 0.0010\n",
      "Iter 1225/5000 | Loss: 0.42729 | Gap: 0.00988 | Gamma: 0.0039\n",
      "Iter 1226/5000 | Loss: 0.42727 | Gap: 0.03464 | Gamma: 0.0010\n",
      "Iter 1227/5000 | Loss: 0.42727 | Gap: 0.01109 | Gamma: 0.0020\n",
      "Iter 1228/5000 | Loss: 0.42726 | Gap: 0.01656 | Gamma: 0.0010\n",
      "Iter 1229/5000 | Loss: 0.42726 | Gap: 0.01184 | Gamma: 0.0020\n",
      "Iter 1230/5000 | Loss: 0.42725 | Gap: 0.01869 | Gamma: 0.0010\n",
      "Iter 1231/5000 | Loss: 0.42725 | Gap: 0.01070 | Gamma: 0.0020\n",
      "Iter 1232/5000 | Loss: 0.42725 | Gap: 0.02299 | Gamma: 0.0010\n",
      "Iter 1233/5000 | Loss: 0.42725 | Gap: 0.01213 | Gamma: 0.0020\n",
      "Iter 1234/5000 | Loss: 0.42724 | Gap: 0.02079 | Gamma: 0.0010\n",
      "Iter 1235/5000 | Loss: 0.42723 | Gap: 0.01052 | Gamma: 0.0020\n",
      "Iter 1236/5000 | Loss: 0.42723 | Gap: 0.01814 | Gamma: 0.0010\n",
      "Iter 1237/5000 | Loss: 0.42722 | Gap: 0.00990 | Gamma: 0.0039\n",
      "Iter 1238/5000 | Loss: 0.42721 | Gap: 0.03195 | Gamma: 0.0010\n",
      "Iter 1239/5000 | Loss: 0.42721 | Gap: 0.01002 | Gamma: 0.0020\n",
      "Iter 1240/5000 | Loss: 0.42720 | Gap: 0.01928 | Gamma: 0.0010\n",
      "Iter 1241/5000 | Loss: 0.42719 | Gap: 0.01035 | Gamma: 0.0020\n",
      "Iter 1242/5000 | Loss: 0.42719 | Gap: 0.01723 | Gamma: 0.0010\n",
      "Iter 1243/5000 | Loss: 0.42718 | Gap: 0.01008 | Gamma: 0.0020\n",
      "Iter 1244/5000 | Loss: 0.42718 | Gap: 0.01924 | Gamma: 0.0010\n",
      "Iter 1245/5000 | Loss: 0.42717 | Gap: 0.01034 | Gamma: 0.0020\n",
      "Iter 1246/5000 | Loss: 0.42717 | Gap: 0.02329 | Gamma: 0.0010\n",
      "Iter 1247/5000 | Loss: 0.42717 | Gap: 0.01033 | Gamma: 0.0039\n",
      "Iter 1248/5000 | Loss: 0.42715 | Gap: 0.03046 | Gamma: 0.0010\n",
      "Iter 1249/5000 | Loss: 0.42715 | Gap: 0.00993 | Gamma: 0.0039\n",
      "Iter 1250/5000 | Loss: 0.42715 | Gap: 0.03324 | Gamma: 0.0010\n",
      "Iter 1251/5000 | Loss: 0.42714 | Gap: 0.01659 | Gamma: 0.0005\n",
      "Iter 1252/5000 | Loss: 0.42714 | Gap: 0.00933 | Gamma: 0.0039\n",
      "Iter 1253/5000 | Loss: 0.42713 | Gap: 0.03536 | Gamma: 0.0010\n",
      "Iter 1254/5000 | Loss: 0.42712 | Gap: 0.01056 | Gamma: 0.0020\n",
      "Iter 1255/5000 | Loss: 0.42712 | Gap: 0.01270 | Gamma: 0.0010\n",
      "Iter 1256/5000 | Loss: 0.42712 | Gap: 0.01075 | Gamma: 0.0020\n",
      "Iter 1257/5000 | Loss: 0.42711 | Gap: 0.01975 | Gamma: 0.0010\n",
      "Iter 1258/5000 | Loss: 0.42711 | Gap: 0.00941 | Gamma: 0.0020\n",
      "Iter 1259/5000 | Loss: 0.42710 | Gap: 0.02193 | Gamma: 0.0010\n",
      "Iter 1260/5000 | Loss: 0.42710 | Gap: 0.01152 | Gamma: 0.0010\n",
      "Iter 1261/5000 | Loss: 0.42709 | Gap: 0.01104 | Gamma: 0.0010\n",
      "Iter 1262/5000 | Loss: 0.42709 | Gap: 0.01107 | Gamma: 0.0010\n",
      "Iter 1263/5000 | Loss: 0.42708 | Gap: 0.01110 | Gamma: 0.0010\n",
      "Iter 1264/5000 | Loss: 0.42708 | Gap: 0.01176 | Gamma: 0.0010\n",
      "Iter 1265/5000 | Loss: 0.42708 | Gap: 0.01224 | Gamma: 0.0010\n",
      "Iter 1266/5000 | Loss: 0.42707 | Gap: 0.00938 | Gamma: 0.0020\n",
      "Iter 1267/5000 | Loss: 0.42706 | Gap: 0.01576 | Gamma: 0.0010\n",
      "Iter 1268/5000 | Loss: 0.42706 | Gap: 0.00871 | Gamma: 0.0039\n",
      "Iter 1269/5000 | Loss: 0.42706 | Gap: 0.03738 | Gamma: 0.0010\n",
      "Iter 1270/5000 | Loss: 0.42705 | Gap: 0.00994 | Gamma: 0.0020\n",
      "Iter 1271/5000 | Loss: 0.42705 | Gap: 0.02163 | Gamma: 0.0010\n",
      "Iter 1272/5000 | Loss: 0.42705 | Gap: 0.01091 | Gamma: 0.0020\n",
      "Iter 1273/5000 | Loss: 0.42705 | Gap: 0.02070 | Gamma: 0.0010\n",
      "Iter 1274/5000 | Loss: 0.42705 | Gap: 0.01258 | Gamma: 0.0010\n",
      "Iter 1275/5000 | Loss: 0.42704 | Gap: 0.01375 | Gamma: 0.0010\n",
      "Iter 1276/5000 | Loss: 0.42704 | Gap: 0.01300 | Gamma: 0.0010\n",
      "Iter 1277/5000 | Loss: 0.42703 | Gap: 0.01246 | Gamma: 0.0010\n",
      "Iter 1278/5000 | Loss: 0.42703 | Gap: 0.00945 | Gamma: 0.0020\n",
      "Iter 1279/5000 | Loss: 0.42702 | Gap: 0.01568 | Gamma: 0.0010\n",
      "Iter 1280/5000 | Loss: 0.42702 | Gap: 0.00979 | Gamma: 0.0020\n",
      "Iter 1281/5000 | Loss: 0.42701 | Gap: 0.02046 | Gamma: 0.0010\n",
      "Iter 1282/5000 | Loss: 0.42700 | Gap: 0.00947 | Gamma: 0.0020\n",
      "Iter 1283/5000 | Loss: 0.42700 | Gap: 0.01712 | Gamma: 0.0010\n",
      "Iter 1284/5000 | Loss: 0.42700 | Gap: 0.01075 | Gamma: 0.0020\n",
      "Iter 1285/5000 | Loss: 0.42699 | Gap: 0.01754 | Gamma: 0.0010\n",
      "Iter 1286/5000 | Loss: 0.42699 | Gap: 0.01037 | Gamma: 0.0020\n",
      "Iter 1287/5000 | Loss: 0.42698 | Gap: 0.01955 | Gamma: 0.0010\n",
      "Iter 1288/5000 | Loss: 0.42697 | Gap: 0.00940 | Gamma: 0.0020\n",
      "Iter 1289/5000 | Loss: 0.42697 | Gap: 0.01777 | Gamma: 0.0010\n",
      "Iter 1290/5000 | Loss: 0.42697 | Gap: 0.01143 | Gamma: 0.0010\n",
      "Iter 1291/5000 | Loss: 0.42696 | Gap: 0.00994 | Gamma: 0.0020\n",
      "Iter 1292/5000 | Loss: 0.42696 | Gap: 0.02055 | Gamma: 0.0010\n",
      "Iter 1293/5000 | Loss: 0.42695 | Gap: 0.00889 | Gamma: 0.0020\n",
      "Iter 1294/5000 | Loss: 0.42695 | Gap: 0.01734 | Gamma: 0.0005\n",
      "Iter 1295/5000 | Loss: 0.42694 | Gap: 0.00822 | Gamma: 0.0039\n",
      "Iter 1296/5000 | Loss: 0.42693 | Gap: 0.02961 | Gamma: 0.0010\n",
      "Iter 1297/5000 | Loss: 0.42693 | Gap: 0.01005 | Gamma: 0.0010\n",
      "Iter 1298/5000 | Loss: 0.42693 | Gap: 0.01118 | Gamma: 0.0010\n",
      "Iter 1299/5000 | Loss: 0.42692 | Gap: 0.01065 | Gamma: 0.0010\n",
      "Iter 1300/5000 | Loss: 0.42692 | Gap: 0.00893 | Gamma: 0.0020\n",
      "Iter 1301/5000 | Loss: 0.42691 | Gap: 0.01870 | Gamma: 0.0010\n",
      "Iter 1302/5000 | Loss: 0.42691 | Gap: 0.00854 | Gamma: 0.0020\n",
      "Iter 1303/5000 | Loss: 0.42691 | Gap: 0.01642 | Gamma: 0.0010\n",
      "Iter 1304/5000 | Loss: 0.42690 | Gap: 0.01165 | Gamma: 0.0010\n",
      "Iter 1305/5000 | Loss: 0.42690 | Gap: 0.01081 | Gamma: 0.0010\n",
      "Iter 1306/5000 | Loss: 0.42689 | Gap: 0.00915 | Gamma: 0.0020\n",
      "Iter 1307/5000 | Loss: 0.42689 | Gap: 0.01699 | Gamma: 0.0010\n",
      "Iter 1308/5000 | Loss: 0.42689 | Gap: 0.01073 | Gamma: 0.0010\n",
      "Iter 1309/5000 | Loss: 0.42688 | Gap: 0.01190 | Gamma: 0.0010\n",
      "Iter 1310/5000 | Loss: 0.42688 | Gap: 0.01064 | Gamma: 0.0010\n",
      "Iter 1311/5000 | Loss: 0.42688 | Gap: 0.01117 | Gamma: 0.0010\n",
      "Iter 1312/5000 | Loss: 0.42688 | Gap: 0.01255 | Gamma: 0.0010\n",
      "Iter 1313/5000 | Loss: 0.42687 | Gap: 0.01122 | Gamma: 0.0010\n",
      "Iter 1314/5000 | Loss: 0.42687 | Gap: 0.00896 | Gamma: 0.0020\n",
      "Iter 1315/5000 | Loss: 0.42686 | Gap: 0.01805 | Gamma: 0.0010\n",
      "Iter 1316/5000 | Loss: 0.42686 | Gap: 0.00847 | Gamma: 0.0020\n",
      "Iter 1317/5000 | Loss: 0.42685 | Gap: 0.01467 | Gamma: 0.0010\n",
      "Iter 1318/5000 | Loss: 0.42685 | Gap: 0.00901 | Gamma: 0.0020\n",
      "Iter 1319/5000 | Loss: 0.42685 | Gap: 0.01999 | Gamma: 0.0010\n",
      "Iter 1320/5000 | Loss: 0.42684 | Gap: 0.00870 | Gamma: 0.0020\n",
      "Iter 1321/5000 | Loss: 0.42684 | Gap: 0.01579 | Gamma: 0.0010\n",
      "Iter 1322/5000 | Loss: 0.42683 | Gap: 0.00870 | Gamma: 0.0020\n",
      "Iter 1323/5000 | Loss: 0.42683 | Gap: 0.01453 | Gamma: 0.0010\n",
      "Iter 1324/5000 | Loss: 0.42683 | Gap: 0.01255 | Gamma: 0.0010\n",
      "Iter 1325/5000 | Loss: 0.42682 | Gap: 0.01209 | Gamma: 0.0010\n",
      "Iter 1326/5000 | Loss: 0.42682 | Gap: 0.01423 | Gamma: 0.0010\n",
      "Iter 1327/5000 | Loss: 0.42682 | Gap: 0.01110 | Gamma: 0.0010\n",
      "Iter 1328/5000 | Loss: 0.42681 | Gap: 0.01001 | Gamma: 0.0010\n",
      "Iter 1329/5000 | Loss: 0.42681 | Gap: 0.01046 | Gamma: 0.0010\n",
      "Iter 1330/5000 | Loss: 0.42681 | Gap: 0.00983 | Gamma: 0.0020\n",
      "Iter 1331/5000 | Loss: 0.42681 | Gap: 0.02049 | Gamma: 0.0010\n",
      "Iter 1332/5000 | Loss: 0.42680 | Gap: 0.01127 | Gamma: 0.0010\n",
      "Iter 1333/5000 | Loss: 0.42680 | Gap: 0.01255 | Gamma: 0.0010\n",
      "Iter 1334/5000 | Loss: 0.42680 | Gap: 0.00962 | Gamma: 0.0020\n",
      "Iter 1335/5000 | Loss: 0.42679 | Gap: 0.01809 | Gamma: 0.0010\n",
      "Iter 1336/5000 | Loss: 0.42679 | Gap: 0.00802 | Gamma: 0.0039\n",
      "Iter 1337/5000 | Loss: 0.42678 | Gap: 0.02873 | Gamma: 0.0010\n",
      "Iter 1338/5000 | Loss: 0.42678 | Gap: 0.00961 | Gamma: 0.0020\n",
      "Iter 1339/5000 | Loss: 0.42678 | Gap: 0.02112 | Gamma: 0.0010\n",
      "Iter 1340/5000 | Loss: 0.42677 | Gap: 0.01002 | Gamma: 0.0020\n",
      "Iter 1341/5000 | Loss: 0.42677 | Gap: 0.01932 | Gamma: 0.0010\n",
      "Iter 1342/5000 | Loss: 0.42677 | Gap: 0.01220 | Gamma: 0.0010\n",
      "Iter 1343/5000 | Loss: 0.42677 | Gap: 0.01267 | Gamma: 0.0010\n",
      "Iter 1344/5000 | Loss: 0.42676 | Gap: 0.00981 | Gamma: 0.0020\n",
      "Iter 1345/5000 | Loss: 0.42675 | Gap: 0.01619 | Gamma: 0.0010\n",
      "Iter 1346/5000 | Loss: 0.42675 | Gap: 0.00870 | Gamma: 0.0020\n",
      "Iter 1347/5000 | Loss: 0.42675 | Gap: 0.02371 | Gamma: 0.0010\n",
      "Iter 1348/5000 | Loss: 0.42675 | Gap: 0.01090 | Gamma: 0.0010\n",
      "Iter 1349/5000 | Loss: 0.42674 | Gap: 0.01031 | Gamma: 0.0010\n",
      "Iter 1350/5000 | Loss: 0.42674 | Gap: 0.01053 | Gamma: 0.0010\n",
      "Iter 1351/5000 | Loss: 0.42673 | Gap: 0.00939 | Gamma: 0.0020\n",
      "Iter 1352/5000 | Loss: 0.42673 | Gap: 0.01760 | Gamma: 0.0010\n",
      "Iter 1353/5000 | Loss: 0.42672 | Gap: 0.00937 | Gamma: 0.0010\n",
      "Iter 1354/5000 | Loss: 0.42672 | Gap: 0.01091 | Gamma: 0.0010\n",
      "Iter 1355/5000 | Loss: 0.42672 | Gap: 0.00948 | Gamma: 0.0020\n",
      "Iter 1356/5000 | Loss: 0.42671 | Gap: 0.01753 | Gamma: 0.0010\n",
      "Iter 1357/5000 | Loss: 0.42671 | Gap: 0.00842 | Gamma: 0.0039\n",
      "Iter 1358/5000 | Loss: 0.42670 | Gap: 0.02867 | Gamma: 0.0010\n",
      "Iter 1359/5000 | Loss: 0.42669 | Gap: 0.00923 | Gamma: 0.0020\n",
      "Iter 1360/5000 | Loss: 0.42669 | Gap: 0.01574 | Gamma: 0.0010\n",
      "Iter 1361/5000 | Loss: 0.42668 | Gap: 0.01017 | Gamma: 0.0010\n",
      "Iter 1362/5000 | Loss: 0.42668 | Gap: 0.00910 | Gamma: 0.0020\n",
      "Iter 1363/5000 | Loss: 0.42668 | Gap: 0.01854 | Gamma: 0.0010\n",
      "Iter 1364/5000 | Loss: 0.42667 | Gap: 0.00847 | Gamma: 0.0020\n",
      "Iter 1365/5000 | Loss: 0.42667 | Gap: 0.02293 | Gamma: 0.0005\n",
      "Iter 1366/5000 | Loss: 0.42666 | Gap: 0.00850 | Gamma: 0.0020\n",
      "Iter 1367/5000 | Loss: 0.42666 | Gap: 0.01772 | Gamma: 0.0005\n",
      "Iter 1368/5000 | Loss: 0.42666 | Gap: 0.00772 | Gamma: 0.0039\n",
      "Iter 1369/5000 | Loss: 0.42665 | Gap: 0.03175 | Gamma: 0.0005\n",
      "Iter 1370/5000 | Loss: 0.42664 | Gap: 0.01207 | Gamma: 0.0010\n",
      "Iter 1371/5000 | Loss: 0.42664 | Gap: 0.00756 | Gamma: 0.0010\n",
      "Iter 1372/5000 | Loss: 0.42664 | Gap: 0.01008 | Gamma: 0.0005\n",
      "Iter 1373/5000 | Loss: 0.42663 | Gap: 0.00744 | Gamma: 0.0010\n",
      "Iter 1374/5000 | Loss: 0.42663 | Gap: 0.01191 | Gamma: 0.0010\n",
      "Iter 1375/5000 | Loss: 0.42663 | Gap: 0.01040 | Gamma: 0.0010\n",
      "Iter 1376/5000 | Loss: 0.42663 | Gap: 0.00867 | Gamma: 0.0010\n",
      "Iter 1377/5000 | Loss: 0.42663 | Gap: 0.01117 | Gamma: 0.0010\n",
      "Iter 1378/5000 | Loss: 0.42662 | Gap: 0.01340 | Gamma: 0.0005\n",
      "Iter 1379/5000 | Loss: 0.42662 | Gap: 0.00736 | Gamma: 0.0020\n",
      "Iter 1380/5000 | Loss: 0.42662 | Gap: 0.02050 | Gamma: 0.0005\n",
      "Iter 1381/5000 | Loss: 0.42661 | Gap: 0.00741 | Gamma: 0.0020\n",
      "Iter 1382/5000 | Loss: 0.42661 | Gap: 0.01430 | Gamma: 0.0005\n",
      "Iter 1383/5000 | Loss: 0.42660 | Gap: 0.00740 | Gamma: 0.0020\n",
      "Iter 1384/5000 | Loss: 0.42660 | Gap: 0.01716 | Gamma: 0.0005\n",
      "Iter 1385/5000 | Loss: 0.42659 | Gap: 0.00733 | Gamma: 0.0020\n",
      "Iter 1386/5000 | Loss: 0.42659 | Gap: 0.01456 | Gamma: 0.0005\n",
      "Iter 1387/5000 | Loss: 0.42659 | Gap: 0.00746 | Gamma: 0.0010\n",
      "Iter 1388/5000 | Loss: 0.42659 | Gap: 0.01345 | Gamma: 0.0005\n",
      "Iter 1389/5000 | Loss: 0.42658 | Gap: 0.00680 | Gamma: 0.0020\n",
      "Iter 1390/5000 | Loss: 0.42658 | Gap: 0.01477 | Gamma: 0.0005\n",
      "Iter 1391/5000 | Loss: 0.42657 | Gap: 0.00644 | Gamma: 0.0020\n",
      "Iter 1392/5000 | Loss: 0.42657 | Gap: 0.02220 | Gamma: 0.0005\n",
      "Iter 1393/5000 | Loss: 0.42657 | Gap: 0.00810 | Gamma: 0.0020\n",
      "Iter 1394/5000 | Loss: 0.42657 | Gap: 0.01818 | Gamma: 0.0005\n",
      "Iter 1395/5000 | Loss: 0.42656 | Gap: 0.00970 | Gamma: 0.0010\n",
      "Iter 1396/5000 | Loss: 0.42656 | Gap: 0.01557 | Gamma: 0.0005\n",
      "Iter 1397/5000 | Loss: 0.42656 | Gap: 0.00731 | Gamma: 0.0020\n",
      "Iter 1398/5000 | Loss: 0.42656 | Gap: 0.02281 | Gamma: 0.0005\n",
      "Iter 1399/5000 | Loss: 0.42655 | Gap: 0.00649 | Gamma: 0.0020\n",
      "Iter 1400/5000 | Loss: 0.42655 | Gap: 0.01746 | Gamma: 0.0005\n",
      "Iter 1401/5000 | Loss: 0.42655 | Gap: 0.00707 | Gamma: 0.0020\n",
      "Iter 1402/5000 | Loss: 0.42654 | Gap: 0.01854 | Gamma: 0.0005\n",
      "Iter 1403/5000 | Loss: 0.42654 | Gap: 0.00878 | Gamma: 0.0010\n",
      "Iter 1404/5000 | Loss: 0.42654 | Gap: 0.00913 | Gamma: 0.0010\n",
      "Iter 1405/5000 | Loss: 0.42654 | Gap: 0.01047 | Gamma: 0.0010\n",
      "Iter 1406/5000 | Loss: 0.42653 | Gap: 0.00873 | Gamma: 0.0010\n",
      "Iter 1407/5000 | Loss: 0.42653 | Gap: 0.00967 | Gamma: 0.0010\n",
      "Iter 1408/5000 | Loss: 0.42653 | Gap: 0.01233 | Gamma: 0.0010\n",
      "Iter 1409/5000 | Loss: 0.42653 | Gap: 0.01014 | Gamma: 0.0010\n",
      "Iter 1410/5000 | Loss: 0.42653 | Gap: 0.01177 | Gamma: 0.0005\n",
      "Iter 1411/5000 | Loss: 0.42652 | Gap: 0.00660 | Gamma: 0.0020\n",
      "Iter 1412/5000 | Loss: 0.42652 | Gap: 0.01541 | Gamma: 0.0005\n",
      "Iter 1413/5000 | Loss: 0.42652 | Gap: 0.00721 | Gamma: 0.0010\n",
      "Iter 1414/5000 | Loss: 0.42651 | Gap: 0.01366 | Gamma: 0.0005\n",
      "Iter 1415/5000 | Loss: 0.42651 | Gap: 0.00675 | Gamma: 0.0039\n",
      "Iter 1416/5000 | Loss: 0.42651 | Gap: 0.03673 | Gamma: 0.0010\n",
      "Iter 1417/5000 | Loss: 0.42651 | Gap: 0.00907 | Gamma: 0.0010\n",
      "Iter 1418/5000 | Loss: 0.42650 | Gap: 0.00910 | Gamma: 0.0010\n",
      "Iter 1419/5000 | Loss: 0.42650 | Gap: 0.01119 | Gamma: 0.0010\n",
      "Iter 1420/5000 | Loss: 0.42650 | Gap: 0.01136 | Gamma: 0.0010\n",
      "Iter 1421/5000 | Loss: 0.42650 | Gap: 0.01081 | Gamma: 0.0010\n",
      "Iter 1422/5000 | Loss: 0.42650 | Gap: 0.01224 | Gamma: 0.0010\n",
      "Iter 1423/5000 | Loss: 0.42650 | Gap: 0.01365 | Gamma: 0.0005\n",
      "Iter 1424/5000 | Loss: 0.42649 | Gap: 0.00707 | Gamma: 0.0039\n",
      "Iter 1425/5000 | Loss: 0.42649 | Gap: 0.03603 | Gamma: 0.0010\n",
      "Iter 1426/5000 | Loss: 0.42648 | Gap: 0.00954 | Gamma: 0.0010\n",
      "Iter 1427/5000 | Loss: 0.42648 | Gap: 0.00885 | Gamma: 0.0010\n",
      "Iter 1428/5000 | Loss: 0.42647 | Gap: 0.00882 | Gamma: 0.0010\n",
      "Iter 1429/5000 | Loss: 0.42647 | Gap: 0.00855 | Gamma: 0.0010\n",
      "Iter 1430/5000 | Loss: 0.42647 | Gap: 0.01025 | Gamma: 0.0010\n",
      "Iter 1431/5000 | Loss: 0.42647 | Gap: 0.01156 | Gamma: 0.0010\n",
      "Iter 1432/5000 | Loss: 0.42647 | Gap: 0.00949 | Gamma: 0.0020\n",
      "Iter 1433/5000 | Loss: 0.42646 | Gap: 0.01577 | Gamma: 0.0010\n",
      "Iter 1434/5000 | Loss: 0.42646 | Gap: 0.00790 | Gamma: 0.0020\n",
      "Iter 1435/5000 | Loss: 0.42645 | Gap: 0.01903 | Gamma: 0.0005\n",
      "Iter 1436/5000 | Loss: 0.42645 | Gap: 0.00893 | Gamma: 0.0020\n",
      "Iter 1437/5000 | Loss: 0.42645 | Gap: 0.01994 | Gamma: 0.0005\n",
      "Iter 1438/5000 | Loss: 0.42644 | Gap: 0.00871 | Gamma: 0.0010\n",
      "Iter 1439/5000 | Loss: 0.42644 | Gap: 0.01081 | Gamma: 0.0005\n",
      "Iter 1440/5000 | Loss: 0.42644 | Gap: 0.00757 | Gamma: 0.0020\n",
      "Iter 1441/5000 | Loss: 0.42644 | Gap: 0.02237 | Gamma: 0.0005\n",
      "Iter 1442/5000 | Loss: 0.42643 | Gap: 0.00708 | Gamma: 0.0020\n",
      "Iter 1443/5000 | Loss: 0.42643 | Gap: 0.01411 | Gamma: 0.0005\n",
      "Iter 1444/5000 | Loss: 0.42642 | Gap: 0.00702 | Gamma: 0.0020\n",
      "Iter 1445/5000 | Loss: 0.42642 | Gap: 0.01982 | Gamma: 0.0005\n",
      "Iter 1446/5000 | Loss: 0.42642 | Gap: 0.00824 | Gamma: 0.0010\n",
      "Iter 1447/5000 | Loss: 0.42642 | Gap: 0.00964 | Gamma: 0.0005\n",
      "Iter 1448/5000 | Loss: 0.42641 | Gap: 0.00801 | Gamma: 0.0010\n",
      "Iter 1449/5000 | Loss: 0.42641 | Gap: 0.01115 | Gamma: 0.0005\n",
      "Iter 1450/5000 | Loss: 0.42641 | Gap: 0.00661 | Gamma: 0.0020\n",
      "Iter 1451/5000 | Loss: 0.42640 | Gap: 0.01850 | Gamma: 0.0005\n",
      "Iter 1452/5000 | Loss: 0.42640 | Gap: 0.00635 | Gamma: 0.0039\n",
      "Iter 1453/5000 | Loss: 0.42639 | Gap: 0.03146 | Gamma: 0.0005\n",
      "Iter 1454/5000 | Loss: 0.42639 | Gap: 0.01150 | Gamma: 0.0005\n",
      "Iter 1455/5000 | Loss: 0.42639 | Gap: 0.00944 | Gamma: 0.0005\n",
      "Iter 1456/5000 | Loss: 0.42638 | Gap: 0.00738 | Gamma: 0.0010\n",
      "Iter 1457/5000 | Loss: 0.42638 | Gap: 0.00885 | Gamma: 0.0010\n",
      "Iter 1458/5000 | Loss: 0.42638 | Gap: 0.01025 | Gamma: 0.0010\n",
      "Iter 1459/5000 | Loss: 0.42638 | Gap: 0.00890 | Gamma: 0.0010\n",
      "Iter 1460/5000 | Loss: 0.42637 | Gap: 0.00870 | Gamma: 0.0010\n",
      "Iter 1461/5000 | Loss: 0.42637 | Gap: 0.00780 | Gamma: 0.0010\n",
      "Iter 1462/5000 | Loss: 0.42637 | Gap: 0.00940 | Gamma: 0.0005\n",
      "Iter 1463/5000 | Loss: 0.42637 | Gap: 0.00689 | Gamma: 0.0010\n",
      "Iter 1464/5000 | Loss: 0.42637 | Gap: 0.01404 | Gamma: 0.0005\n",
      "Iter 1465/5000 | Loss: 0.42636 | Gap: 0.00653 | Gamma: 0.0010\n",
      "Iter 1466/5000 | Loss: 0.42636 | Gap: 0.01005 | Gamma: 0.0005\n",
      "Iter 1467/5000 | Loss: 0.42636 | Gap: 0.00629 | Gamma: 0.0010\n",
      "Iter 1468/5000 | Loss: 0.42636 | Gap: 0.01128 | Gamma: 0.0005\n",
      "Iter 1469/5000 | Loss: 0.42635 | Gap: 0.00602 | Gamma: 0.0039\n",
      "Iter 1470/5000 | Loss: 0.42635 | Gap: 0.03194 | Gamma: 0.0005\n",
      "Iter 1471/5000 | Loss: 0.42634 | Gap: 0.01062 | Gamma: 0.0010\n",
      "Iter 1472/5000 | Loss: 0.42634 | Gap: 0.00917 | Gamma: 0.0005\n",
      "Iter 1473/5000 | Loss: 0.42634 | Gap: 0.00675 | Gamma: 0.0010\n",
      "Iter 1474/5000 | Loss: 0.42634 | Gap: 0.00940 | Gamma: 0.0005\n",
      "Iter 1475/5000 | Loss: 0.42633 | Gap: 0.00620 | Gamma: 0.0010\n",
      "Iter 1476/5000 | Loss: 0.42633 | Gap: 0.01039 | Gamma: 0.0005\n",
      "Iter 1477/5000 | Loss: 0.42632 | Gap: 0.00552 | Gamma: 0.0039\n",
      "Iter 1478/5000 | Loss: 0.42632 | Gap: 0.02355 | Gamma: 0.0005\n",
      "Iter 1479/5000 | Loss: 0.42632 | Gap: 0.01176 | Gamma: 0.0005\n",
      "Iter 1480/5000 | Loss: 0.42632 | Gap: 0.00861 | Gamma: 0.0005\n",
      "Iter 1481/5000 | Loss: 0.42632 | Gap: 0.00728 | Gamma: 0.0005\n",
      "Iter 1482/5000 | Loss: 0.42632 | Gap: 0.00735 | Gamma: 0.0010\n",
      "Iter 1483/5000 | Loss: 0.42631 | Gap: 0.01293 | Gamma: 0.0005\n",
      "Iter 1484/5000 | Loss: 0.42631 | Gap: 0.00620 | Gamma: 0.0010\n",
      "Iter 1485/5000 | Loss: 0.42631 | Gap: 0.01050 | Gamma: 0.0005\n",
      "Iter 1486/5000 | Loss: 0.42631 | Gap: 0.00605 | Gamma: 0.0020\n",
      "Iter 1487/5000 | Loss: 0.42630 | Gap: 0.02024 | Gamma: 0.0005\n",
      "Iter 1488/5000 | Loss: 0.42630 | Gap: 0.00610 | Gamma: 0.0010\n",
      "Iter 1489/5000 | Loss: 0.42630 | Gap: 0.00905 | Gamma: 0.0005\n",
      "Iter 1490/5000 | Loss: 0.42630 | Gap: 0.00642 | Gamma: 0.0010\n",
      "Iter 1491/5000 | Loss: 0.42630 | Gap: 0.00968 | Gamma: 0.0005\n",
      "Iter 1492/5000 | Loss: 0.42629 | Gap: 0.00574 | Gamma: 0.0010\n",
      "Iter 1493/5000 | Loss: 0.42629 | Gap: 0.01080 | Gamma: 0.0005\n",
      "Iter 1494/5000 | Loss: 0.42629 | Gap: 0.00534 | Gamma: 0.0039\n",
      "Iter 1495/5000 | Loss: 0.42628 | Gap: 0.03282 | Gamma: 0.0005\n",
      "Iter 1496/5000 | Loss: 0.42628 | Gap: 0.01202 | Gamma: 0.0005\n",
      "Iter 1497/5000 | Loss: 0.42628 | Gap: 0.01053 | Gamma: 0.0005\n",
      "Iter 1498/5000 | Loss: 0.42628 | Gap: 0.00565 | Gamma: 0.0020\n",
      "Iter 1499/5000 | Loss: 0.42628 | Gap: 0.01893 | Gamma: 0.0005\n",
      "Iter 1500/5000 | Loss: 0.42628 | Gap: 0.00580 | Gamma: 0.0010\n",
      "Iter 1501/5000 | Loss: 0.42628 | Gap: 0.01582 | Gamma: 0.0005\n",
      "Iter 1502/5000 | Loss: 0.42627 | Gap: 0.00629 | Gamma: 0.0020\n",
      "Iter 1503/5000 | Loss: 0.42627 | Gap: 0.01949 | Gamma: 0.0005\n",
      "Iter 1504/5000 | Loss: 0.42627 | Gap: 0.00776 | Gamma: 0.0010\n",
      "Iter 1505/5000 | Loss: 0.42627 | Gap: 0.01032 | Gamma: 0.0005\n",
      "Iter 1506/5000 | Loss: 0.42626 | Gap: 0.00624 | Gamma: 0.0020\n",
      "Iter 1507/5000 | Loss: 0.42626 | Gap: 0.01798 | Gamma: 0.0005\n",
      "Iter 1508/5000 | Loss: 0.42626 | Gap: 0.00650 | Gamma: 0.0010\n",
      "Iter 1509/5000 | Loss: 0.42625 | Gap: 0.01105 | Gamma: 0.0005\n",
      "Iter 1510/5000 | Loss: 0.42625 | Gap: 0.00687 | Gamma: 0.0010\n",
      "Iter 1511/5000 | Loss: 0.42625 | Gap: 0.00956 | Gamma: 0.0005\n",
      "Iter 1512/5000 | Loss: 0.42625 | Gap: 0.00612 | Gamma: 0.0010\n",
      "Iter 1513/5000 | Loss: 0.42625 | Gap: 0.00946 | Gamma: 0.0005\n",
      "Iter 1514/5000 | Loss: 0.42624 | Gap: 0.00559 | Gamma: 0.0020\n",
      "Iter 1515/5000 | Loss: 0.42624 | Gap: 0.01571 | Gamma: 0.0005\n",
      "Iter 1516/5000 | Loss: 0.42624 | Gap: 0.00562 | Gamma: 0.0020\n",
      "Iter 1517/5000 | Loss: 0.42623 | Gap: 0.01880 | Gamma: 0.0005\n",
      "Iter 1518/5000 | Loss: 0.42623 | Gap: 0.00651 | Gamma: 0.0020\n",
      "Iter 1519/5000 | Loss: 0.42623 | Gap: 0.01611 | Gamma: 0.0005\n",
      "Iter 1520/5000 | Loss: 0.42623 | Gap: 0.00756 | Gamma: 0.0005\n",
      "Iter 1521/5000 | Loss: 0.42622 | Gap: 0.00724 | Gamma: 0.0005\n",
      "Iter 1522/5000 | Loss: 0.42622 | Gap: 0.00674 | Gamma: 0.0010\n",
      "Iter 1523/5000 | Loss: 0.42622 | Gap: 0.01279 | Gamma: 0.0005\n",
      "Iter 1524/5000 | Loss: 0.42622 | Gap: 0.00613 | Gamma: 0.0010\n",
      "Iter 1525/5000 | Loss: 0.42622 | Gap: 0.01156 | Gamma: 0.0005\n",
      "Iter 1526/5000 | Loss: 0.42622 | Gap: 0.00550 | Gamma: 0.0020\n",
      "Iter 1527/5000 | Loss: 0.42621 | Gap: 0.01658 | Gamma: 0.0005\n",
      "Iter 1528/5000 | Loss: 0.42621 | Gap: 0.00603 | Gamma: 0.0020\n",
      "Iter 1529/5000 | Loss: 0.42621 | Gap: 0.01819 | Gamma: 0.0005\n",
      "Iter 1530/5000 | Loss: 0.42621 | Gap: 0.00930 | Gamma: 0.0002\n",
      "Iter 1531/5000 | Loss: 0.42621 | Gap: 0.00633 | Gamma: 0.0010\n",
      "Iter 1532/5000 | Loss: 0.42620 | Gap: 0.01059 | Gamma: 0.0005\n",
      "Iter 1533/5000 | Loss: 0.42620 | Gap: 0.00664 | Gamma: 0.0010\n",
      "Iter 1534/5000 | Loss: 0.42620 | Gap: 0.01052 | Gamma: 0.0005\n",
      "Iter 1535/5000 | Loss: 0.42620 | Gap: 0.00571 | Gamma: 0.0020\n",
      "Iter 1536/5000 | Loss: 0.42620 | Gap: 0.02009 | Gamma: 0.0005\n",
      "Iter 1537/5000 | Loss: 0.42620 | Gap: 0.00696 | Gamma: 0.0005\n",
      "Iter 1538/5000 | Loss: 0.42619 | Gap: 0.00839 | Gamma: 0.0005\n",
      "Iter 1539/5000 | Loss: 0.42619 | Gap: 0.00589 | Gamma: 0.0020\n",
      "Iter 1540/5000 | Loss: 0.42619 | Gap: 0.01841 | Gamma: 0.0005\n",
      "Iter 1541/5000 | Loss: 0.42619 | Gap: 0.00657 | Gamma: 0.0010\n",
      "Iter 1542/5000 | Loss: 0.42619 | Gap: 0.01192 | Gamma: 0.0005\n",
      "Iter 1543/5000 | Loss: 0.42618 | Gap: 0.00602 | Gamma: 0.0020\n",
      "Iter 1544/5000 | Loss: 0.42618 | Gap: 0.01896 | Gamma: 0.0005\n",
      "Iter 1545/5000 | Loss: 0.42618 | Gap: 0.00737 | Gamma: 0.0010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- TEST FW STANDARD (Delta=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDELTA\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 3. Esegui l'algoritmo (usiamo Armijo che è più veloce)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Se vuoi provare la grid search, cambia ls_method='grid'\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m P_hat, history, gaps \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFW_standard\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mR_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDELTA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_ITER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzeros\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Puoi provare anche 'random'\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mls_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marmijo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# La tua scelta efficiente\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 4. Valutazione sul Test Set\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Usiamo la tua funzione di loss per calcolare l'RMSE sui dati nascosti\u001b[39;00m\n\u001b[0;32m     29\u001b[0m test_rmse \u001b[38;5;241m=\u001b[39m fn\u001b[38;5;241m.\u001b[39mfunction_loss(R_test, P_hat)\n",
      "File \u001b[1;32mc:\\Users\\utente13\\Desktop\\FW_OptimizationProject\\functions.py:212\u001b[0m, in \u001b[0;36mFW_standard\u001b[1;34m(R, delta, max_iter, tol, init_type, ls_method)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# 6. LINE SEARCH (Scelta Dinamica)\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ls_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marmijo\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# Passiamo grad e D densi per velocità\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m     gamma \u001b[38;5;241m=\u001b[39m \u001b[43marmijoRule_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ls_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# La tua vecchia funzione vuole D sparso\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     D_sparse \u001b[38;5;241m=\u001b[39m csr_matrix(D)\n",
      "File \u001b[1;32mc:\\Users\\utente13\\Desktop\\FW_OptimizationProject\\functions.py:132\u001b[0m, in \u001b[0;36marmijoRule_optimized\u001b[1;34m(R, P, f_loss, grad, D, gamma_max)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     P_test \u001b[38;5;241m=\u001b[39m P \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m D\n\u001b[1;32m--> 132\u001b[0m     new_loss \u001b[38;5;241m=\u001b[39m \u001b[43mf_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# Condizione di Armijo\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_loss \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m current_loss \u001b[38;5;241m+\u001b[39m sigma \u001b[38;5;241m*\u001b[39m alpha \u001b[38;5;241m*\u001b[39m grad_dot_D:\n",
      "File \u001b[1;32mc:\\Users\\utente13\\Desktop\\FW_OptimizationProject\\functions.py:22\u001b[0m, in \u001b[0;36mfunction_loss\u001b[1;34m(R, P)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mComputes the  Mean Squared Error (RMSE) between the non-zero elements\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mof R and the corresponding elements in P.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m- The MSE (actual matrix - predicted one)^2.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Find the indices of known ratings (non-zero in R)\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m known_indices \u001b[38;5;241m=\u001b[39m \u001b[43mR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Extract only the known ratings and corresponding predictions\u001b[39;00m\n\u001b[0;32m     25\u001b[0m R_known \u001b[38;5;241m=\u001b[39m R[known_indices]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import functions as fn\n",
    "\n",
    "# 1. Ricarica le funzioni (fondamentale se hai appena modificato functions.py)\n",
    "importlib.reload(fn)\n",
    "\n",
    "# 2. Imposta i parametri\n",
    "# Delta dipende dai dati. Per rating 1-5 e matrici di queste dimensioni, \n",
    "# un valore tra 1000 e 5000 è spesso un buon punto di partenza.\n",
    "DELTA = 5000\n",
    "MAX_ITER = 5000\n",
    "\n",
    "print(f\"--- TEST FW STANDARD (Delta={DELTA}) ---\")\n",
    "\n",
    "# 3. Esegui l'algoritmo (usiamo Armijo che è più veloce)\n",
    "# Se vuoi provare la grid search, cambia ls_method='grid'\n",
    "P_hat, history, gaps = fn.FW_standard(\n",
    "    R_train, \n",
    "    delta=DELTA, \n",
    "    max_iter=MAX_ITER, \n",
    "    tol=1e-3, \n",
    "    init_type='zeros',    # Puoi provare anche 'random'\n",
    "    ls_method='armijo'    # La tua scelta efficiente\n",
    ")\n",
    "\n",
    "# 4. Valutazione sul Test Set\n",
    "# Usiamo la tua funzione di loss per calcolare l'RMSE sui dati nascosti\n",
    "test_rmse = fn.function_loss(R_test, P_hat)\n",
    "\n",
    "print(f\"\\nRISULTATI FINALI:\")\n",
    "print(f\"Final Training RMSE: {history[-1]:.4f}\")\n",
    "print(f\"Final Test RMSE:     {test_rmse:.4f}\")\n",
    "print(f\"Final Duality Gap:   {gaps[-1]:.4f}\")\n",
    "\n",
    "# 5. Grafici di Convergenza\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Grafico Loss (RMSE)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history, marker='o', label='Train RMSE')\n",
    "plt.axhline(y=test_rmse, color='r', linestyle='--', label=f'Test RMSE ({test_rmse:.3f})')\n",
    "plt.xlabel('Iterazioni')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Convergenza della Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Grafico Duality Gap (deve andare a zero)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(gaps, marker='o', color='orange', label='Duality Gap')\n",
    "plt.xlabel('Iterazioni')\n",
    "plt.ylabel('Gap')\n",
    "plt.yscale('log') # Scala logaritmica perché il gap scende molto veloce\n",
    "plt.title('Convergenza del Duality Gap')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08662731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
